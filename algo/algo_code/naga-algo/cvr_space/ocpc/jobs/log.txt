+ work_path=/home/ad_user/personal/ling.fang/
+ cd /home/ad_user/personal/ling.fang//cvr_space/click_join_trans
+ bash -x merge_hdfs_file.sh
++ pwd
+ ROOT_PATH=/home/ad_user/personal/ling.fang/cvr_space/click_join_trans
+ USER_NAME=ad_user
+ JOB_PATH=/home/ad_user/personal/ling.fang/cvr_space/click_join_trans/
+ HADOOP_HOME=/usr/local/hadoop-2.6.3
+ SPARK_HOME=/usr/local/spark-2.1.1-bin-hadoop2.6
+ LOG_PATH=hdfs:///user/ad_user/spark_eventlog
+ '[' 0 -eq 1 ']'
++ date -d ' 1 days ago ' +%Y%m%d
+ DATE=20191212
++ date -d '1 days ago 20191212' +%Y%m%d
+ YESTDAY=20191211
+ CLK_PATH=/user/ad_user/ocpc/click_join_trans/clk_path/
+ TRANS_PATH=/user/ad_user/ocpc/click_join_trans/trans_path/
+ SSPSTAT_PATH=/user/ad_user/ocpc/click_join_trans/sspstat_path/
+ sparksubmit='/usr/local/spark-2.1.1-bin-hadoop2.6/bin/spark-submit                  --master yarn                  --queue root.ad-root.etl.dailyetl.high                  --num-executors 50                  --executor-memory 8g                  --driver-memory 12g                  --conf spark.dynamicAllocation.enabled=true                  --conf spark.dynamicAllocation.maxExecutors=100                  --conf spark.dynamicAllocation.minExecutors=50                  --conf spark.driver.maxResultSize=8G                  --conf spark.shuffle.service.enabled=true                  --conf spark.eventLog.enabled=true                  --conf spark.sql.broadcastTimeout=36000                  --conf spark.eventLog.dir=hdfs:///user/ad_user/spark_eventlog'
+ /usr/local/spark-2.1.1-bin-hadoop2.6/bin/spark-submit --master yarn --queue root.ad-root.etl.dailyetl.high --num-executors 50 --executor-memory 8g --driver-memory 12g --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.maxExecutors=100 --conf spark.dynamicAllocation.minExecutors=50 --conf spark.driver.maxResultSize=8G --conf spark.shuffle.service.enabled=true --conf spark.eventLog.enabled=true --conf spark.sql.broadcastTimeout=36000 --conf spark.eventLog.dir=hdfs:///user/ad_user/spark_eventlog /home/ad_user/personal/ling.fang/cvr_space/click_join_trans//merge_hdfs_file.py /user/ad_user/ocpc/click_join_trans/clk_path/ /user/ad_user/ocpc/click_join_trans/trans_path/ /user/ad_user/ocpc/click_join_trans/sspstat_path/ 20191212
JAVA_LIBRARY_PATH: :/usr/local/hadoop-ha/lib/native:/usr/lib
LD_LIBRARY_PATH :/usr/local/hadoop-ha/lib/native:/usr/lib
19/12/13 02:00:04 INFO spark.SparkContext: Running Spark version 2.4.3
19/12/13 02:00:04 INFO spark.SparkContext: Submitted application: naga DSP: ed_join_click_20191212
19/12/13 02:00:04 INFO spark.SecurityManager: Changing view acls to: ad_user
19/12/13 02:00:04 INFO spark.SecurityManager: Changing modify acls to: ad_user
19/12/13 02:00:04 INFO spark.SecurityManager: Changing view acls groups to: 
19/12/13 02:00:04 INFO spark.SecurityManager: Changing modify acls groups to: 
19/12/13 02:00:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ad_user); groups with view permissions: Set(); users  with modify permissions: Set(ad_user); groups with modify permissions: Set()
19/12/13 02:00:04 INFO util.Utils: Successfully started service 'sparkDriver' on port 23881.
19/12/13 02:00:04 INFO spark.SparkEnv: Registering MapOutputTracker
19/12/13 02:00:05 INFO spark.SparkEnv: Registering BlockManagerMaster
19/12/13 02:00:05 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/12/13 02:00:05 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/12/13 02:00:05 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-49466aed-e6b4-446a-920b-7f5fed608bd2
19/12/13 02:00:05 INFO memory.MemoryStore: MemoryStore started with capacity 6.9 GB
19/12/13 02:00:05 INFO spark.SparkEnv: Registering OutputCommitCoordinator
19/12/13 02:00:05 INFO util.log: Logging initialized @2776ms
19/12/13 02:00:05 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
19/12/13 02:00:05 INFO server.Server: Started @2845ms
19/12/13 02:00:05 INFO server.AbstractConnector: Started ServerConnector@198b1405{HTTP/1.1,[http/1.1]}{0.0.0.0:34275}
19/12/13 02:00:05 INFO util.Utils: Successfully started service 'SparkUI' on port 34275.
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2475874{/jobs,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ca3472a{/jobs/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@353b7e9d{/jobs/job,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@766352d{/jobs/job/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@710ab8fc{/stages,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fabffd6{/stages/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17837fff{/stages/stage,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@458674cf{/stages/stage/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49f68707{/stages/pool,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f019851{/stages/pool/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21fbb59b{/storage,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57ad8d67{/storage/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a9ee62a{/storage/rdd,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19ee3038{/storage/rdd/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65c519f3{/environment,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5eb87699{/environment/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ca64d2{/executors,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b4deb63{/executors/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@268a6d78{/executors/threadDump,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b1bcb8e{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b27d4cb{/static,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b785a12{/,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6deec869{/api,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@112ce3a1{/jobs/job/kill,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7787e779{/stages/stage/kill,null,AVAILABLE,@Spark}
19/12/13 02:00:05 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.30:34275
19/12/13 02:00:05 WARN util.Utils: spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
19/12/13 02:00:05 INFO util.Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
19/12/13 02:00:06 INFO client.RMProxy: Connecting to ResourceManager at hadoop2-namenode.corp.cootek.com/192.168.30.10:8032
19/12/13 02:00:06 INFO yarn.Client: Requesting a new application from cluster with 126 NodeManagers
19/12/13 02:00:06 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (32768 MB per container)
19/12/13 02:00:06 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
19/12/13 02:00:06 INFO yarn.Client: Setting up container launch context for our AM
19/12/13 02:00:06 INFO yarn.Client: Setting up the launch environment for our AM container
19/12/13 02:00:06 INFO yarn.Client: Preparing resources for our AM container
19/12/13 02:00:06 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
19/12/13 02:00:09 INFO yarn.Client: Uploading resource file:/tmp/spark-20361a15-ea86-4df2-9cbd-33580dd2df88/__spark_libs__5941137393330066503.zip -> hdfs://cootek/user/ad_user/.sparkStaging/application_1573729450353_315921/__spark_libs__5941137393330066503.zip
19/12/13 02:00:12 INFO yarn.Client: Uploading resource file:/usr/local/spark-2.1.1-bin-hadoop2.6/python/lib/pyspark.zip -> hdfs://cootek/user/ad_user/.sparkStaging/application_1573729450353_315921/pyspark.zip
19/12/13 02:00:12 INFO yarn.Client: Uploading resource file:/usr/local/spark-2.1.1-bin-hadoop2.6/python/lib/py4j-0.10.7-src.zip -> hdfs://cootek/user/ad_user/.sparkStaging/application_1573729450353_315921/py4j-0.10.7-src.zip
19/12/13 02:00:12 INFO yarn.Client: Uploading resource file:/tmp/spark-20361a15-ea86-4df2-9cbd-33580dd2df88/__spark_conf__2950845159599359916.zip -> hdfs://cootek/user/ad_user/.sparkStaging/application_1573729450353_315921/__spark_conf__.zip
19/12/13 02:00:12 INFO spark.SecurityManager: Changing view acls to: ad_user
19/12/13 02:00:12 INFO spark.SecurityManager: Changing modify acls to: ad_user
19/12/13 02:00:12 INFO spark.SecurityManager: Changing view acls groups to: 
19/12/13 02:00:12 INFO spark.SecurityManager: Changing modify acls groups to: 
19/12/13 02:00:12 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ad_user); groups with view permissions: Set(); users  with modify permissions: Set(ad_user); groups with modify permissions: Set()
19/12/13 02:00:13 INFO security.HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-396942047_18, ugi=ad_user@CORP.COOTEK.COM (auth:KERBEROS)]]
19/12/13 02:00:13 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5998067 for ad_user on ha-hdfs:cootek
19/12/13 02:00:13 WARN conf.HiveConf: HiveConf of name hive.mapred.num.threshold does not exist
19/12/13 02:00:13 WARN conf.HiveConf: HiveConf of name hive.query.history.Password does not exist
19/12/13 02:00:13 WARN conf.HiveConf: HiveConf of name hive.query.history.userName does not exist
19/12/13 02:00:13 WARN conf.HiveConf: HiveConf of name hive.query.history.connectionstring does not exist
19/12/13 02:00:14 INFO yarn.Client: Submitting application application_1573729450353_315921 to ResourceManager
19/12/13 02:00:15 INFO impl.YarnClientImpl: Submitted application application_1573729450353_315921
19/12/13 02:00:15 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1573729450353_315921 and attemptId None
19/12/13 02:00:16 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:16 INFO yarn.Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.ad-root.etl.dailyetl.high
	 start time: 1576173614905
	 final status: UNDEFINED
	 tracking URL: http://hadoop2-namenode.corp.cootek.com:8088/proxy/application_1573729450353_315921/
	 user: ad_user
19/12/13 02:00:17 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:18 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:19 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:20 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:21 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:22 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:23 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:24 INFO yarn.Client: Application report for application_1573729450353_315921 (state: ACCEPTED)
19/12/13 02:00:24 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hadoop2-namenode.corp.cootek.com, PROXY_URI_BASES -> http://hadoop2-namenode.corp.cootek.com:8088/proxy/application_1573729450353_315921), /proxy/application_1573729450353_315921
19/12/13 02:00:24 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
19/12/13 02:00:25 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
19/12/13 02:00:25 INFO yarn.Client: Application report for application_1573729450353_315921 (state: RUNNING)
19/12/13 02:00:25 INFO yarn.Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.30.226
	 ApplicationMaster RPC port: -1
	 queue: root.ad-root.etl.dailyetl.high
	 start time: 1576173614905
	 final status: UNDEFINED
	 tracking URL: http://hadoop2-namenode.corp.cootek.com:8088/proxy/application_1573729450353_315921/
	 user: ad_user
19/12/13 02:00:25 INFO cluster.YarnClientSchedulerBackend: Application application_1573729450353_315921 has started running.
19/12/13 02:00:25 INFO cluster.YarnScheduler: Starting speculative execution thread
19/12/13 02:00:25 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24425.
19/12/13 02:00:25 INFO netty.NettyBlockTransferService: Server created on 192.168.0.30:24425
19/12/13 02:00:25 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/12/13 02:00:25 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.30, 24425, None)
19/12/13 02:00:25 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.30:24425 with 6.9 GB RAM, BlockManagerId(driver, 192.168.0.30, 24425, None)
19/12/13 02:00:25 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.30, 24425, None)
19/12/13 02:00:25 INFO storage.BlockManager: external shuffle service port = 7337
19/12/13 02:00:25 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.30, 24425, None)
19/12/13 02:00:25 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
19/12/13 02:00:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78392d07{/metrics/json,null,AVAILABLE,@Spark}
19/12/13 02:00:25 INFO scheduler.EventLoggingListener: Logging events to hdfs:/user/ad_user/spark_eventlog/application_1573729450353_315921
19/12/13 02:00:25 WARN util.Utils: spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
19/12/13 02:00:25 INFO util.Utils: Using initial executors = 50, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
19/12/13 02:00:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.218:28100) with ID 29
19/12/13 02:00:31 INFO spark.ExecutorAllocationManager: New executor 29 has registered (new total is 1)
19/12/13 02:00:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.73:52808) with ID 45
19/12/13 02:00:31 INFO spark.ExecutorAllocationManager: New executor 45 has registered (new total is 2)
19/12/13 02:00:31 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0126.corp.cootek.com:31883 with 4.5 GB RAM, BlockManagerId(29, hadoop-datanode-0126.corp.cootek.com, 31883, None)
19/12/13 02:00:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.219:57522) with ID 30
19/12/13 02:00:31 INFO spark.ExecutorAllocationManager: New executor 30 has registered (new total is 3)
19/12/13 02:00:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.208:64140) with ID 49
19/12/13 02:00:31 INFO spark.ExecutorAllocationManager: New executor 49 has registered (new total is 4)
19/12/13 02:00:31 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0105.corp.cootek.com:37759 with 4.5 GB RAM, BlockManagerId(45, hadoop-datanode-0105.corp.cootek.com, 37759, None)
19/12/13 02:00:31 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0129.corp.cootek.com:35609 with 4.5 GB RAM, BlockManagerId(49, hadoop-datanode-0129.corp.cootek.com, 35609, None)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0127.corp.cootek.com:20569 with 4.5 GB RAM, BlockManagerId(30, hadoop-datanode-0127.corp.cootek.com, 20569, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.231:37832) with ID 21
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 21 has registered (new total is 5)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.113:54109) with ID 19
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 19 has registered (new total is 6)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.104:22556) with ID 22
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 22 has registered (new total is 7)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0140.corp.cootek.com:33321 with 4.5 GB RAM, BlockManagerId(21, hadoop-datanode-0140.corp.cootek.com, 33321, None)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0084.corp.cootek.com:63768 with 4.5 GB RAM, BlockManagerId(19, hadoop-datanode-0084.corp.cootek.com, 63768, None)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0073.corp.cootek.com:31809 with 4.5 GB RAM, BlockManagerId(22, hadoop-datanode-0073.corp.cootek.com, 31809, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.154:34372) with ID 39
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 39 has registered (new total is 8)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0102.corp.cootek.com:14087 with 4.5 GB RAM, BlockManagerId(39, hadoop-datanode-0102.corp.cootek.com, 14087, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.33:41530) with ID 18
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 18 has registered (new total is 9)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.35:46248) with ID 27
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 27 has registered (new total is 10)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0032.corp.cootek.com:40777 with 4.5 GB RAM, BlockManagerId(18, hadoop-datanode-0032.corp.cootek.com, 40777, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.90:54952) with ID 8
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 8 has registered (new total is 11)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0034.corp.cootek.com:50329 with 4.5 GB RAM, BlockManagerId(27, hadoop-datanode-0034.corp.cootek.com, 50329, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.105:59979) with ID 17
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 17 has registered (new total is 12)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.20:44911) with ID 4
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 4 has registered (new total is 13)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.64:49584) with ID 1
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 14)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0146.corp.cootek.com:28885 with 4.5 GB RAM, BlockManagerId(8, hadoop-datanode-0146.corp.cootek.com, 28885, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.45:31742) with ID 48
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 48 has registered (new total is 15)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0074.corp.cootek.com:62509 with 4.5 GB RAM, BlockManagerId(17, hadoop-datanode-0074.corp.cootek.com, 62509, None)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0019.corp.cootek.com:58334 with 4.5 GB RAM, BlockManagerId(4, hadoop-datanode-0019.corp.cootek.com, 58334, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.49:17827) with ID 36
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 36 has registered (new total is 16)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0063.corp.cootek.com:60269 with 4.5 GB RAM, BlockManagerId(1, hadoop-datanode-0063.corp.cootek.com, 60269, None)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0044.corp.cootek.com:33000 with 4.5 GB RAM, BlockManagerId(48, hadoop-datanode-0044.corp.cootek.com, 33000, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.40:15081) with ID 9
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 9 has registered (new total is 17)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0048.corp.cootek.com:63117 with 4.5 GB RAM, BlockManagerId(36, hadoop-datanode-0048.corp.cootek.com, 63117, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.60:33424) with ID 5
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 5 has registered (new total is 18)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0039.corp.cootek.com:14219 with 4.5 GB RAM, BlockManagerId(9, hadoop-datanode-0039.corp.cootek.com, 14219, None)
19/12/13 02:00:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0059.corp.cootek.com:14344 with 4.5 GB RAM, BlockManagerId(5, hadoop-datanode-0059.corp.cootek.com, 14344, None)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.224:61608) with ID 41
19/12/13 02:00:32 INFO spark.ExecutorAllocationManager: New executor 41 has registered (new total is 19)
19/12/13 02:00:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.55:32815) with ID 15
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 15 has registered (new total is 20)
19/12/13 02:00:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0054.corp.cootek.com:53550 with 4.5 GB RAM, BlockManagerId(15, hadoop-datanode-0054.corp.cootek.com, 53550, None)
19/12/13 02:00:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0133.corp.cootek.com:24487 with 4.5 GB RAM, BlockManagerId(41, hadoop-datanode-0133.corp.cootek.com, 24487, None)
19/12/13 02:00:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.106:57805) with ID 35
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 35 has registered (new total is 21)
19/12/13 02:00:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0076.corp.cootek.com:61043 with 4.5 GB RAM, BlockManagerId(35, hadoop-datanode-0076.corp.cootek.com, 61043, None)
19/12/13 02:00:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.63:13070) with ID 43
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 43 has registered (new total is 22)
19/12/13 02:00:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.130:37274) with ID 31
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 31 has registered (new total is 23)
19/12/13 02:00:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0062.corp.cootek.com:57013 with 4.5 GB RAM, BlockManagerId(43, hadoop-datanode-0062.corp.cootek.com, 57013, None)
19/12/13 02:00:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.124:40430) with ID 32
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 32 has registered (new total is 24)
19/12/13 02:00:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.118:52834) with ID 47
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 47 has registered (new total is 25)
19/12/13 02:00:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0101.corp.cootek.com:31539 with 4.5 GB RAM, BlockManagerId(31, hadoop-datanode-0101.corp.cootek.com, 31539, None)
19/12/13 02:00:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0095.corp.cootek.com:31270 with 4.5 GB RAM, BlockManagerId(32, hadoop-datanode-0095.corp.cootek.com, 31270, None)
19/12/13 02:00:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.48:13252) with ID 50
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 50 has registered (new total is 26)
19/12/13 02:00:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0089.corp.cootek.com:20562 with 4.5 GB RAM, BlockManagerId(47, hadoop-datanode-0089.corp.cootek.com, 20562, None)
19/12/13 02:00:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0047.corp.cootek.com:23383 with 4.5 GB RAM, BlockManagerId(50, hadoop-datanode-0047.corp.cootek.com, 23383, None)
19/12/13 02:00:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.121:42032) with ID 20
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 20 has registered (new total is 27)
19/12/13 02:00:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.57:55626) with ID 38
19/12/13 02:00:33 INFO spark.ExecutorAllocationManager: New executor 38 has registered (new total is 28)
19/12/13 02:00:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.131:26650) with ID 26
19/12/13 02:00:34 INFO spark.ExecutorAllocationManager: New executor 26 has registered (new total is 29)
19/12/13 02:00:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0092.corp.cootek.com:36973 with 4.5 GB RAM, BlockManagerId(20, hadoop-datanode-0092.corp.cootek.com, 36973, None)
19/12/13 02:00:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0056.corp.cootek.com:30772 with 4.5 GB RAM, BlockManagerId(38, hadoop-datanode-0056.corp.cootek.com, 30772, None)
19/12/13 02:00:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.129:44934) with ID 34
19/12/13 02:00:34 INFO spark.ExecutorAllocationManager: New executor 34 has registered (new total is 30)
19/12/13 02:00:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0086.corp.cootek.com:24217 with 4.5 GB RAM, BlockManagerId(26, hadoop-datanode-0086.corp.cootek.com, 24217, None)
19/12/13 02:00:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.36:51152) with ID 6
19/12/13 02:00:34 INFO spark.ExecutorAllocationManager: New executor 6 has registered (new total is 31)
19/12/13 02:00:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.52:28014) with ID 42
19/12/13 02:00:34 INFO spark.ExecutorAllocationManager: New executor 42 has registered (new total is 32)
19/12/13 02:00:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.109:48096) with ID 13
19/12/13 02:00:34 INFO spark.ExecutorAllocationManager: New executor 13 has registered (new total is 33)
19/12/13 02:00:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0035.corp.cootek.com:57176 with 4.5 GB RAM, BlockManagerId(6, hadoop-datanode-0035.corp.cootek.com, 57176, None)
19/12/13 02:00:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0100.corp.cootek.com:29377 with 4.5 GB RAM, BlockManagerId(34, hadoop-datanode-0100.corp.cootek.com, 29377, None)
19/12/13 02:00:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0051.corp.cootek.com:10029 with 4.5 GB RAM, BlockManagerId(42, hadoop-datanode-0051.corp.cootek.com, 10029, None)
19/12/13 02:00:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0080.corp.cootek.com:32774 with 4.5 GB RAM, BlockManagerId(13, hadoop-datanode-0080.corp.cootek.com, 32774, None)
19/12/13 02:00:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.44:59120) with ID 2
19/12/13 02:00:34 INFO spark.ExecutorAllocationManager: New executor 2 has registered (new total is 34)
19/12/13 02:00:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0043.corp.cootek.com:38519 with 4.5 GB RAM, BlockManagerId(2, hadoop-datanode-0043.corp.cootek.com, 38519, None)
19/12/13 02:00:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.32:37516) with ID 16
19/12/13 02:00:34 INFO spark.ExecutorAllocationManager: New executor 16 has registered (new total is 35)
19/12/13 02:00:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.133:38710) with ID 28
19/12/13 02:00:34 INFO spark.ExecutorAllocationManager: New executor 28 has registered (new total is 36)
19/12/13 02:00:35 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0031.corp.cootek.com:54295 with 4.5 GB RAM, BlockManagerId(16, hadoop-datanode-0031.corp.cootek.com, 54295, None)
19/12/13 02:00:35 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0088.corp.cootek.com:31151 with 4.5 GB RAM, BlockManagerId(28, hadoop-datanode-0088.corp.cootek.com, 31151, None)
19/12/13 02:00:35 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.132:56342) with ID 44
19/12/13 02:00:35 INFO spark.ExecutorAllocationManager: New executor 44 has registered (new total is 37)
19/12/13 02:00:35 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
19/12/13 02:00:35 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0087.corp.cootek.com:32833 with 4.5 GB RAM, BlockManagerId(44, hadoop-datanode-0087.corp.cootek.com, 32833, None)
19/12/13 02:00:35 INFO internal.SharedState: loading hive config file: file:/usr/local/spark-2.4.3-bin-hadoop2.6/conf/hive-site.xml
19/12/13 02:00:35 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/data/internal').
19/12/13 02:00:35 INFO internal.SharedState: Warehouse path is '/data/internal'.
19/12/13 02:00:35 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
19/12/13 02:00:35 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37be2265{/SQL,null,AVAILABLE,@Spark}
19/12/13 02:00:35 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
19/12/13 02:00:35 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1aa73a0f{/SQL/json,null,AVAILABLE,@Spark}
19/12/13 02:00:35 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
19/12/13 02:00:35 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77260743{/SQL/execution,null,AVAILABLE,@Spark}
19/12/13 02:00:35 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
19/12/13 02:00:35 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7086d171{/SQL/execution/json,null,AVAILABLE,@Spark}
19/12/13 02:00:35 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
19/12/13 02:00:35 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1493d3a9{/static/sql,null,AVAILABLE,@Spark}
19/12/13 02:00:36 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/12/13 02:00:36 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/12/13 02:00:36 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.23:38722) with ID 24
19/12/13 02:00:36 INFO spark.ExecutorAllocationManager: New executor 24 has registered (new total is 38)
19/12/13 02:00:37 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0022.corp.cootek.com:51776 with 4.5 GB RAM, BlockManagerId(24, hadoop-datanode-0022.corp.cootek.com, 51776, None)
19/12/13 02:00:37 WARN conf.HiveConf: HiveConf of name hive.mapred.num.threshold does not exist
19/12/13 02:00:37 WARN conf.HiveConf: HiveConf of name hive.query.history.Password does not exist
19/12/13 02:00:37 WARN conf.HiveConf: HiveConf of name hive.query.history.userName does not exist
19/12/13 02:00:37 WARN conf.HiveConf: HiveConf of name hive.query.history.connectionstring does not exist
19/12/13 02:00:37 INFO hive.metastore: Trying to connect to metastore with URI thrift://cn-bdp-worker01.corp.cootek.com:9083
19/12/13 02:00:37 INFO hive.metastore: Connected to metastore.
19/12/13 02:00:38 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.54:38448) with ID 14
19/12/13 02:00:38 INFO spark.ExecutorAllocationManager: New executor 14 has registered (new total is 39)
19/12/13 02:00:38 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0053.corp.cootek.com:57410 with 4.5 GB RAM, BlockManagerId(14, hadoop-datanode-0053.corp.cootek.com, 57410, None)
19/12/13 02:00:38 INFO session.SessionState: Created local directory: /tmp/cc4ab12a-00f2-4aa5-ba76-658f53738138_resources
19/12/13 02:00:38 INFO session.SessionState: Created HDFS directory: /tmp/hive-scratchdir/ad_user/cc4ab12a-00f2-4aa5-ba76-658f53738138
19/12/13 02:00:38 INFO session.SessionState: Created local directory: /tmp/ad_user/cc4ab12a-00f2-4aa5-ba76-658f53738138
19/12/13 02:00:38 INFO session.SessionState: Created HDFS directory: /tmp/hive-scratchdir/ad_user/cc4ab12a-00f2-4aa5-ba76-658f53738138/_tmp_space.db
19/12/13 02:00:38 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /data/internal
19/12/13 02:00:38 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.28:58337) with ID 3
19/12/13 02:00:38 INFO spark.ExecutorAllocationManager: New executor 3 has registered (new total is 40)
19/12/13 02:00:39 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0027.corp.cootek.com:29092 with 4.5 GB RAM, BlockManagerId(3, hadoop-datanode-0027.corp.cootek.com, 29092, None)
19/12/13 02:00:40 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
19/12/13 02:00:40 INFO codegen.CodeGenerator: Code generated in 225.397516 ms
19/12/13 02:00:40 INFO codegen.CodeGenerator: Code generated in 17.585729 ms
19/12/13 02:00:41 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 374.4 KB, free 6.9 GB)
19/12/13 02:00:41 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.9 KB, free 6.9 GB)
19/12/13 02:00:41 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.30:24425 (size: 30.9 KB, free: 6.9 GB)
19/12/13 02:00:41 INFO spark.SparkContext: Created broadcast 0 from 
19/12/13 02:00:44 WARN conf.HiveConf: HiveConf of name hive.mapred.num.threshold does not exist
19/12/13 02:00:44 WARN conf.HiveConf: HiveConf of name hive.query.history.Password does not exist
19/12/13 02:00:44 WARN conf.HiveConf: HiveConf of name hive.query.history.userName does not exist
19/12/13 02:00:44 WARN conf.HiveConf: HiveConf of name hive.query.history.connectionstring does not exist
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645942 end=1576173645947 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645942 end=1576173645948 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645942 end=1576173645948 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645942 end=1576173645948 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645942 end=1576173645947 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645942 end=1576173645948 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645960 end=1576173645961 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645962 end=1576173645964 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645967 end=1576173645968 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645967 end=1576173645969 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645974 end=1576173645974 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645974 end=1576173645975 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645976 end=1576173645977 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645979 end=1576173645980 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645981 end=1576173645982 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645981 end=1576173645982 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645983 end=1576173645984 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645985 end=1576173645986 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645985 end=1576173645986 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645989 end=1576173645991 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645989 end=1576173645992 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645993 end=1576173645995 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645993 end=1576173645996 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:45 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645997 end=1576173645999 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:45 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173645997 end=1576173645999 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646001 end=1576173646003 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646001 end=1576173646005 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646005 end=1576173646006 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646008 end=1576173646012 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646008 end=1576173646013 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646013 end=1576173646014 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646016 end=1576173646018 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646016 end=1576173646020 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646020 end=1576173646021 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646022 end=1576173646025 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646022 end=1576173646026 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646022 end=1576173646027 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646028 end=1576173646032 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646028 end=1576173646033 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646035 end=1576173646039 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646035 end=1576173646040 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646042 end=1576173646045 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646042 end=1576173646046 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646048 end=1576173646052 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646053 end=1576173646058 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646059 end=1576173646064 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646064 end=1576173646069 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646070 end=1576173646074 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646075 end=1576173646080 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646081 end=1576173646085 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646086 end=1576173646091 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646091 end=1576173646095 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646096 end=1576173646100 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646101 end=1576173646105 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646106 end=1576173646110 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646111 end=1576173646115 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646116 end=1576173646121 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646122 end=1576173646127 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646128 end=1576173646149 duration=21 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646149 end=1576173646150 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646149 end=1576173646151 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646149 end=1576173646150 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646149 end=1576173646150 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646149 end=1576173646151 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646153 end=1576173646153 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646154 end=1576173646157 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646154 end=1576173646158 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646159 end=1576173646159 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646160 end=1576173646160 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646161 end=1576173646164 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646164 end=1576173646165 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646164 end=1576173646165 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646167 end=1576173646169 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646169 end=1576173646170 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646171 end=1576173646171 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646172 end=1576173646173 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646174 end=1576173646174 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646175 end=1576173646177 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646178 end=1576173646178 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646178 end=1576173646179 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646180 end=1576173646182 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646180 end=1576173646183 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646184 end=1576173646184 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646184 end=1576173646188 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646184 end=1576173646188 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646189 end=1576173646193 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646189 end=1576173646194 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646194 end=1576173646198 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646199 end=1576173646204 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646204 end=1576173646209 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646209 end=1576173646210 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646211 end=1576173646223 duration=12 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646224 end=1576173646228 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646228 end=1576173646232 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646233 end=1576173646235 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646236 end=1576173646237 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646237 end=1576173646240 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646240 end=1576173646242 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646243 end=1576173646245 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646246 end=1576173646248 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646248 end=1576173646250 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646251 end=1576173646255 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646251 end=1576173646255 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646251 end=1576173646255 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646257 end=1576173646260 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646257 end=1576173646261 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646257 end=1576173646261 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646262 end=1576173646263 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646264 end=1576173646266 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646268 end=1576173646268 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646269 end=1576173646273 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646269 end=1576173646274 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646275 end=1576173646278 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646275 end=1576173646279 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646280 end=1576173646284 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646286 end=1576173646288 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646286 end=1576173646289 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646290 end=1576173646294 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646295 end=1576173646299 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646300 end=1576173646319 duration=19 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646320 end=1576173646325 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646326 end=1576173646331 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646331 end=1576173646340 duration=9 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646341 end=1576173646346 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646347 end=1576173646352 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646354 end=1576173646359 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646361 end=1576173646366 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646367 end=1576173646381 duration=14 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646383 end=1576173646385 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646386 end=1576173646388 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646389 end=1576173646391 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646389 end=1576173646391 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646392 end=1576173646394 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646395 end=1576173646396 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646395 end=1576173646397 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646398 end=1576173646401 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646402 end=1576173646403 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646404 end=1576173646408 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646408 end=1576173646409 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646409 end=1576173646414 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646415 end=1576173646417 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646418 end=1576173646424 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646425 end=1576173646440 duration=15 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646441 end=1576173646445 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646445 end=1576173646449 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646450 end=1576173646453 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646454 end=1576173646454 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646455 end=1576173646459 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646455 end=1576173646459 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646460 end=1576173646464 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646465 end=1576173646470 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646470 end=1576173646474 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646475 end=1576173646475 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646476 end=1576173646478 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646479 end=1576173646479 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646480 end=1576173646481 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646482 end=1576173646483 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646484 end=1576173646484 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646485 end=1576173646486 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646487 end=1576173646487 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646487 end=1576173646488 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646489 end=1576173646491 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646492 end=1576173646492 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646493 end=1576173646511 duration=18 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646512 end=1576173646516 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646517 end=1576173646520 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646521 end=1576173646526 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646521 end=1576173646526 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646526 end=1576173646530 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646526 end=1576173646542 duration=16 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646543 end=1576173646549 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646543 end=1576173646574 duration=31 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646575 end=1576173646579 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646580 end=1576173646590 duration=10 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646591 end=1576173646596 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646591 end=1576173646625 duration=34 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646591 end=1576173646683 duration=92 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646684 end=1576173646762 duration=78 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646763 end=1576173646768 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646769 end=1576173646775 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646776 end=1576173646781 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 29
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646782 end=1576173646895 duration=113 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646896 end=1576173646901 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646902 end=1576173646907 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646909 end=1576173646914 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646915 end=1576173646920 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 30, 45, 49
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/1
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646915 end=1576173646950 duration=35 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646951 end=1576173646956 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646957 end=1576173646962 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646964 end=1576173646968 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646969 end=1576173646974 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:46 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173646975 end=1576173646980 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:46 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
19/12/13 02:00:47 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/12/13 02:00:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
19/12/13 02:00:47 INFO scheduler.DAGScheduler: Parents of final stage: List()
19/12/13 02:00:47 INFO scheduler.DAGScheduler: Missing parents: List()
19/12/13 02:00:47 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[580] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
19/12/13 02:00:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 21, 22, 19
19/12/13 02:00:47 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 187.6 KB, free 6.9 GB)
19/12/13 02:00:47 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 38.0 KB, free 6.9 GB)
19/12/13 02:00:47 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.30:24425 (size: 38.0 KB, free: 6.9 GB)
19/12/13 02:00:47 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
19/12/13 02:00:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[580] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/12/13 02:00:47 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks
19/12/13 02:00:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 39
19/12/13 02:00:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 0, NODE_LOCAL, 8086 bytes)
19/12/13 02:00:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 27, 18
19/12/13 02:00:47 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-datanode-0105.corp.cootek.com:37759 (size: 38.0 KB, free: 4.5 GB)
19/12/13 02:00:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 8
19/12/13 02:00:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 17, 1, 4
19/12/13 02:00:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 48, 36
19/12/13 02:00:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 9
19/12/13 02:00:47 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-datanode-0105.corp.cootek.com:37759 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:00:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 5
19/12/13 02:00:48 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 15, 41
19/12/13 02:00:48 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 35
19/12/13 02:00:48 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 43
19/12/13 02:00:48 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 32, 31
19/12/13 02:00:48 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 47
19/12/13 02:00:48 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 50
19/12/13 02:00:48 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 20
19/12/13 02:00:49 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 38
19/12/13 02:00:49 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 26
19/12/13 02:00:49 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 6, 34
19/12/13 02:00:49 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 42, 13
19/12/13 02:00:49 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 2
19/12/13 02:00:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2396 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (1/1)
19/12/13 02:00:49 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/12/13 02:00:49 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.525 s
19/12/13 02:00:49 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.655710 s
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+--------+
|extra                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |time          |src     |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+--------+
|{"promoted_app":"","city_id":"57400","osv":"9000","applist_len":"0","ad_closed":false,"adw":"180","tu":"104823","track_type":"0","adh":"180","adpctr":"0.036906","bid":"0.050000","didmd5":"d41d8cd98f00b204e9800998ecf8427e","dv":"1","dt":"1","impid":"ba5f218d-4fa7-4c4e-bf18-fd921e8366f9-4","did":"","gender":"0","campaignid":"C634s91mo332rxlw","planid":"P924mbdxrenodjn5","org_type":"2","ifa":"7b61b1324ca635ae","appid":"54477aa2185881ba98acc651c9065a42","identifier":"13d243b3885a871c65495c57a45f48a7","adtarget":"1","didsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","ad_style":"5","req_style":"5","use_at":"false","reqid":"ba5f218d-4fa7-4c4e-bf18-fd921e8366f9","cost":0.05,"bid_floor":"0.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"","profitability":"1.743200","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.061427","ocpc_bid":"0.000000","bu":"2","dpid":"7b61b1324ca635ae","is_ocpc":"true","lan_t":"3","app_series":"2","industry":"111114100","target_cpa":"0.000000","pctr_cal":"0.036906","support_traffic":"1","at_edkey":"","operator":"46003","instl":"0","p_name":"浮标-203169-CPC","pw":"0","spam":0,"adx_ecpm":"1.743200","sdk_version":"0.3.0","traffic_pool":"0","ip":"125.111.200.243","ph":"0","user_plan_clkfreq":"0","publisher_id":"10020","userid":"7b61b1324ca635ae","reqprt":"1575662430777","phone":"","bundle_id":"com.hunting.matrix_showcaller","fac_ts":"1575616748","is_native":"1","cash":0.05,"triggerd_expids":"6302_6403_6410_6603_7203_7302","cmod":"1","site_id":"","adecpm":"2.266160","customer_support_revenue":"0.000000","orgid":"130","max_bid_floor":"0.000000","latency":2050643,"make":"VIVO","track_id":"unknown","con_t":"0","dpidsha1":"e77472ed18d4ceff43e888f2231820bb778dbc6a","transform_type":"0","plid":"9e71a7a1073b204610b6093c8d6a97d2","user_plan_edfreq":"0","nt":"1","macsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","gsp_price":"0.000000","macmd5":"d41d8cd98f00b204e9800998ecf8427e","user_edclk":"-1_-1","mac":"","adid":"A696h7y0dv3adprh","reqtype":"1","date":"2019-12-7","dpidmd5":"711776c687397199ef24c14ccd20ea4c","os":"1","age":"0","cash_ecpm":"1743.199706","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:0:43","model":"V1829A"}                                                                                                       |20191207040043|DSPCLICK|
|{"promoted_app":"","city_id":"2802","osv":"9000","applist_len":"0","ad_closed":false,"adw":"720","tu":"4408","track_type":"0","adh":"1280","adpctr":"0.143366","bid":"1.450000","didmd5":"d41d8cd98f00b204e9800998ecf8427e","dv":"1","dt":"1","impid":"66875b02-1027-4024-9c5a-314ec0706457-16","did":"","gender":"0","campaignid":"C2936dbej9m31zpz","planid":"P481wskvr2i230z1","org_type":"2","ifa":"334398bbb1107aa8","appid":"14eff348a331221f07e90caf8c9ae697","identifier":"48f5f58c061d942c5f4b989e21cb45d3","adtarget":"1","didsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","ad_style":"4","req_style":"4","use_at":"true","reqid":"66875b02-1027-4024-9c5a-314ec0706457","cost":0.92417,"bid_floor":"125.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"4","profitability":"0.000000","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.032753","ocpc_bid":"0.000000","bu":"1","dpid":"334398bbb1107aa8","is_ocpc":"true","lan_t":"3","app_series":"4","industry":"102101100","target_cpa":"0.000000","pctr_cal":"0.143366","support_traffic":"1","at_edkey":"at_edinfo:3:4:4:102101100:20191207","operator":"46000","instl":"0","p_name":"新激励视频 - copy","pw":"720","spam":0,"adx_ecpm":"125.000000","sdk_version":"0.2.3","traffic_pool":"0","ip":"183.222.110.4","ph":"1280","user_plan_clkfreq":"0","publisher_id":"10019","userid":"334398bbb1107aa8","reqprt":"1575662393606","phone":"","bundle_id":"com.feka.games.hi.sushimaster.chef.cooking.merge.free.android","fac_ts":"0","is_native":"0","cash":0.92417,"triggerd_expids":"6301_6402_6410_6603_6702_7203_7301","cmod":"1","site_id":"","adecpm":"139.237469","customer_support_revenue":"17.894254","orgid":"2180","max_bid_floor":"125.000000","latency":1946574,"make":"XIAOMI","track_id":"unknown","con_t":"0","dpidsha1":"7537879c7593006cd65f7387ac3fb61af64ddde5","transform_type":"0","plid":"75b6c0815664be0d4c03b599f2cb7e9d","user_plan_edfreq":"0","nt":"1","macsha1":"56d9377814cf64856cd041648849df04112d8eb0","gsp_price":"0.924170","macmd5":"f804544151d7d47a608d5e739ea6ab2d","user_edclk":"-1_-1","mac":"18:01:F1:68:11:90","adid":"A492tcg3qyfjmn61","reqtype":"3","date":"2019-12-7","dpidmd5":"83ffb6c858d8850b8d251150dcc3012c","os":"1","age":"0","cash_ecpm":"107105.745570","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:1:3","model":"Redmi 7"}|20191207040103|DSPCLICK|
|{"promoted_app":"","city_id":"41200","osv":"7110","applist_len":"0","ad_closed":false,"adw":"720","tu":"4374","track_type":"0","adh":"1280","adpctr":"0.110494","bid":"1.450000","didmd5":"d41d8cd98f00b204e9800998ecf8427e","dv":"1","dt":"1","impid":"0bce14ae-cf90-4bbb-98c9-0afb94ebf798-16","did":"","gender":"0","campaignid":"C533sm539hbc1z1f","planid":"P21274cwyvm3zona","org_type":"2","ifa":"1896fc350319ff9","appid":"4b4307ffe0d9bcef86316307bff64ba6","identifier":"52052c22fca1b79180d612e0f504b4bc","adtarget":"1","didsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","ad_style":"4","req_style":"4","use_at":"true","reqid":"0bce14ae-cf90-4bbb-98c9-0afb94ebf798","cost":1.45,"bid_floor":"125.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"4","profitability":"0.000000","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.043349","ocpc_bid":"0.000000","bu":"1","dpid":"1896fc350319ff9","is_ocpc":"false","lan_t":"3","app_series":"4","industry":"102101100","target_cpa":"0.000000","pctr_cal":"0.110494","support_traffic":"1","at_edkey":"at_edinfo:3:4:4:102101100:20191207","operator":"46001","instl":"0","p_name":"新激励视频 - copy","pw":"720","spam":0,"adx_ecpm":"125.000000","sdk_version":"0.2.3","traffic_pool":"0","ip":"112.40.112.14","ph":"1280","user_plan_clkfreq":"0","publisher_id":"10019","userid":"1896fc350319ff9","reqprt":"1575662533137","phone":"","bundle_id":"feka.games.chargerent.merge.home.earn.money.android","fac_ts":"0","is_native":"0","cash":1.45,"triggerd_expids":"6302_6404_6410_6603_7201_7302","cmod":"1","site_id":"","adecpm":"160.216738","customer_support_revenue":"1.756356","orgid":"2499","max_bid_floor":"125.000000","latency":2197656,"make":"OPPO","track_id":"unknown","con_t":"0","dpidsha1":"270ccbcd153e51bdec869c47411e3dd546fd5186","transform_type":"0","plid":"e927ee198fb913461f503eff146011bf","user_plan_edfreq":"0","nt":"1","macsha1":"65ca1c0e91ca06200fdd60980a8f63aeb01b9f80","gsp_price":"0.000000","macmd5":"b84a0536bb00888c872aba837f9feb44","user_edclk":"-1_-1","mac":"DC:55:83:27:B5:0F","adid":"A226gqbfpdbmclnj","reqtype":"3","date":"2019-12-7","dpidmd5":"ec0324e5b0ba3a44b11a7539b5362016","os":"1","age":"0","cash_ecpm":"123243.644470","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:3:17","model":"OPPO R11s Plus"}                 |20191207040317|DSPCLICK|
|{"promoted_app":"","city_id":"37000","osv":"8100","applist_len":"0","ad_closed":false,"adw":"180","tu":"202025#202030","track_type":"0","adh":"180","adpctr":"0.010325","bid":"0.060000","didmd5":"c84a28c4b4ac9e6265dc2f8457ec34eb","dv":"1","dt":"1","impid":"13d56de0-c807-473b-866e-f3decada1391-4","did":"869902045534952","gender":"0","campaignid":"C5245drewyzkn6th","planid":"P7822w8uwdif9vuy","org_type":"1","ifa":"8f0687e2b81d8483","appid":"bc712539bcb19563a544755fe892d167","identifier":"fb980b34618c2345e50959e4e6cce1d5","adtarget":"1","didsha1":"7318f00395bec0dca4f5450d0a9dd206bc6199a2","ad_style":"5","req_style":"5","use_at":"false","reqid":"13d56de0-c807-473b-866e-f3decada1391","cost":0.06,"bid_floor":"0.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"","profitability":"0.458877","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.000000","ocpc_bid":"0.000000","bu":"2","dpid":"8f0687e2b81d8483","is_ocpc":"false","lan_t":"3","app_series":"3","industry":"105104100","target_cpa":"0.000000","pctr_cal":"0.010325","support_traffic":"1","at_edkey":"","operator":"46000","instl":"0","p_name":"小说系/34f10f5734e54330/刮奖--0.06/11.28 ","pw":"0","spam":0,"adx_ecpm":"0.458877","sdk_version":"","traffic_pool":"1","ip":"223.89.235.21","ph":"0","user_plan_clkfreq":"0","publisher_id":"10016","userid":"8f0687e2b81d8483","reqprt":"1575662688637","phone":"+8618272663183","bundle_id":"com.cootek.crazyreader","fac_ts":"1573506162","is_native":"1","cash":0.06,"triggerd_expids":"6302_6401_6410_6601_7201_7301","cmod":"1","site_id":"","adecpm":"0.619484","customer_support_revenue":"0.000000","orgid":"1638","max_bid_floor":"0.000000","latency":2172210,"make":"VIVO","track_id":"unknown","con_t":"0","dpidsha1":"abb102dad66524eb8178d1a7bd39e52928a53fdd","transform_type":"0","plid":"fa8ab8bd29bd2fcf6272d8d3e0167c0f","user_plan_edfreq":"0","nt":"1","macsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","gsp_price":"0.000000","macmd5":"d41d8cd98f00b204e9800998ecf8427e","user_edclk":"252_72","mac":"","adid":"A792p0d30p2eoeey","reqtype":"1","date":"2019-12-7","dpidmd5":"f9f33c29d38f06ecaccf0ac6854faa19","os":"1","age":"0","cash_ecpm":"458.876708","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:4:51","model":"V1809A"}                                                     |20191207040451|DSPCLICK|
|{"promoted_app":"","city_id":"37700","osv":"9000","applist_len":"265","ad_closed":false,"adw":"180","tu":"202025#202030","track_type":"0","adh":"180","adpctr":"0.010142","bid":"0.060000","didmd5":"d41d8cd98f00b204e9800998ecf8427e","dv":"1","dt":"1","impid":"9716466a-509c-4e71-9ea6-6800f619387a-4","did":"","gender":"2","campaignid":"C5245drewyzkn6th","planid":"P223q8ll4d19dxa9","org_type":"1","ifa":"84e3d06722bbc641","appid":"bc712539bcb19563a544755fe892d167","identifier":"852331ac2ed699bb1afe28411dc44107","adtarget":"1","didsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","ad_style":"5","req_style":"5","use_at":"false","reqid":"9716466a-509c-4e71-9ea6-6800f619387a","cost":0.06,"bid_floor":"0.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"","profitability":"0.450758","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.000000","ocpc_bid":"0.000000","bu":"2","dpid":"84e3d06722bbc641","is_ocpc":"false","lan_t":"3","app_series":"3","industry":"105104100","target_cpa":"0.000000","pctr_cal":"0.010142","support_traffic":"1","at_edkey":"","operator":"46000","instl":"0","p_name":"小说系/34f10f5734e54330/签1--0.06/11.27","pw":"0","spam":0,"adx_ecpm":"0.450758","sdk_version":"","traffic_pool":"1","ip":"117.136.36.79","ph":"0","user_plan_clkfreq":"0","publisher_id":"10016","userid":"84e3d06722bbc641","reqprt":"1575662691832","phone":"+8613619741788","bundle_id":"com.cootek.crazyreader","fac_ts":"1574376805","is_native":"1","cash":0.06,"triggerd_expids":"6302_6404_6410_6603_7203_7301","cmod":"1","site_id":"","adecpm":"0.608523","customer_support_revenue":"0.000000","orgid":"1638","max_bid_floor":"0.000000","latency":2023917,"make":"OPPO","track_id":"unknown","con_t":"0","dpidsha1":"3516d65d4904878a5b9533eb2e99a43d94a43ba6","transform_type":"0","plid":"fa8ab8bd29bd2fcf6272d8d3e0167c0f","user_plan_edfreq":"0","nt":"3","macsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","gsp_price":"0.000000","macmd5":"d41d8cd98f00b204e9800998ecf8427e","user_edclk":"-1_-1","mac":"","adid":"A2335lwyfnmz8r85","reqtype":"1","date":"2019-12-7","dpidmd5":"93b23aeb03390a1e689bdf75a337f933","os":"1","age":"5","cash_ecpm":"450.757528","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:4:59","model":"PCAM10"}                                                                     |20191207040459|DSPCLICK|
|{"promoted_app":"","city_id":"76900","osv":"5100","applist_len":"0","ad_closed":false,"adw":"720","tu":"4374","track_type":"0","adh":"1280","adpctr":"0.130580","bid":"1.080000","didmd5":"1a31d823a2e441188fc142d3ba762c40","dv":"1","dt":"1","impid":"d759212d-a73f-432c-9e2e-7dd4c461fb15-16","did":"867931021085798","gender":"0","campaignid":"C460qnnelu5ozags","planid":"P727jewcr9pynrn1","org_type":"2","ifa":"51167c41f220eb2b","appid":"4b4307ffe0d9bcef86316307bff64ba6","identifier":"ae006034dfc9755602892d285a2bc1cf","adtarget":"1","didsha1":"9c9df006ebeea5ac089fb2586c355fd70feb63b4","ad_style":"4","req_style":"4","use_at":"true","reqid":"d759212d-a73f-432c-9e2e-7dd4c461fb15","cost":1.08,"bid_floor":"125.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"4","profitability":"0.000000","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.023364","ocpc_bid":"0.000000","bu":"1","dpid":"51167c41f220eb2b","is_ocpc":"false","lan_t":"3","app_series":"4","industry":"102101100","target_cpa":"0.000000","pctr_cal":"0.130580","support_traffic":"1","at_edkey":"at_edinfo:3:4:4:102101100:20191207","operator":"46000","instl":"0","p_name":"激励竖屏","pw":"720","spam":0,"adx_ecpm":"125.000000","sdk_version":"0.2.3","traffic_pool":"0","ip":"223.74.173.168","ph":"1280","user_plan_clkfreq":"0","publisher_id":"10019","userid":"51167c41f220eb2b","reqprt":"1575662629517","phone":"","bundle_id":"feka.games.chargerent.merge.home.earn.money.android","fac_ts":"0","is_native":"0","cash":1.08,"triggerd_expids":"6303_6404_6410_6603_7201_7301","cmod":"1","site_id":"","adecpm":"141.026120","customer_support_revenue":"16.518370","orgid":"1888","max_bid_floor":"125.000000","latency":1854673,"make":"HUAWEI","track_id":"unknown","con_t":"0","dpidsha1":"e1083c6bf364938490de8302b2cdc52eb269d7af","transform_type":"0","plid":"e927ee198fb913461f503eff146011bf","user_plan_edfreq":"0","nt":"1","macsha1":"726281471e4701a4058a61183f47e14e0ab4f813","gsp_price":"0.000000","macmd5":"3903977fefe51256ecf160c6ed4f6466","user_edclk":"54_22","mac":"7C:B1:5D:80:9B:88","adid":"A643hstjp7v274c2","reqtype":"3","date":"2019-12-7","dpidmd5":"507afeeef5a188e58130040ebf184b54","os":"1","age":"0","cash_ecpm":"108481.630411","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:5:16","model":"HUAWEI TAG-AL00"}   |20191207040516|DSPCLICK|
|{"promoted_app":"","city_id":"39400","osv":"8100","applist_len":"0","ad_closed":false,"adw":"720","tu":"202016","track_type":"0","adh":"1280","adpctr":"0.115338","bid":"0.650000","didmd5":"eafd3e93ec8e5c0b2276297328d5f837","dv":"1","dt":"1","impid":"012603ff-4845-49c4-8133-3c0eb441fab8-2","did":"865906048486957","gender":"0","campaignid":"C3935mexr1cvvh25","planid":"P72170bmkhekd2o4","org_type":"2","ifa":"4a6073e5bbbad22d","appid":"bc712539bcb19563a544755fe892d167","identifier":"524dd667c7d4e6d117416a4e5a3637f6","adtarget":"1","didsha1":"bd7e22d0a088d5eae7873f9ad48d0b1251b4cb48","ad_style":"7","req_style":"7","use_at":"false","reqid":"012603ff-4845-49c4-8133-3c0eb441fab8","cost":0,"bid_floor":"0.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"4","profitability":"0.000000","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.064757","ocpc_bid":"0.000000","bu":"2","dpid":"4a6073e5bbbad22d","is_ocpc":"true","lan_t":"3","app_series":"3","industry":"102101100","target_cpa":"0.000000","pctr_cal":"0.115338","support_traffic":"1","at_edkey":"","operator":"46001","instl":"1","p_name":"插屏视频","pw":"720","spam":2,"adx_ecpm":"76.780000","sdk_version":"0.2.2","traffic_pool":"0","ip":"222.136.178.221","ph":"1280","user_plan_clkfreq":"0","publisher_id":"10016","userid":"4a6073e5bbbad22d","reqprt":"1575662685126","phone":"+8617633956842","bundle_id":"com.cootek.crazyreader","fac_ts":"1575622756","is_native":"1","cash":0,"triggerd_expids":"6302_6403_6410_6603_6806_7203_7302","cmod":"1","site_id":"","adecpm":"96.025578","customer_support_revenue":"2.914171","orgid":"1733","max_bid_floor":"76.780000","latency":448641,"make":"OPPO","track_id":"unknown","con_t":"0","dpidsha1":"56946afc5a3750372762191c3bc1f1130901a2fb","transform_type":"0","plid":"48d0534bb387ebef834f561761086a62","user_plan_edfreq":"0","nt":"1","macsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","gsp_price":"0.000000","macmd5":"d41d8cd98f00b204e9800998ecf8427e","user_edclk":"69_0","mac":"","adid":"A892tu3gb9i22lwu","reqtype":"1_3","date":"2019-12-7","dpidmd5":"4b95109766057aeaa76aa4b52c615801","os":"1","age":"0","cash_ecpm":"73865.829006","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:5:24","model":"PACT00"}                                                                             |20191207040524|DSPCLICK|
|{"promoted_app":"","city_id":"2801","osv":"8100","applist_len":"0","ad_closed":false,"adw":"720","tu":"4374","track_type":"0","adh":"1280","adpctr":"0.166395","bid":"1.360000","didmd5":"d41d8cd98f00b204e9800998ecf8427e","dv":"1","dt":"1","impid":"0daca6e1-2e17-48ce-8745-4d6ae6d98431-16","did":"","gender":"0","campaignid":"C8634h665nb60dxk","planid":"P241lbxc9czc8hql","org_type":"2","ifa":"a7897498bd155469","appid":"4b4307ffe0d9bcef86316307bff64ba6","identifier":"4b7b0d13cf08a1aaa936373b26ae82af","adtarget":"1","didsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","ad_style":"4","req_style":"4","use_at":"true","reqid":"0daca6e1-2e17-48ce-8745-4d6ae6d98431","cost":1.36,"bid_floor":"125.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"","profitability":"49.074608","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.030933","ocpc_bid":"0.000000","bu":"1","dpid":"a7897498bd155469","is_ocpc":"false","lan_t":"3","app_series":"4","industry":"102101100","target_cpa":"0.000000","pctr_cal":"0.166395","support_traffic":"1","at_edkey":"at_edinfo:3:4:4:102101100:20191207","operator":"46003","instl":"0","p_name":"激励竖版","pw":"720","spam":0,"adx_ecpm":"174.074608","sdk_version":"0.2.3","traffic_pool":"2","ip":"171.209.164.68","ph":"1280","user_plan_clkfreq":"0","publisher_id":"10019","userid":"a7897498bd155469","reqprt":"1575662571638","phone":"","bundle_id":"feka.games.chargerent.merge.home.earn.money.android","fac_ts":"0","is_native":"0","cash":1.36,"triggerd_expids":"6302_6404_6410_6601_7201_7302","cmod":"1","site_id":"","adecpm":"226.296991","customer_support_revenue":"0.000000","orgid":"1652","max_bid_floor":"125.000000","latency":1516455,"make":"OPPO","track_id":"unknown","con_t":"0","dpidsha1":"8ccd98a5d92cb29b3880ba46b8062c7d14955b8a","transform_type":"0","plid":"e927ee198fb913461f503eff146011bf","user_plan_edfreq":"0","nt":"4","macsha1":"e7db49e99802d92115642f513849fc35aea32474","gsp_price":"0.000000","macmd5":"1680a95f4ab48442950c1b9b342fcf9a","user_edclk":"-1_-1","mac":"9C:0C:DF:CF:A3:A3","adid":"A7729poj1r84h0x8","reqtype":"3","date":"2019-12-7","dpidmd5":"9a29d1cba830cbb3c470aea835389675","os":"1","age":"0","cash_ecpm":"174074.608282","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:5:33","model":"PADM00"}                               |20191207040533|DSPCLICK|
|{"promoted_app":"","city_id":"41100","osv":"8100","applist_len":"0","ad_closed":false,"adw":"720","tu":"4374","track_type":"0","adh":"1280","adpctr":"0.139227","bid":"1.450000","didmd5":"d41d8cd98f00b204e9800998ecf8427e","dv":"1","dt":"1","impid":"97e52495-f769-46c0-b0d9-b3ef5e395298-16","did":"","gender":"0","campaignid":"C8109mfoj29kmhtq","planid":"P22023lc9gtzbyqu","org_type":"2","ifa":"940a6b180f8c0725","appid":"4b4307ffe0d9bcef86316307bff64ba6","identifier":"6b66f4a67bd965a5ca4937e227b83cf6","adtarget":"1","didsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","ad_style":"4","req_style":"4","use_at":"true","reqid":"97e52495-f769-46c0-b0d9-b3ef5e395298","cost":0,"bid_floor":"125.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"","profitability":"65.652401","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.061404","ocpc_bid":"0.000000","bu":"1","dpid":"940a6b180f8c0725","is_ocpc":"true","lan_t":"3","app_series":"4","industry":"102101100","target_cpa":"0.000000","pctr_cal":"0.139227","support_traffic":"1","at_edkey":"at_edinfo:3:4:4:102101100:20191207","operator":"46000","instl":"0","p_name":"激励视频","pw":"720","spam":2,"adx_ecpm":"190.652401","sdk_version":"0.2.3","traffic_pool":"0","ip":"223.104.177.61","ph":"1280","user_plan_clkfreq":"0","publisher_id":"10019","userid":"940a6b180f8c0725","reqprt":"1575662656939","phone":"","bundle_id":"feka.games.chargerent.merge.home.earn.money.android","fac_ts":"0","is_native":"0","cash":0,"triggerd_expids":"6301_6404_6410_6601_6702_7203_7302","cmod":"1","site_id":"","adecpm":"247.848122","customer_support_revenue":"0.000000","orgid":"2502","max_bid_floor":"125.000000","latency":481565,"make":"VIVO","track_id":"unknown","con_t":"0","dpidsha1":"69b5ebecd88ab2fa7ca745acdee306b139ad4ae2","transform_type":"0","plid":"e927ee198fb913461f503eff146011bf","user_plan_edfreq":"0","nt":"4","macsha1":"4609ea888cd6f3ae9820349895c29daf08e26b83","gsp_price":"1.450000","macmd5":"cf6d0d8ab758a86bc499f2d3a079881b","user_edclk":"-1_-1","mac":"70:47:E9:A3:33:A7","adid":"A248au5td972khzc","reqtype":"3","date":"2019-12-7","dpidmd5":"2137f77058f5b0193464ec69cad0577c","os":"1","age":"0","cash_ecpm":"190652.401184","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:5:56","model":"vivo X20A"}                              |20191207040556|DSPCLICK|
|{"promoted_app":"","city_id":"2100","osv":"9000","applist_len":"299","ad_closed":false,"adw":"180","tu":"202025#202030","track_type":"0","adh":"180","adpctr":"0.002519","bid":"0.270000","didmd5":"d41d8cd98f00b204e9800998ecf8427e","dv":"1","dt":"1","impid":"1b0c13b2-f383-4b6b-be79-4d77600df7e6-4","did":"","gender":"1","campaignid":"C6733sbta3qmtzab","planid":"P683yleqsn3964zs","org_type":"1","ifa":"a0d4f8c6dacc55b1","appid":"bc712539bcb19563a544755fe892d167","identifier":"c8a93b82509a6daf8ecf6146e9116872","adtarget":"1","didsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","ad_style":"5","req_style":"5","use_at":"false","reqid":"1b0c13b2-f383-4b6b-be79-4d77600df7e6","cost":0.27,"bid_floor":"0.000000","clicktype":"_CLICKTYPE_","traffic_source":"2","support_type":"","profitability":"0.503734","adxsrc":"naga","lp_host":"","support_revenue":"0.000000","adpcvr":"0.000000","ocpc_bid":"0.000000","bu":"2","dpid":"a0d4f8c6dacc55b1","is_ocpc":"false","lan_t":"3","app_series":"3","industry":"105105100","target_cpa":"0.000000","pctr_cal":"0.002519","support_traffic":"1","at_edkey":"","operator":"46000","instl":"0","p_name":"icon-小说-凤光1-05","pw":"0","spam":0,"adx_ecpm":"0.503734","sdk_version":"","traffic_pool":"1","ip":"223.104.212.119","ph":"0","user_plan_clkfreq":"0","publisher_id":"10016","userid":"a0d4f8c6dacc55b1","reqprt":"1575662812237","phone":"+8618367083237","bundle_id":"com.cootek.crazyreader","fac_ts":"1575150417","is_native":"1","cash":0.27,"triggerd_expids":"6303_6404_6410_6603_7203_7302","cmod":"1","site_id":"","adecpm":"0.680041","customer_support_revenue":"0.000000","orgid":"87","max_bid_floor":"0.000000","latency":1912193,"make":"VIVO","track_id":"unknown","con_t":"0","dpidsha1":"c2b5c73fee5102ee5e1633e65fd824ded8f4b756","transform_type":"0","plid":"fa8ab8bd29bd2fcf6272d8d3e0167c0f","user_plan_edfreq":"0","nt":"3","macsha1":"da39a3ee5e6b4b0d3255bfef95601890afd80709","gsp_price":"0.000000","macmd5":"d41d8cd98f00b204e9800998ecf8427e","user_edclk":"-1_-1","mac":"","adid":"A69342v5opmsv4vg","reqtype":"1","date":"2019-12-7","dpidmd5":"80a27ab32b12711e9ab6ae58a0c2e9b3","os":"1","age":"4","cash_ecpm":"503.734225","pcvr_cal":"0.000000","traffic_cate":"0","time":"4:6:57","model":"vivo X21UD"}                                                                                       |20191207040657|DSPCLICK|
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+--------+
only showing top 10 rows

19/12/13 02:00:49 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 16
19/12/13 02:00:49 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 373.7 KB, free 6.9 GB)
19/12/13 02:00:49 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.5 KB, free 6.9 GB)
19/12/13 02:00:49 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.30:24425 (size: 30.5 KB, free: 6.9 GB)
19/12/13 02:00:49 INFO spark.SparkContext: Created broadcast 2 from 
19/12/13 02:00:50 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 28
19/12/13 02:00:50 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 44
19/12/13 02:00:52 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 24
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 16
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 15
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 19
19/12/13 02:00:52 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.30:24425 in memory (size: 30.9 KB, free: 6.9 GB)
19/12/13 02:00:52 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on hadoop-datanode-0105.corp.cootek.com:37759 in memory (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 2
19/12/13 02:00:52 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.30:24425 in memory (size: 38.0 KB, free: 6.9 GB)
19/12/13 02:00:52 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hadoop-datanode-0105.corp.cootek.com:37759 in memory (size: 38.0 KB, free: 4.5 GB)
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 1
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 11
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 13
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 26
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 24
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 12
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 18
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 6
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 23
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 22
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 27
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 21
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 3
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 8
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 17
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 9
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 10
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 25
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 20
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 14
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 5
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 4
19/12/13 02:00:52 INFO spark.ContextCleaner: Cleaned accumulator 7
19/12/13 02:00:53 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 14
19/12/13 02:00:53 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 3
19/12/13 02:00:54 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.209:64680) with ID 12
19/12/13 02:00:54 INFO spark.ExecutorAllocationManager: New executor 12 has registered (new total is 41)
19/12/13 02:00:54 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0117.corp.cootek.com:20713 with 4.5 GB RAM, BlockManagerId(12, hadoop-datanode-0117.corp.cootek.com, 20713, None)
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654691 end=1576173654691 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654691 end=1576173654692 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654692 end=1576173654693 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654695 end=1576173654695 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654696 end=1576173654697 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654699 end=1576173654699 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654699 end=1576173654699 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654700 end=1576173654700 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654700 end=1576173654701 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654701 end=1576173654702 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654703 end=1576173654703 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654703 end=1576173654704 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654705 end=1576173654707 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654705 end=1576173654707 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654708 end=1576173654708 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654709 end=1576173654709 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654709 end=1576173654710 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654710 end=1576173654711 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654712 end=1576173654713 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654713 end=1576173654713 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654713 end=1576173654714 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654714 end=1576173654715 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654715 end=1576173654716 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654716 end=1576173654716 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654717 end=1576173654718 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654718 end=1576173654719 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654720 end=1576173654720 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654720 end=1576173654721 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654721 end=1576173654722 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654723 end=1576173654723 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654724 end=1576173654724 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654724 end=1576173654725 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654724 end=1576173654725 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654726 end=1576173654727 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654728 end=1576173654728 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654729 end=1576173654729 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654729 end=1576173654730 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654730 end=1576173654731 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654732 end=1576173654733 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654733 end=1576173654734 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654735 end=1576173654735 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654735 end=1576173654736 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654737 end=1576173654737 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654737 end=1576173654738 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654739 end=1576173654739 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654739 end=1576173654740 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654740 end=1576173654741 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654741 end=1576173654742 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654743 end=1576173654743 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654743 end=1576173654744 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654743 end=1576173654745 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654746 end=1576173654748 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654749 end=1576173654749 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654749 end=1576173654750 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654750 end=1576173654751 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654751 end=1576173654751 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654751 end=1576173654752 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654752 end=1576173654752 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654753 end=1576173654754 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654755 end=1576173654755 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654756 end=1576173654756 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654756 end=1576173654757 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654757 end=1576173654758 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654758 end=1576173654759 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654759 end=1576173654759 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654760 end=1576173654760 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654760 end=1576173654761 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654762 end=1576173654762 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654763 end=1576173654764 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654763 end=1576173654765 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654766 end=1576173654766 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654766 end=1576173654767 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654766 end=1576173654767 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654767 end=1576173654767 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654767 end=1576173654768 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654769 end=1576173654769 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654770 end=1576173654770 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654771 end=1576173654771 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654771 end=1576173654772 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654773 end=1576173654773 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654774 end=1576173654775 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654774 end=1576173654775 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654774 end=1576173654776 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654776 end=1576173654777 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654777 end=1576173654777 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654777 end=1576173654778 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654778 end=1576173654778 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654779 end=1576173654779 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654780 end=1576173654780 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654780 end=1576173654780 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654780 end=1576173654781 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654781 end=1576173654782 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654782 end=1576173654783 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654783 end=1576173654783 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654784 end=1576173654784 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654784 end=1576173654784 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654785 end=1576173654785 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654785 end=1576173654786 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654786 end=1576173654786 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654786 end=1576173654787 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654787 end=1576173654788 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654788 end=1576173654788 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654788 end=1576173654789 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654789 end=1576173654789 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654789 end=1576173654790 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654790 end=1576173654791 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654792 end=1576173654793 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654793 end=1576173654793 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654793 end=1576173654794 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654794 end=1576173654795 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654795 end=1576173654795 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654795 end=1576173654796 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654796 end=1576173654797 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654797 end=1576173654797 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654798 end=1576173654798 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654799 end=1576173654801 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654799 end=1576173654801 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654802 end=1576173654802 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654802 end=1576173654803 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654802 end=1576173654803 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654803 end=1576173654804 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654804 end=1576173654804 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654805 end=1576173654805 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654806 end=1576173654806 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654807 end=1576173654808 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654807 end=1576173654809 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654809 end=1576173654809 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654809 end=1576173654810 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654810 end=1576173654810 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654810 end=1576173654810 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654812 end=1576173654812 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654813 end=1576173654813 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654813 end=1576173654813 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654814 end=1576173654815 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654815 end=1576173654816 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654815 end=1576173654816 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654815 end=1576173654817 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654817 end=1576173654817 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654818 end=1576173654818 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654818 end=1576173654819 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654820 end=1576173654821 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654820 end=1576173654821 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654821 end=1576173654821 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654822 end=1576173654822 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654823 end=1576173654823 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654824 end=1576173654824 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654824 end=1576173654825 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654824 end=1576173654825 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654826 end=1576173654827 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654826 end=1576173654827 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654826 end=1576173654827 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654826 end=1576173654828 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654829 end=1576173654829 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654830 end=1576173654830 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654830 end=1576173654830 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654831 end=1576173654831 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654831 end=1576173654832 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654832 end=1576173654833 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654833 end=1576173654833 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654833 end=1576173654834 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654834 end=1576173654835 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654835 end=1576173654835 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654836 end=1576173654836 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654836 end=1576173654836 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654836 end=1576173654837 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654837 end=1576173654837 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654838 end=1576173654838 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654838 end=1576173654839 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654839 end=1576173654840 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654840 end=1576173654840 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654840 end=1576173654841 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654841 end=1576173654842 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654842 end=1576173654842 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654842 end=1576173654842 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654842 end=1576173654843 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654843 end=1576173654843 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654844 end=1576173654845 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654844 end=1576173654846 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654844 end=1576173654846 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654846 end=1576173654846 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654846 end=1576173654847 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654847 end=1576173654847 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654848 end=1576173654849 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654848 end=1576173654850 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654850 end=1576173654850 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654850 end=1576173654850 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654851 end=1576173654851 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654851 end=1576173654853 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654854 end=1576173654854 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654855 end=1576173654858 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654858 end=1576173654858 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:00:54 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173654858 end=1576173654861 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:00:54 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
19/12/13 02:00:54 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/12/13 02:00:54 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
19/12/13 02:00:54 INFO scheduler.DAGScheduler: Parents of final stage: List()
19/12/13 02:00:54 INFO scheduler.DAGScheduler: Missing parents: List()
19/12/13 02:00:54 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[1161] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
19/12/13 02:00:54 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 186.1 KB, free 6.9 GB)
19/12/13 02:00:54 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.4 KB, free 6.9 GB)
19/12/13 02:00:54 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.30:24425 (size: 36.4 KB, free: 6.9 GB)
19/12/13 02:00:54 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
19/12/13 02:00:54 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[1161] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/12/13 02:00:54 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks
19/12/13 02:00:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, hadoop-datanode-0088.corp.cootek.com, executor 28, partition 0, NODE_LOCAL, 8090 bytes)
19/12/13 02:00:55 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.111:29795) with ID 7
19/12/13 02:00:55 INFO spark.ExecutorAllocationManager: New executor 7 has registered (new total is 42)
19/12/13 02:00:55 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0082.corp.cootek.com:37427 with 4.5 GB RAM, BlockManagerId(7, hadoop-datanode-0082.corp.cootek.com, 37427, None)
19/12/13 02:00:55 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-datanode-0088.corp.cootek.com:31151 (size: 36.4 KB, free: 4.5 GB)
19/12/13 02:00:55 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-datanode-0088.corp.cootek.com:31151 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:00:55 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.227:54032) with ID 40
19/12/13 02:00:55 INFO spark.ExecutorAllocationManager: New executor 40 has registered (new total is 43)
19/12/13 02:00:55 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0136.corp.cootek.com:15703 with 4.5 GB RAM, BlockManagerId(40, hadoop-datanode-0136.corp.cootek.com, 15703, None)
19/12/13 02:00:56 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.229:31616) with ID 37
19/12/13 02:00:56 INFO spark.ExecutorAllocationManager: New executor 37 has registered (new total is 44)
19/12/13 02:00:56 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0138.corp.cootek.com:31273 with 4.5 GB RAM, BlockManagerId(37, hadoop-datanode-0138.corp.cootek.com, 31273, None)
19/12/13 02:00:56 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.232:32470) with ID 11
19/12/13 02:00:56 INFO spark.ExecutorAllocationManager: New executor 11 has registered (new total is 45)
19/12/13 02:00:56 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.56:50403) with ID 10
19/12/13 02:00:56 INFO spark.ExecutorAllocationManager: New executor 10 has registered (new total is 46)
19/12/13 02:00:56 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0141.corp.cootek.com:27271 with 4.5 GB RAM, BlockManagerId(11, hadoop-datanode-0141.corp.cootek.com, 27271, None)
19/12/13 02:00:56 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.66:35717) with ID 46
19/12/13 02:00:56 INFO spark.ExecutorAllocationManager: New executor 46 has registered (new total is 47)
19/12/13 02:00:56 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0055.corp.cootek.com:11885 with 4.5 GB RAM, BlockManagerId(10, hadoop-datanode-0055.corp.cootek.com, 11885, None)
19/12/13 02:00:56 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0065.corp.cootek.com:45732 with 4.5 GB RAM, BlockManagerId(46, hadoop-datanode-0065.corp.cootek.com, 45732, None)
19/12/13 02:00:57 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.68:65367) with ID 25
19/12/13 02:00:57 INFO spark.ExecutorAllocationManager: New executor 25 has registered (new total is 48)
19/12/13 02:00:57 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.34:46494) with ID 33
19/12/13 02:00:57 INFO spark.ExecutorAllocationManager: New executor 33 has registered (new total is 49)
19/12/13 02:00:57 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0067.corp.cootek.com:40275 with 4.5 GB RAM, BlockManagerId(25, hadoop-datanode-0067.corp.cootek.com, 40275, None)
19/12/13 02:00:57 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0033.corp.cootek.com:42904 with 4.5 GB RAM, BlockManagerId(33, hadoop-datanode-0033.corp.cootek.com, 42904, None)
19/12/13 02:00:57 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2569 ms on hadoop-datanode-0088.corp.cootek.com (executor 28) (1/1)
19/12/13 02:00:57 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/12/13 02:00:57 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.605 s
19/12/13 02:00:57 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 2.665088 s
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+------------+
|extra                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |time          |src         |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+------------+
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1406518,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522068","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:1:10","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                        |20191205130110|DSPTRANSFORM|
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1534833,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522276","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:4:36","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                        |20191205130436|DSPTRANSFORM|
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1253865,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522195","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:3:12","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                        |20191205130312|DSPTRANSFORM|
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1682316,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522061","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:1:0","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                         |20191205130100|DSPTRANSFORM|
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1246089,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522502","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:8:22","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                        |20191205130822|DSPTRANSFORM|
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1122167,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522575","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:9:35","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                        |20191205130935|DSPTRANSFORM|
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1263956,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522060","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:1:1","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                         |20191205130101|DSPTRANSFORM|
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1153502,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522289","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:4:49","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                        |20191205130449|DSPTRANSFORM|
|{"adtarget":"","promoted_app":"","tu":"","event_type":"10000","city_id":"","ad_style":"","use_at":"","imei":"","reqid":"","pkg":"","cc2":"","advid":"","user_id":"","track_id":"","spam":4,"make":"","traffic_source":"","adxsrc":"","crid":"","androidid":"","clickid":"","lau_t":"","latency":1394926,"bid":"","plid":"","reqprt":"","planid":"","mac":"","adid":"","get_no_click_info_from_redis":false,"date":"2019-12-5","bundle_id":"","conv_time":"1575522302","purchase_bid":"","cid":"","gender":"","age":"","campaignid":"","org_id":"","ip":"","carrier":"","time":"13:5:2","appid":"","model":"","os":"1"}                                                                                                                                                                                                                                                                                                                         |20191205130502|DSPTRANSFORM|
|{"adtarget":"2","promoted_app":"com.jzyd.coupon","tu":"202081","event_type":"0","city_id":"37500","ad_style":"4","use_at":"false","date":"2019-12-5","reqid":"73fbc7c1-b8fe-4ec7-ba16-78b684125ac1","cc2":"","advid":"","user_id":"62e9a304090a64b6","track_id":"T11064w63os8g4gz","spam":0,"make":"HUAWEI","traffic_source":"2","adxsrc":"naga","planid":"P595t6n1ad8h2kzc","ip":"61.53.177.6","clickid":"NzNmYmM3YzEtYjhmZS00ZWM3LWJhMTYtNzhiNjg0MTI1YWMxX1QxMTA2NHc2M29zOGc0Z3pfQTYwNTducXh6dmM0bXgzcA==","lau_t":"3","latency":2939254,"bid":"0.036776","plid":"a359362f289f44642ce7c5e122678479","reqprt":"1575522137240","adid":"A6057nqxzvc4mx3p","get_no_click_info_from_redis":false,"imei":"d41d8cd98f00b204e9800998ecf8427e","bundle_id":"com.cootek.crazyreader","purchase_bid":"","campaignid":"C475kzsvflrqhx1y","gender":"1","age":"6","org_id":"1566","carrier":"","time":"13:2:20","appid":"bc712539bcb19563a544755fe892d167"}|20191205130220|DSPTRANSFORM|
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+------------+
only showing top 10 rows

19/12/13 02:00:57 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.30.119:45706) with ID 23
19/12/13 02:00:57 INFO spark.ExecutorAllocationManager: New executor 23 has registered (new total is 50)
19/12/13 02:00:57 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-0090.corp.cootek.com:36916 with 4.5 GB RAM, BlockManagerId(23, hadoop-datanode-0090.corp.cootek.com, 36916, None)
19/12/13 02:00:57 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/12/13 02:00:57 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 374.4 KB, free 6.9 GB)
19/12/13 02:00:57 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 30.9 KB, free 6.9 GB)
19/12/13 02:00:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.30:24425 (size: 30.9 KB, free: 6.9 GB)
19/12/13 02:00:57 INFO spark.SparkContext: Created broadcast 4 from 
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 33
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 41
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 32
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 44
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 51
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 53
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 46
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 36
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 52
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 43
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 34
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 37
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 49
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 47
19/12/13 02:00:59 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.30:24425 in memory (size: 36.4 KB, free: 6.9 GB)
19/12/13 02:00:59 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hadoop-datanode-0088.corp.cootek.com:31151 in memory (size: 36.4 KB, free: 4.5 GB)
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 40
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 54
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 35
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 30
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 38
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 31
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 48
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 42
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 39
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 45
19/12/13 02:00:59 INFO spark.ContextCleaner: Cleaned accumulator 50
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660631 end=1576173660632 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660632 end=1576173660633 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660634 end=1576173660634 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660635 end=1576173660636 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660637 end=1576173660637 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660638 end=1576173660639 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660638 end=1576173660640 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660641 end=1576173660642 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660641 end=1576173660642 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660643 end=1576173660653 duration=10 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660643 end=1576173660654 duration=11 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660654 end=1576173660654 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660655 end=1576173660655 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660658 end=1576173660660 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660658 end=1576173660661 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660661 end=1576173660662 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660663 end=1576173660668 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660663 end=1576173660669 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660669 end=1576173660674 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660669 end=1576173660675 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660669 end=1576173660675 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660676 end=1576173660677 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660678 end=1576173660679 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660678 end=1576173660680 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660681 end=1576173660682 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660683 end=1576173660685 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660683 end=1576173660685 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660686 end=1576173660687 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660686 end=1576173660687 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660689 end=1576173660689 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660689 end=1576173660690 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660691 end=1576173660692 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660691 end=1576173660693 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660694 end=1576173660695 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660694 end=1576173660695 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660694 end=1576173660695 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660696 end=1576173660697 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660697 end=1576173660698 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660699 end=1576173660700 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660699 end=1576173660700 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660699 end=1576173660701 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660701 end=1576173660702 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660702 end=1576173660702 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660702 end=1576173660703 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660704 end=1576173660705 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660704 end=1576173660706 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660704 end=1576173660707 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660707 end=1576173660707 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660708 end=1576173660709 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660710 end=1576173660712 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660710 end=1576173660712 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660712 end=1576173660713 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660713 end=1576173660714 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660713 end=1576173660714 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660715 end=1576173660716 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660717 end=1576173660717 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660718 end=1576173660718 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660718 end=1576173660719 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660720 end=1576173660721 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660720 end=1576173660721 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660722 end=1576173660723 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660722 end=1576173660723 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660724 end=1576173660725 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660724 end=1576173660725 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660726 end=1576173660727 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660726 end=1576173660728 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660728 end=1576173660729 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660730 end=1576173660732 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660733 end=1576173660735 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660736 end=1576173660737 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660740 end=1576173660741 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660742 end=1576173660743 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660743 end=1576173660743 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660744 end=1576173660745 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660746 end=1576173660747 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660747 end=1576173660749 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660750 end=1576173660750 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660750 end=1576173660750 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660751 end=1576173660754 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660751 end=1576173660754 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660751 end=1576173660754 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660756 end=1576173660758 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660756 end=1576173660758 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660813 end=1576173660818 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660813 end=1576173660818 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660819 end=1576173660821 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660822 end=1576173660822 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660823 end=1576173660824 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660824 end=1576173660826 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660824 end=1576173660826 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660824 end=1576173660826 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660826 end=1576173660827 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660828 end=1576173660828 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660829 end=1576173660831 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660829 end=1576173660832 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660829 end=1576173660832 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660832 end=1576173660832 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660833 end=1576173660835 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660835 end=1576173660836 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660835 end=1576173660836 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660837 end=1576173660837 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660838 end=1576173660838 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660839 end=1576173660840 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660840 end=1576173660841 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660841 end=1576173660842 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660842 end=1576173660843 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660844 end=1576173660845 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660844 end=1576173660848 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660848 end=1576173660848 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660848 end=1576173660849 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660849 end=1576173660851 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660852 end=1576173660852 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660852 end=1576173660852 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660854 end=1576173660855 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660855 end=1576173660856 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660856 end=1576173660856 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660857 end=1576173660859 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660860 end=1576173660860 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660861 end=1576173660864 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660865 end=1576173660866 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660866 end=1576173660868 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660868 end=1576173660870 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660871 end=1576173660871 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660872 end=1576173660880 duration=8 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660881 end=1576173660884 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660884 end=1576173660887 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660887 end=1576173660890 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660891 end=1576173660893 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660894 end=1576173660897 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660897 end=1576173660900 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660897 end=1576173660901 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660901 end=1576173660904 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660901 end=1576173660904 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660905 end=1576173660908 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660905 end=1576173660908 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660909 end=1576173660914 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660914 end=1576173660917 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660918 end=1576173660921 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660921 end=1576173660925 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660925 end=1576173660926 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660927 end=1576173660927 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660928 end=1576173660929 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660929 end=1576173660929 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660930 end=1576173660930 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660930 end=1576173660931 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660931 end=1576173660933 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660931 end=1576173660933 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660934 end=1576173660934 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660934 end=1576173660935 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660935 end=1576173660937 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660935 end=1576173660937 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660938 end=1576173660939 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660940 end=1576173660941 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660942 end=1576173660942 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660942 end=1576173660945 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660942 end=1576173660945 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660946 end=1576173660948 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660949 end=1576173660979 duration=30 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660980 end=1576173660984 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660984 end=1576173660988 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660989 end=1576173660990 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:00 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660991 end=1576173660996 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:00 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173660996 end=1576173661002 duration=6 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661003 end=1576173661008 duration=5 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661009 end=1576173661010 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661011 end=1576173661014 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661015 end=1576173661016 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661016 end=1576173661020 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661020 end=1576173661024 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661024 end=1576173661025 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661026 end=1576173661028 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661026 end=1576173661028 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661030 end=1576173661031 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661030 end=1576173661031 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661032 end=1576173661032 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661032 end=1576173661033 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661033 end=1576173661034 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661034 end=1576173661035 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661036 end=1576173661036 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661036 end=1576173661037 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661037 end=1576173661039 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661040 end=1576173661040 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661041 end=1576173661041 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661042 end=1576173661044 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661044 end=1576173661045 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661044 end=1576173661055 duration=11 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661044 end=1576173661098 duration=54 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661044 end=1576173661102 duration=58 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661103 end=1576173661106 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661107 end=1576173661172 duration=65 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 1/1
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661173 end=1576173661240 duration=67 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:01:01 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173661241 end=1576173661245 duration=4 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:01:01 INFO spark.SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
19/12/13 02:01:01 INFO scheduler.DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 220 output partitions
19/12/13 02:01:01 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
19/12/13 02:01:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
19/12/13 02:01:01 INFO scheduler.DAGScheduler: Missing parents: List()
19/12/13 02:01:01 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[1740] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
19/12/13 02:01:01 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 358.0 KB, free 6.9 GB)
19/12/13 02:01:01 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 100.9 KB, free 6.9 GB)
19/12/13 02:01:01 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.30:24425 (size: 100.9 KB, free: 6.9 GB)
19/12/13 02:01:01 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
19/12/13 02:01:01 INFO scheduler.DAGScheduler: Submitting 220 missing tasks from ResultStage 2 (MapPartitionsRDD[1740] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
19/12/13 02:01:01 INFO cluster.YarnScheduler: Adding task set 2.0 with 220 tasks
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 2.0 (TID 2, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 20, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 2.0 (TID 3, hadoop-datanode-0044.corp.cootek.com, executor 48, partition 70, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 2.0 (TID 4, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 21, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 2.0 (TID 5, hadoop-datanode-0054.corp.cootek.com, executor 15, partition 130, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 2.0 (TID 6, hadoop-datanode-0034.corp.cootek.com, executor 27, partition 27, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 2.0 (TID 7, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 56, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 2.0 (TID 8, hadoop-datanode-0090.corp.cootek.com, executor 23, partition 172, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 2.0 (TID 9, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 119, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 2.0 (TID 10, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 6, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 2.0 (TID 11, hadoop-datanode-0088.corp.cootek.com, executor 28, partition 10, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 2.0 (TID 12, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 17, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 2.0 (TID 13, hadoop-datanode-0027.corp.cootek.com, executor 3, partition 46, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 2.0 (TID 14, hadoop-datanode-0031.corp.cootek.com, executor 16, partition 31, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 2.0 (TID 15, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 8, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 2.0 (TID 16, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 62, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 2.0 (TID 17, hadoop-datanode-0092.corp.cootek.com, executor 20, partition 126, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 2.0 (TID 18, hadoop-datanode-0084.corp.cootek.com, executor 19, partition 26, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 2.0 (TID 19, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 5, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 2.0 (TID 20, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 35, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 2.0 (TID 21, hadoop-datanode-0087.corp.cootek.com, executor 44, partition 42, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 2.0 (TID 22, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 16, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 2.0 (TID 23, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 57, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 2.0 (TID 24, hadoop-datanode-0062.corp.cootek.com, executor 43, partition 43, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 2.0 (TID 25, hadoop-datanode-0051.corp.cootek.com, executor 42, partition 7, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 2.0 (TID 26, hadoop-datanode-0048.corp.cootek.com, executor 36, partition 76, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 27, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 4, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 2.0 (TID 28, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 29, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 2.0 (TID 29, hadoop-datanode-0101.corp.cootek.com, executor 31, partition 28, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 2.0 (TID 30, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 81, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 2.0 (TID 31, hadoop-datanode-0043.corp.cootek.com, executor 2, partition 9, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 2.0 (TID 32, hadoop-datanode-0074.corp.cootek.com, executor 17, partition 60, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 2.0 (TID 33, hadoop-datanode-0126.corp.cootek.com, executor 29, partition 24, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 2.0 (TID 34, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 78, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 2.0 (TID 35, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 44, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 2.0 (TID 36, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 63, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 2.0 (TID 37, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 22, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 2.0 (TID 38, hadoop-datanode-0063.corp.cootek.com, executor 1, partition 111, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 2.0 (TID 39, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 134, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 2.0 (TID 40, hadoop-datanode-0089.corp.cootek.com, executor 47, partition 30, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 41, hadoop-datanode-0117.corp.cootek.com, executor 12, partition 0, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 2.0 (TID 42, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 51, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 2.0 (TID 43, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 34, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 2.0 (TID 44, hadoop-datanode-0065.corp.cootek.com, executor 46, partition 59, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 2.0 (TID 45, hadoop-datanode-0039.corp.cootek.com, executor 9, partition 19, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 2.0 (TID 46, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 47, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 2.0 (TID 47, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 72, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 2.0 (TID 48, hadoop-datanode-0086.corp.cootek.com, executor 26, partition 150, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 2.0 (TID 49, hadoop-datanode-0019.corp.cootek.com, executor 4, partition 102, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 2.0 (TID 50, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 71, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:01 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0105.corp.cootek.com:37759 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:01 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0088.corp.cootek.com:31151 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:01 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0105.corp.cootek.com:37759 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:01 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0048.corp.cootek.com:63117 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0126.corp.cootek.com:31883 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0073.corp.cootek.com:31809 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0044.corp.cootek.com:33000 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0033.corp.cootek.com:42904 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0133.corp.cootek.com:24487 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0059.corp.cootek.com:14344 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0138.corp.cootek.com:31273 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0140.corp.cootek.com:33321 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0117.corp.cootek.com:20713 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0136.corp.cootek.com:15703 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0022.corp.cootek.com:51776 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0051.corp.cootek.com:10029 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0062.corp.cootek.com:57013 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0031.corp.cootek.com:54295 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0032.corp.cootek.com:40777 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0035.corp.cootek.com:57176 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0054.corp.cootek.com:53550 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0034.corp.cootek.com:50329 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0084.corp.cootek.com:63768 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0129.corp.cootek.com:35609 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0056.corp.cootek.com:30772 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0086.corp.cootek.com:24217 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0092.corp.cootek.com:36973 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0067.corp.cootek.com:40275 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0076.corp.cootek.com:61043 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0102.corp.cootek.com:14087 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0141.corp.cootek.com:27271 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0087.corp.cootek.com:32833 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0039.corp.cootek.com:14219 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0127.corp.cootek.com:20569 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0082.corp.cootek.com:37427 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0053.corp.cootek.com:57410 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0055.corp.cootek.com:11885 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0063.corp.cootek.com:60269 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0065.corp.cootek.com:45732 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0019.corp.cootek.com:58334 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0090.corp.cootek.com:36916 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0027.corp.cootek.com:29092 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0101.corp.cootek.com:31539 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0080.corp.cootek.com:32774 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0088.corp.cootek.com:31151 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0100.corp.cootek.com:29377 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0074.corp.cootek.com:62509 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0146.corp.cootek.com:28885 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0089.corp.cootek.com:20562 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 51, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 1, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0043.corp.cootek.com:38519 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0095.corp.cootek.com:31270 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0117.corp.cootek.com:20713 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0136.corp.cootek.com:15703 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-datanode-0047.corp.cootek.com:23383 (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0084.corp.cootek.com:63768 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0073.corp.cootek.com:31809 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0140.corp.cootek.com:33321 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 51)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0126.corp.cootek.com:31883 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0138.corp.cootek.com:31273 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0065.corp.cootek.com:45732 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0034.corp.cootek.com:50329 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0051.corp.cootek.com:10029 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0141.corp.cootek.com:27271 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0086.corp.cootek.com:24217 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0127.corp.cootek.com:20569 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0129.corp.cootek.com:35609 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0146.corp.cootek.com:28885 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0035.corp.cootek.com:57176 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0133.corp.cootek.com:24487 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0033.corp.cootek.com:42904 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0082.corp.cootek.com:37427 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0059.corp.cootek.com:14344 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0102.corp.cootek.com:14087 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0048.corp.cootek.com:63117 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0092.corp.cootek.com:36973 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0063.corp.cootek.com:60269 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0090.corp.cootek.com:36916 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0067.corp.cootek.com:40275 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0032.corp.cootek.com:40777 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0089.corp.cootek.com:20562 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0054.corp.cootek.com:53550 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0080.corp.cootek.com:32774 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0027.corp.cootek.com:29092 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0022.corp.cootek.com:51776 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0031.corp.cootek.com:54295 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0019.corp.cootek.com:58334 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0056.corp.cootek.com:30772 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0062.corp.cootek.com:57013 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0055.corp.cootek.com:11885 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0100.corp.cootek.com:29377 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0101.corp.cootek.com:31539 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0039.corp.cootek.com:14219 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0044.corp.cootek.com:33000 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0053.corp.cootek.com:57410 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0076.corp.cootek.com:61043 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0095.corp.cootek.com:31270 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0074.corp.cootek.com:62509 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0087.corp.cootek.com:32833 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:03 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0047.corp.cootek.com:23383 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:03 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-0043.corp.cootek.com:38519 (size: 30.9 KB, free: 4.5 GB)
19/12/13 02:01:03 INFO spark.ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 53)
19/12/13 02:01:04 INFO spark.ExecutorAllocationManager: Requesting 4 new executors because tasks are backlogged (new desired total will be 57)
19/12/13 02:01:05 INFO spark.ExecutorAllocationManager: Requesting 8 new executors because tasks are backlogged (new desired total will be 65)
19/12/13 02:01:05 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 2.0 (TID 52, hadoop-datanode-0126.corp.cootek.com, executor 29, partition 84, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 2.0 (TID 33) in 4208 ms on hadoop-datanode-0126.corp.cootek.com (executor 29) (1/220)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 53, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 2, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 2.0 (TID 46) in 4236 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (2/220)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 2.0 (TID 54, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 89, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 2.0 (TID 50) in 4349 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (3/220)
19/12/13 02:01:06 INFO spark.ExecutorAllocationManager: Requesting 16 new executors because tasks are backlogged (new desired total will be 81)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 2.0 (TID 55, hadoop-datanode-0065.corp.cootek.com, executor 46, partition 103, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 2.0 (TID 44) in 4770 ms on hadoop-datanode-0065.corp.cootek.com (executor 46) (4/220)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 2.0 (TID 56, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 53, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:06 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 2.0 (TID 28) in 5130 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (5/220)
19/12/13 02:01:07 INFO spark.ExecutorAllocationManager: Requesting 19 new executors because tasks are backlogged (new desired total will be 100)
19/12/13 02:01:08 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 2.0 (TID 57, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 114, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:08 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 2.0 (TID 37) in 6316 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (6/220)
19/12/13 02:01:08 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 2.0 (TID 58, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 73, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:08 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 2.0 (TID 42) in 6341 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (7/220)
19/12/13 02:01:08 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 2.0 (TID 59, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 50, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:08 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 2.0 (TID 12) in 6637 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (8/220)
19/12/13 02:01:08 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 2.0 (TID 60, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 161, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:08 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 2.0 (TID 7) in 7164 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (9/220)
19/12/13 02:01:09 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 2.0 (TID 61, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 171, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:09 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 2.0 (TID 39) in 7406 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (10/220)
19/12/13 02:01:10 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 2.0 (TID 62, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 67, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:10 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 2.0 (TID 4) in 8428 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (11/220)
19/12/13 02:01:11 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 2.0 (TID 63, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 54, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:11 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 2.0 (TID 19) in 9403 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (12/220)
19/12/13 02:01:11 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 2.0 (TID 64, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 41, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:11 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 2.0 (TID 10) in 9558 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (13/220)
19/12/13 02:01:15 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 2.0 (TID 65, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 91, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:15 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 2.0 (TID 47) in 13400 ms on hadoop-datanode-0082.corp.cootek.com (executor 7) (14/220)
19/12/13 02:01:15 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 66, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 3, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:15 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 53) in 9317 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (15/220)
19/12/13 02:01:16 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 2.0 (TID 67, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 68, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:16 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 2.0 (TID 16) in 14894 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (16/220)
19/12/13 02:01:18 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 2.0 (TID 68, hadoop-datanode-0086.corp.cootek.com, executor 26, partition 11, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:18 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 2.0 (TID 48) in 16237 ms on hadoop-datanode-0086.corp.cootek.com (executor 26) (17/220)
19/12/13 02:01:18 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 2.0 (TID 69, hadoop-datanode-0019.corp.cootek.com, executor 4, partition 129, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:18 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 2.0 (TID 49) in 16259 ms on hadoop-datanode-0019.corp.cootek.com (executor 4) (18/220)
19/12/13 02:01:18 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 2.0 (TID 70, hadoop-datanode-0031.corp.cootek.com, executor 16, partition 39, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:18 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 2.0 (TID 14) in 16572 ms on hadoop-datanode-0031.corp.cootek.com (executor 16) (19/220)
19/12/13 02:01:18 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 2.0 (TID 71, hadoop-datanode-0034.corp.cootek.com, executor 27, partition 37, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:18 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 2.0 (TID 6) in 16765 ms on hadoop-datanode-0034.corp.cootek.com (executor 27) (20/220)
19/12/13 02:01:19 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 2.0 (TID 72, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 58, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:19 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 2.0 (TID 20) in 17971 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (21/220)
19/12/13 02:01:22 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 2.0 (TID 73, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 127, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:22 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 2.0 (TID 34) in 20230 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (22/220)
19/12/13 02:01:22 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 2.0 (TID 74, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 98, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:22 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 2.0 (TID 23) in 20271 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (23/220)
19/12/13 02:01:22 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 2.0 (TID 75, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 116, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:22 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 2.0 (TID 22) in 20581 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (24/220)
19/12/13 02:01:23 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 2.0 (TID 76, hadoop-datanode-0092.corp.cootek.com, executor 20, partition 165, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:23 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 2.0 (TID 17) in 22004 ms on hadoop-datanode-0092.corp.cootek.com (executor 20) (25/220)
19/12/13 02:01:23 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 2.0 (TID 77, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 83, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:23 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 2.0 (TID 35) in 22071 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (26/220)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 2.0 (TID 78, hadoop-datanode-0088.corp.cootek.com, executor 28, partition 14, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 2.0 (TID 11) in 22344 ms on hadoop-datanode-0088.corp.cootek.com (executor 28) (27/220)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 2.0 (TID 79, hadoop-datanode-0092.corp.cootek.com, executor 20, partition 169, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 2.0 (TID 76) in 399 ms on hadoop-datanode-0092.corp.cootek.com (executor 20) (28/220)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 2.0 (TID 80, hadoop-datanode-0065.corp.cootek.com, executor 46, partition 166, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 2.0 (TID 55) in 17659 ms on hadoop-datanode-0065.corp.cootek.com (executor 46) (29/220)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 2.0 (TID 81, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 12, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 51) in 22709 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (30/220)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 2.0 (TID 82, hadoop-datanode-0086.corp.cootek.com, executor 26, partition 13, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:24 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 2.0 (TID 68) in 6928 ms on hadoop-datanode-0086.corp.cootek.com (executor 26) (31/220)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 2.0 (TID 83, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 122, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 2.0 (TID 9) in 23386 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (32/220)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 2.0 (TID 84, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 140, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 2.0 (TID 54) in 19047 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (33/220)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 2.0 (TID 85, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 118, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 2.0 (TID 74) in 3342 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (34/220)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 2.0 (TID 86, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 199, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 2.0 (TID 85) in 321 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (35/220)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 2.0 (TID 87, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 92, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:25 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 2.0 (TID 56) in 19053 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (36/220)
19/12/13 02:01:26 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 2.0 (TID 88, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 105, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:26 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 2.0 (TID 58) in 18352 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (37/220)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 2.0 (TID 89, hadoop-datanode-0084.corp.cootek.com, executor 19, partition 40, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 2.0 (TID 18) in 25256 ms on hadoop-datanode-0084.corp.cootek.com (executor 19) (38/220)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 2.0 (TID 90, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 96, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 2.0 (TID 59) in 18698 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (39/220)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 2.0 (TID 91, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 94, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 2.0 (TID 63) in 16061 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (40/220)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 2.0 (TID 92, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 75, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 2.0 (TID 62) in 17420 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (41/220)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 2.0 (TID 93, hadoop-datanode-0051.corp.cootek.com, executor 42, partition 100, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 2.0 (TID 25) in 26123 ms on hadoop-datanode-0051.corp.cootek.com (executor 42) (42/220)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 2.0 (TID 94, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 113, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:27 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 2.0 (TID 36) in 26170 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (43/220)
19/12/13 02:01:28 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 2.0 (TID 95, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 156, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:28 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 2.0 (TID 43) in 26262 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (44/220)
19/12/13 02:01:28 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 2.0 (TID 96, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 142, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:28 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 2.0 (TID 30) in 26618 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (45/220)
19/12/13 02:01:29 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 2.0 (TID 97, hadoop-datanode-0126.corp.cootek.com, executor 29, partition 87, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:29 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 2.0 (TID 52) in 23698 ms on hadoop-datanode-0126.corp.cootek.com (executor 29) (46/220)
19/12/13 02:01:29 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 2.0 (TID 98, hadoop-datanode-0086.corp.cootek.com, executor 26, partition 15, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:29 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 2.0 (TID 82) in 4913 ms on hadoop-datanode-0086.corp.cootek.com (executor 26) (47/220)
19/12/13 02:01:29 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 2.0 (TID 99, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 190, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:29 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 2.0 (TID 60) in 21003 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (48/220)
19/12/13 02:01:30 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 2.0 (TID 100, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 183, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:30 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 2.0 (TID 73) in 8360 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (49/220)
19/12/13 02:01:30 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 2.0 (TID 101, hadoop-datanode-0084.corp.cootek.com, executor 19, partition 109, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:30 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 2.0 (TID 89) in 3683 ms on hadoop-datanode-0084.corp.cootek.com (executor 19) (50/220)
19/12/13 02:01:30 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 2.0 (TID 102, hadoop-datanode-0101.corp.cootek.com, executor 31, partition 146, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:30 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 2.0 (TID 29) in 29042 ms on hadoop-datanode-0101.corp.cootek.com (executor 31) (51/220)
19/12/13 02:01:32 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 2.0 (TID 103, hadoop-datanode-0051.corp.cootek.com, executor 42, partition 106, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:32 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 2.0 (TID 93) in 4515 ms on hadoop-datanode-0051.corp.cootek.com (executor 42) (52/220)
19/12/13 02:01:32 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 2.0 (TID 104, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 95, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:32 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 2.0 (TID 67) in 16233 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (53/220)
19/12/13 02:01:33 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 2.0 (TID 105, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 18, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:33 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 2.0 (TID 84) in 8407 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (54/220)
19/12/13 02:01:34 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 2.0 (TID 106, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 123, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:34 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 2.0 (TID 64) in 23027 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (55/220)
19/12/13 02:01:35 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 2.0 (TID 107, hadoop-datanode-0101.corp.cootek.com, executor 31, partition 178, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:35 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 2.0 (TID 102) in 4967 ms on hadoop-datanode-0101.corp.cootek.com (executor 31) (56/220)
19/12/13 02:01:36 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 2.0 (TID 108, hadoop-datanode-0087.corp.cootek.com, executor 44, partition 175, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:36 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 2.0 (TID 21) in 34275 ms on hadoop-datanode-0087.corp.cootek.com (executor 44) (57/220)
19/12/13 02:01:36 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 2.0 (TID 109, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 23, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:36 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 66) in 20746 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (58/220)
19/12/13 02:01:36 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 2.0 (TID 110, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 148, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:36 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 2.0 (TID 57) in 28516 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (59/220)
19/12/13 02:01:37 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 2.0 (TID 111, hadoop-datanode-0126.corp.cootek.com, executor 29, partition 179, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:37 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 2.0 (TID 97) in 7369 ms on hadoop-datanode-0126.corp.cootek.com (executor 29) (60/220)
19/12/13 02:01:37 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 2.0 (TID 112, hadoop-datanode-0039.corp.cootek.com, executor 9, partition 82, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:37 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 2.0 (TID 45) in 35342 ms on hadoop-datanode-0039.corp.cootek.com (executor 9) (61/220)
19/12/13 02:01:37 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 2.0 (TID 113, hadoop-datanode-0054.corp.cootek.com, executor 15, partition 155, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:37 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 2.0 (TID 5) in 35449 ms on hadoop-datanode-0054.corp.cootek.com (executor 15) (62/220)
19/12/13 02:01:37 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 2.0 (TID 114, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 153, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:37 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 2.0 (TID 15) in 35574 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (63/220)
19/12/13 02:01:38 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 2.0 (TID 115, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 25, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:38 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 2.0 (TID 95) in 10297 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (64/220)
19/12/13 02:01:39 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 2.0 (TID 116, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 32, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:39 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 2.0 (TID 2) in 37870 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (65/220)
19/12/13 02:01:40 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 2.0 (TID 117, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 124, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:40 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 2.0 (TID 104) in 7745 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (66/220)
19/12/13 02:01:40 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 2.0 (TID 118, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 149, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:40 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 2.0 (TID 72) in 20972 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (67/220)
19/12/13 02:01:41 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 2.0 (TID 119, hadoop-datanode-0089.corp.cootek.com, executor 47, partition 69, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:41 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 2.0 (TID 40) in 39546 ms on hadoop-datanode-0089.corp.cootek.com (executor 47) (68/220)
19/12/13 02:01:41 INFO scheduler.TaskSetManager: Starting task 218.0 in stage 2.0 (TID 120, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 218, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:41 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 2.0 (TID 86) in 16157 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (69/220)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 2.0 (TID 121, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 101, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 2.0 (TID 77) in 18208 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (70/220)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 2.0 (TID 122, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 65, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 2.0 (TID 116) in 2496 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (71/220)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 2.0 (TID 123, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 136, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 2.0 (TID 91) in 14976 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (72/220)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 2.0 (TID 124, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 132, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 2.0 (TID 90) in 15451 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (73/220)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 2.0 (TID 125, hadoop-datanode-0065.corp.cootek.com, executor 46, partition 192, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 2.0 (TID 80) in 18443 ms on hadoop-datanode-0065.corp.cootek.com (executor 46) (74/220)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 2.0 (TID 126, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 33, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:42 INFO scheduler.TaskSetManager: Finished task 218.0 in stage 2.0 (TID 120) in 920 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (75/220)
19/12/13 02:01:44 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 2.0 (TID 127, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 36, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:44 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 2.0 (TID 61) in 35101 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (76/220)
19/12/13 02:01:45 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 2.0 (TID 128, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 38, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:45 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 2.0 (TID 88) in 18921 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (77/220)
19/12/13 02:01:45 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 2.0 (TID 129, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 177, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:45 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 2.0 (TID 65) in 30487 ms on hadoop-datanode-0082.corp.cootek.com (executor 7) (78/220)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 2.0 (TID 130, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 45, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 2.0 (TID 100) in 15672 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (79/220)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 2.0 (TID 131, hadoop-datanode-0088.corp.cootek.com, executor 28, partition 48, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 2.0 (TID 78) in 22456 ms on hadoop-datanode-0088.corp.cootek.com (executor 28) (80/220)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 2.0 (TID 132, hadoop-datanode-0031.corp.cootek.com, executor 16, partition 79, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 2.0 (TID 70) in 28437 ms on hadoop-datanode-0031.corp.cootek.com (executor 16) (81/220)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 2.0 (TID 133, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 49, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 2.0 (TID 114) in 9472 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (82/220)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 2.0 (TID 134, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 93, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:46 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 2.0 (TID 87) in 20919 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (83/220)
19/12/13 02:01:47 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 2.0 (TID 135, hadoop-datanode-0063.corp.cootek.com, executor 1, partition 141, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:47 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 2.0 (TID 38) in 45410 ms on hadoop-datanode-0063.corp.cootek.com (executor 1) (84/220)
19/12/13 02:01:47 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 2.0 (TID 136, hadoop-datanode-0034.corp.cootek.com, executor 27, partition 143, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:47 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 2.0 (TID 71) in 28805 ms on hadoop-datanode-0034.corp.cootek.com (executor 27) (85/220)
19/12/13 02:01:47 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 2.0 (TID 137, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 52, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:47 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 2.0 (TID 126) in 4956 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (86/220)
19/12/13 02:01:48 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 2.0 (TID 138, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 85, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:48 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 27) in 46311 ms on hadoop-datanode-0053.corp.cootek.com (executor 14) (87/220)
19/12/13 02:01:48 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 2.0 (TID 139, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 55, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:48 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 2.0 (TID 127) in 4447 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (88/220)
19/12/13 02:01:48 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 2.0 (TID 140, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 61, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:48 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 2.0 (TID 106) in 14493 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (89/220)
19/12/13 02:01:49 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 2.0 (TID 141, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 64, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:49 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 2.0 (TID 130) in 3224 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (90/220)
19/12/13 02:01:50 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 2.0 (TID 142, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 182, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:50 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 2.0 (TID 96) in 21974 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (91/220)
19/12/13 02:01:52 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 2.0 (TID 143, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 66, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:52 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 2.0 (TID 81) in 27866 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (92/220)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 2.0 (TID 144, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 131, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 2.0 (TID 92) in 25657 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (93/220)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Starting task 201.0 in stage 2.0 (TID 145, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 201, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 2.0 (TID 117) in 12631 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (94/220)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 2.0 (TID 146, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 121, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 2.0 (TID 134) in 6439 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (95/220)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 2.0 (TID 147, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 74, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 2.0 (TID 121) in 11627 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (96/220)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 2.0 (TID 148, hadoop-datanode-0051.corp.cootek.com, executor 42, partition 117, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:53 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 2.0 (TID 103) in 21527 ms on hadoop-datanode-0051.corp.cootek.com (executor 42) (97/220)
19/12/13 02:01:54 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 2.0 (TID 149, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 77, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:54 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 2.0 (TID 115) in 15885 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (98/220)
19/12/13 02:01:54 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 2.0 (TID 150, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 193, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:54 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 2.0 (TID 123) in 12301 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (99/220)
19/12/13 02:01:54 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 2.0 (TID 151, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 80, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:54 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 2.0 (TID 110) in 17922 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (100/220)
19/12/13 02:01:56 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 2.0 (TID 152, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 86, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:56 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 2.0 (TID 141) in 6825 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (101/220)
19/12/13 02:01:56 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 2.0 (TID 153, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 88, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:56 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 2.0 (TID 147) in 2777 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (102/220)
19/12/13 02:01:56 INFO scheduler.TaskSetManager: Starting task 212.0 in stage 2.0 (TID 154, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 212, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:56 INFO scheduler.TaskSetManager: Finished task 201.0 in stage 2.0 (TID 145) in 3275 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (103/220)
19/12/13 02:01:56 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 2.0 (TID 155, hadoop-datanode-0086.corp.cootek.com, executor 26, partition 90, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:56 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 2.0 (TID 98) in 26867 ms on hadoop-datanode-0086.corp.cootek.com (executor 26) (104/220)
19/12/13 02:01:57 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 2.0 (TID 156, hadoop-datanode-0084.corp.cootek.com, executor 19, partition 97, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:57 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 2.0 (TID 101) in 26991 ms on hadoop-datanode-0084.corp.cootek.com (executor 19) (105/220)
19/12/13 02:01:58 INFO scheduler.TaskSetManager: Starting task 207.0 in stage 2.0 (TID 157, hadoop-datanode-0092.corp.cootek.com, executor 20, partition 207, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:58 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 2.0 (TID 79) in 33809 ms on hadoop-datanode-0092.corp.cootek.com (executor 20) (106/220)
19/12/13 02:01:58 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 2.0 (TID 158, hadoop-datanode-0086.corp.cootek.com, executor 26, partition 99, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:58 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 2.0 (TID 155) in 1479 ms on hadoop-datanode-0086.corp.cootek.com (executor 26) (107/220)
19/12/13 02:01:58 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 2.0 (TID 159, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 104, RACK_LOCAL, 8086 bytes)
19/12/13 02:01:58 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 2.0 (TID 149) in 4074 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (108/220)
19/12/13 02:01:58 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 2.0 (TID 160, hadoop-datanode-0117.corp.cootek.com, executor 12, partition 125, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:58 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 41) in 57050 ms on hadoop-datanode-0117.corp.cootek.com (executor 12) (109/220)
19/12/13 02:01:59 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 2.0 (TID 161, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 163, NODE_LOCAL, 8086 bytes)
19/12/13 02:01:59 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 2.0 (TID 83) in 33999 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (110/220)
19/12/13 02:02:00 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 2.0 (TID 162, hadoop-datanode-0090.corp.cootek.com, executor 23, partition 107, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:00 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 2.0 (TID 8) in 58254 ms on hadoop-datanode-0090.corp.cootek.com (executor 23) (111/220)
19/12/13 02:02:00 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 2.0 (TID 163, hadoop-datanode-0062.corp.cootek.com, executor 43, partition 173, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:00 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 2.0 (TID 24) in 58715 ms on hadoop-datanode-0062.corp.cootek.com (executor 43) (112/220)
19/12/13 02:02:00 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 2.0 (TID 164, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 108, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:00 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 2.0 (TID 143) in 7945 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (113/220)
19/12/13 02:02:00 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 2.0 (TID 165, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 110, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:00 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 2.0 (TID 109) in 24598 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (114/220)
19/12/13 02:02:01 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 2.0 (TID 166, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 112, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:01 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 2.0 (TID 151) in 6593 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (115/220)
19/12/13 02:02:01 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 2.0 (TID 167, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 147, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:01 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 2.0 (TID 75) in 39150 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (116/220)
19/12/13 02:02:01 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 2.0 (TID 168, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 115, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:01 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 2.0 (TID 94) in 33806 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (117/220)
19/12/13 02:02:01 INFO scheduler.TaskSetManager: Starting task 206.0 in stage 2.0 (TID 169, hadoop-datanode-0089.corp.cootek.com, executor 47, partition 206, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:01 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 2.0 (TID 119) in 20627 ms on hadoop-datanode-0089.corp.cootek.com (executor 47) (118/220)
19/12/13 02:02:03 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 2.0 (TID 170, hadoop-datanode-0019.corp.cootek.com, executor 4, partition 120, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:03 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 2.0 (TID 69) in 45057 ms on hadoop-datanode-0019.corp.cootek.com (executor 4) (119/220)
19/12/13 02:02:03 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 2.0 (TID 171, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 128, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:03 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 2.0 (TID 133) in 16444 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (120/220)
19/12/13 02:02:04 INFO scheduler.TaskSetManager: Starting task 215.0 in stage 2.0 (TID 172, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 215, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:04 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 2.0 (TID 122) in 22067 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (121/220)
19/12/13 02:02:04 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 2.0 (TID 173, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 133, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:04 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 2.0 (TID 164) in 3791 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (122/220)
19/12/13 02:02:04 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 2.0 (TID 174, hadoop-datanode-0034.corp.cootek.com, executor 27, partition 151, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:04 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 2.0 (TID 136) in 17299 ms on hadoop-datanode-0034.corp.cootek.com (executor 27) (123/220)
19/12/13 02:02:05 INFO scheduler.TaskSetManager: Starting task 204.0 in stage 2.0 (TID 175, hadoop-datanode-0126.corp.cootek.com, executor 29, partition 204, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:05 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 2.0 (TID 111) in 28322 ms on hadoop-datanode-0126.corp.cootek.com (executor 29) (124/220)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 2.0 (TID 176, hadoop-datanode-0101.corp.cootek.com, executor 31, partition 135, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 2.0 (TID 107) in 30237 ms on hadoop-datanode-0101.corp.cootek.com (executor 31) (125/220)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 2.0 (TID 177, hadoop-datanode-0027.corp.cootek.com, executor 3, partition 137, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 2.0 (TID 13) in 64434 ms on hadoop-datanode-0027.corp.cootek.com (executor 3) (126/220)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 2.0 (TID 178, hadoop-datanode-0048.corp.cootek.com, executor 36, partition 159, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 2.0 (TID 26) in 64783 ms on hadoop-datanode-0048.corp.cootek.com (executor 36) (127/220)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 2.0 (TID 179, hadoop-datanode-0039.corp.cootek.com, executor 9, partition 139, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 2.0 (TID 112) in 29563 ms on hadoop-datanode-0039.corp.cootek.com (executor 9) (128/220)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 2.0 (TID 180, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 181, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:06 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 2.0 (TID 118) in 26253 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (129/220)
19/12/13 02:02:07 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 2.0 (TID 181, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 138, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:07 INFO scheduler.TaskSetManager: Finished task 212.0 in stage 2.0 (TID 154) in 10570 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (130/220)
19/12/13 02:02:07 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 2.0 (TID 182, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 144, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:07 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 2.0 (TID 139) in 18423 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (131/220)
19/12/13 02:02:07 INFO scheduler.TaskSetManager: Starting task 208.0 in stage 2.0 (TID 183, hadoop-datanode-0054.corp.cootek.com, executor 15, partition 208, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:07 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 2.0 (TID 113) in 30190 ms on hadoop-datanode-0054.corp.cootek.com (executor 15) (132/220)
19/12/13 02:02:07 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 2.0 (TID 184, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 145, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:07 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 2.0 (TID 159) in 9468 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (133/220)
19/12/13 02:02:08 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 2.0 (TID 185, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 152, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:08 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 2.0 (TID 129) in 22616 ms on hadoop-datanode-0082.corp.cootek.com (executor 7) (134/220)
19/12/13 02:02:08 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 2.0 (TID 186, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 154, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:08 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 2.0 (TID 140) in 19740 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (135/220)
19/12/13 02:02:09 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 2.0 (TID 187, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 157, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:09 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 2.0 (TID 138) in 21041 ms on hadoop-datanode-0053.corp.cootek.com (executor 14) (136/220)
19/12/13 02:02:09 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 2.0 (TID 188, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 158, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:09 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 2.0 (TID 128) in 24018 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (137/220)
19/12/13 02:02:09 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 2.0 (TID 189, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 160, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:09 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 2.0 (TID 105) in 35921 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (138/220)
19/12/13 02:02:09 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 2.0 (TID 190, hadoop-datanode-0087.corp.cootek.com, executor 44, partition 187, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:09 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 2.0 (TID 108) in 33695 ms on hadoop-datanode-0087.corp.cootek.com (executor 44) (139/220)
19/12/13 02:02:10 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 2.0 (TID 191, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 162, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:10 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 2.0 (TID 185) in 2081 ms on hadoop-datanode-0082.corp.cootek.com (executor 7) (140/220)
19/12/13 02:02:10 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 2.0 (TID 192, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 164, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:10 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 2.0 (TID 161) in 11662 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (141/220)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 2.0 (TID 193, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 196, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 2.0 (TID 124) in 29330 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (142/220)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 2.0 (TID 194, hadoop-datanode-0117.corp.cootek.com, executor 12, partition 184, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 2.0 (TID 170) in 8785 ms on hadoop-datanode-0019.corp.cootek.com (executor 4) (143/220)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 2.0 (TID 160) in 13053 ms on hadoop-datanode-0117.corp.cootek.com (executor 12) (144/220)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 2.0 (TID 195, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 195, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 2.0 (TID 146) in 18571 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (145/220)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 2.0 (TID 196, hadoop-datanode-0062.corp.cootek.com, executor 43, partition 167, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:11 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 2.0 (TID 163) in 11404 ms on hadoop-datanode-0062.corp.cootek.com (executor 43) (146/220)
19/12/13 02:02:12 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 2.0 (TID 197, hadoop-datanode-0019.corp.cootek.com, executor 4, partition 168, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:12 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 2.0 (TID 198, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 170, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:12 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 2.0 (TID 165) in 11946 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (147/220)
19/12/13 02:02:13 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 2.0 (TID 199, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 174, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:13 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 2.0 (TID 150) in 19113 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (148/220)
19/12/13 02:02:14 INFO scheduler.TaskSetManager: Starting task 217.0 in stage 2.0 (TID 200, hadoop-datanode-0054.corp.cootek.com, executor 15, partition 217, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:14 INFO scheduler.TaskSetManager: Finished task 208.0 in stage 2.0 (TID 183) in 7027 ms on hadoop-datanode-0054.corp.cootek.com (executor 15) (149/220)
19/12/13 02:02:16 INFO scheduler.TaskSetManager: Starting task 209.0 in stage 2.0 (TID 201, hadoop-datanode-0117.corp.cootek.com, executor 12, partition 209, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:16 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 2.0 (TID 194) in 4691 ms on hadoop-datanode-0117.corp.cootek.com (executor 12) (150/220)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 2.0 (TID 202, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 176, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 2.0 (TID 153) in 20528 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (151/220)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 2.0 (TID 203, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 191, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 2.0 (TID 99) in 47095 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (152/220)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 2.0 (TID 204, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 180, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 2.0 (TID 181) in 10641 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (153/220)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Starting task 214.0 in stage 2.0 (TID 205, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 214, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 2.0 (TID 142) in 27405 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (154/220)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 2.0 (TID 206, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 185, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:17 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 2.0 (TID 152) in 21859 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (155/220)
19/12/13 02:02:18 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 2.0 (TID 207, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 186, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:18 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 2.0 (TID 144) in 25071 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (156/220)
19/12/13 02:02:18 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 2.0 (TID 208, hadoop-datanode-0031.corp.cootek.com, executor 16, partition 188, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:18 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 2.0 (TID 132) in 31621 ms on hadoop-datanode-0031.corp.cootek.com (executor 16) (157/220)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Starting task 203.0 in stage 2.0 (TID 209, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 203, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 2.0 (TID 167) in 17601 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (158/220)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 2.0 (TID 210, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 189, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 2.0 (TID 203) in 2367 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (159/220)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Starting task 216.0 in stage 2.0 (TID 211, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 216, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 2.0 (TID 195) in 7566 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (160/220)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 2.0 (TID 212, hadoop-datanode-0034.corp.cootek.com, executor 27, partition 194, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 2.0 (TID 174) in 14838 ms on hadoop-datanode-0034.corp.cootek.com (executor 27) (161/220)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 2.0 (TID 213, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 197, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:19 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 2.0 (TID 210) in 504 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (162/220)
19/12/13 02:02:20 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 2.0 (TID 214, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 198, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:20 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 2.0 (TID 137) in 32720 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (163/220)
19/12/13 02:02:20 INFO scheduler.TaskSetManager: Starting task 200.0 in stage 2.0 (TID 215, hadoop-datanode-0027.corp.cootek.com, executor 3, partition 200, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:20 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 2.0 (TID 177) in 14597 ms on hadoop-datanode-0027.corp.cootek.com (executor 3) (164/220)
19/12/13 02:02:21 INFO scheduler.TaskSetManager: Starting task 202.0 in stage 2.0 (TID 216, hadoop-datanode-0019.corp.cootek.com, executor 4, partition 202, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:21 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 2.0 (TID 197) in 9166 ms on hadoop-datanode-0019.corp.cootek.com (executor 4) (165/220)
19/12/13 02:02:21 INFO scheduler.TaskSetManager: Starting task 205.0 in stage 2.0 (TID 217, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 205, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:21 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 2.0 (TID 198) in 8918 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (166/220)
19/12/13 02:02:21 INFO scheduler.TaskSetManager: Starting task 210.0 in stage 2.0 (TID 218, hadoop-datanode-0089.corp.cootek.com, executor 47, partition 210, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:21 INFO scheduler.TaskSetManager: Finished task 206.0 in stage 2.0 (TID 169) in 19878 ms on hadoop-datanode-0089.corp.cootek.com (executor 47) (167/220)
19/12/13 02:02:22 INFO scheduler.TaskSetManager: Starting task 213.0 in stage 2.0 (TID 219, hadoop-datanode-0063.corp.cootek.com, executor 1, partition 213, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:22 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 2.0 (TID 135) in 35634 ms on hadoop-datanode-0063.corp.cootek.com (executor 1) (168/220)
19/12/13 02:02:23 INFO scheduler.TaskSetManager: Starting task 219.0 in stage 2.0 (TID 220, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 219, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:23 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 2.0 (TID 193) in 11150 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (169/220)
19/12/13 02:02:23 INFO scheduler.TaskSetManager: Starting task 211.0 in stage 2.0 (TID 221, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 211, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:23 INFO scheduler.TaskSetManager: Finished task 214.0 in stage 2.0 (TID 205) in 5288 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (170/220)
19/12/13 02:02:23 INFO scheduler.TaskSetManager: Finished task 216.0 in stage 2.0 (TID 211) in 3842 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (171/220)
19/12/13 02:02:23 INFO scheduler.TaskSetManager: Finished task 203.0 in stage 2.0 (TID 209) in 4291 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (172/220)
19/12/13 02:02:23 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 2.0 (TID 166) in 22461 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (173/220)
19/12/13 02:02:23 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 2.0 (TID 204) in 6129 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (174/220)
19/12/13 02:02:24 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 2.0 (TID 189) in 14507 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (175/220)
19/12/13 02:02:24 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 2.0 (TID 199) in 10904 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (176/220)
19/12/13 02:02:25 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 2.0 (TID 187) in 16539 ms on hadoop-datanode-0053.corp.cootek.com (executor 14) (177/220)
19/12/13 02:02:25 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 2.0 (TID 176) in 19721 ms on hadoop-datanode-0101.corp.cootek.com (executor 31) (178/220)
19/12/13 02:02:26 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 2.0 (TID 148) in 32467 ms on hadoop-datanode-0051.corp.cootek.com (executor 42) (179/220)
19/12/13 02:02:26 INFO scheduler.TaskSetManager: Finished task 211.0 in stage 2.0 (TID 221) in 3523 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (180/220)
19/12/13 02:02:26 INFO scheduler.TaskSetManager: Finished task 200.0 in stage 2.0 (TID 215) in 5887 ms on hadoop-datanode-0027.corp.cootek.com (executor 3) (181/220)
19/12/13 02:02:26 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 2.0 (TID 196) in 15014 ms on hadoop-datanode-0062.corp.cootek.com (executor 43) (182/220)
19/12/13 02:02:27 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 2.0 (TID 131) in 41308 ms on hadoop-datanode-0088.corp.cootek.com (executor 28) (183/220)
19/12/13 02:02:28 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 2.0 (TID 188) in 18611 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (184/220)
19/12/13 02:02:28 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 2.0 (TID 186) in 19614 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (185/220)
19/12/13 02:02:28 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 2.0 (TID 182) in 21045 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (186/220)
19/12/13 02:02:28 INFO scheduler.TaskSetManager: Finished task 204.0 in stage 2.0 (TID 175) in 23343 ms on hadoop-datanode-0126.corp.cootek.com (executor 29) (187/220)
19/12/13 02:02:29 INFO scheduler.TaskSetManager: Finished task 209.0 in stage 2.0 (TID 201) in 12804 ms on hadoop-datanode-0117.corp.cootek.com (executor 12) (188/220)
19/12/13 02:02:30 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 2.0 (TID 32) in 88636 ms on hadoop-datanode-0074.corp.cootek.com (executor 17) (189/220)
19/12/13 02:02:31 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 2.0 (TID 179) in 24990 ms on hadoop-datanode-0039.corp.cootek.com (executor 9) (190/220)
19/12/13 02:02:31 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 2.0 (TID 202) in 14826 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (191/220)
19/12/13 02:02:32 INFO scheduler.TaskSetManager: Finished task 202.0 in stage 2.0 (TID 216) in 11380 ms on hadoop-datanode-0019.corp.cootek.com (executor 4) (192/220)
19/12/13 02:02:32 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 2.0 (TID 168) in 31052 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (193/220)
19/12/13 02:02:33 INFO scheduler.TaskSetManager: Finished task 215.0 in stage 2.0 (TID 172) in 29164 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (194/220)
19/12/13 02:02:34 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 2.0 (TID 192) in 23273 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (195/220)
19/12/13 02:02:34 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 2.0 (TID 214) in 13828 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (196/220)
19/12/13 02:02:34 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 2.0 (TID 171) in 31095 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (197/220)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Finished task 219.0 in stage 2.0 (TID 220) in 12097 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (198/220)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Marking task 9 in stage 2.0 (on hadoop-datanode-0043.corp.cootek.com) as speculatable because it ran more than 35893 ms
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Marking task 192 in stage 2.0 (on hadoop-datanode-0065.corp.cootek.com) as speculatable because it ran more than 35893 ms
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Marking task 70 in stage 2.0 (on hadoop-datanode-0044.corp.cootek.com) as speculatable because it ran more than 35893 ms
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Marking task 97 in stage 2.0 (on hadoop-datanode-0084.corp.cootek.com) as speculatable because it ran more than 35893 ms
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Marking task 207 in stage 2.0 (on hadoop-datanode-0092.corp.cootek.com) as speculatable because it ran more than 35893 ms
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Marking task 99 in stage 2.0 (on hadoop-datanode-0086.corp.cootek.com) as speculatable because it ran more than 35893 ms
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Starting task 207.1 in stage 2.0 (TID 222, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 207, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Starting task 99.1 in stage 2.0 (TID 223, hadoop-datanode-0039.corp.cootek.com, executor 9, partition 99, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Starting task 97.1 in stage 2.0 (TID 224, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 97, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Starting task 9.1 in stage 2.0 (TID 225, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 9, NODE_LOCAL, 8086 bytes)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Starting task 70.1 in stage 2.0 (TID 226, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 70, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Starting task 192.1 in stage 2.0 (TID 227, hadoop-datanode-0101.corp.cootek.com, executor 31, partition 192, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Killing attempt 1 for task 70.1 in stage 2.0 (TID 226) on hadoop-datanode-0146.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0044.corp.cootek.com
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 2.0 (TID 3) in 93641 ms on hadoop-datanode-0044.corp.cootek.com (executor 48) (199/220)
19/12/13 02:02:35 WARN scheduler.TaskSetManager: Lost task 70.1 in stage 2.0 (TID 226, hadoop-datanode-0146.corp.cootek.com, executor 8): TaskKilled (another attempt succeeded)
19/12/13 02:02:35 INFO scheduler.TaskSetManager: Task 70.1 in stage 2.0 (TID 226) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:02:36 INFO scheduler.TaskSetManager: Marking task 107 in stage 2.0 (on hadoop-datanode-0090.corp.cootek.com) as speculatable because it ran more than 35942 ms
19/12/13 02:02:36 INFO scheduler.TaskSetManager: Starting task 107.1 in stage 2.0 (TID 228, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 107, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:36 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 2.0 (TID 190) in 26475 ms on hadoop-datanode-0087.corp.cootek.com (executor 44) (200/220)
19/12/13 02:02:36 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 2.0 (TID 178) in 29744 ms on hadoop-datanode-0048.corp.cootek.com (executor 36) (201/220)
19/12/13 02:02:37 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 2.0 (TID 184) in 29556 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (202/220)
19/12/13 02:02:38 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 39
19/12/13 02:02:38 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 18
19/12/13 02:02:38 INFO scheduler.TaskSetManager: Finished task 210.0 in stage 2.0 (TID 218) in 16662 ms on hadoop-datanode-0089.corp.cootek.com (executor 47) (203/220)
19/12/13 02:02:38 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 34
19/12/13 02:02:38 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 45
19/12/13 02:02:39 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 24
19/12/13 02:02:40 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 2.0 (TID 191) in 29849 ms on hadoop-datanode-0082.corp.cootek.com (executor 7) (204/220)
19/12/13 02:02:40 INFO scheduler.TaskSetManager: Killing attempt 1 for task 207.1 in stage 2.0 (TID 222) on hadoop-datanode-0140.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0092.corp.cootek.com
19/12/13 02:02:40 INFO scheduler.TaskSetManager: Finished task 207.0 in stage 2.0 (TID 157) in 42244 ms on hadoop-datanode-0092.corp.cootek.com (executor 20) (205/220)
19/12/13 02:02:40 WARN scheduler.TaskSetManager: Lost task 207.1 in stage 2.0 (TID 222, hadoop-datanode-0140.corp.cootek.com, executor 21): TaskKilled (another attempt succeeded)
19/12/13 02:02:40 INFO scheduler.TaskSetManager: Task 207.1 in stage 2.0 (TID 222) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:02:40 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 2.0 (TID 180) in 33496 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (206/220)
19/12/13 02:02:40 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 14
19/12/13 02:02:41 INFO scheduler.TaskSetManager: Killing attempt 1 for task 192.1 in stage 2.0 (TID 227) on hadoop-datanode-0101.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0065.corp.cootek.com
19/12/13 02:02:41 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 2.0 (TID 125) in 58349 ms on hadoop-datanode-0065.corp.cootek.com (executor 46) (207/220)
19/12/13 02:02:41 WARN scheduler.TaskSetManager: Lost task 192.1 in stage 2.0 (TID 227, hadoop-datanode-0101.corp.cootek.com, executor 31): TaskKilled (another attempt succeeded)
19/12/13 02:02:41 INFO scheduler.TaskSetManager: Task 192.1 in stage 2.0 (TID 227) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:02:41 INFO scheduler.TaskSetManager: Marking task 133 in stage 2.0 (on hadoop-datanode-0047.corp.cootek.com) as speculatable because it ran more than 36846 ms
19/12/13 02:02:41 INFO scheduler.TaskSetManager: Starting task 133.1 in stage 2.0 (TID 229, hadoop-datanode-0101.corp.cootek.com, executor 31, partition 133, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:41 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 42
19/12/13 02:02:41 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 11
19/12/13 02:02:41 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 3
19/12/13 02:02:41 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 43
19/12/13 02:02:42 INFO scheduler.TaskSetManager: Killing attempt 1 for task 97.1 in stage 2.0 (TID 224) on hadoop-datanode-0055.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0084.corp.cootek.com
19/12/13 02:02:42 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 2.0 (TID 156) in 44595 ms on hadoop-datanode-0084.corp.cootek.com (executor 19) (208/220)
19/12/13 02:02:42 WARN scheduler.TaskSetManager: Lost task 97.1 in stage 2.0 (TID 224, hadoop-datanode-0055.corp.cootek.com, executor 10): TaskKilled (another attempt succeeded)
19/12/13 02:02:42 INFO scheduler.TaskSetManager: Task 97.1 in stage 2.0 (TID 224) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:02:42 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 28
19/12/13 02:02:43 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 41
19/12/13 02:02:43 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 2.0 (TID 212) in 23683 ms on hadoop-datanode-0034.corp.cootek.com (executor 27) (209/220)
19/12/13 02:02:43 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 33
19/12/13 02:02:43 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 29
19/12/13 02:02:43 INFO scheduler.TaskSetManager: Killing attempt 1 for task 107.1 in stage 2.0 (TID 228) on hadoop-datanode-0095.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0090.corp.cootek.com
19/12/13 02:02:43 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 2.0 (TID 162) in 43781 ms on hadoop-datanode-0090.corp.cootek.com (executor 23) (210/220)
19/12/13 02:02:43 WARN scheduler.TaskSetManager: Lost task 107.1 in stage 2.0 (TID 228, hadoop-datanode-0095.corp.cootek.com, executor 32): TaskKilled (another attempt succeeded)
19/12/13 02:02:43 INFO scheduler.TaskSetManager: Task 107.1 in stage 2.0 (TID 228) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:02:44 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 12
19/12/13 02:02:45 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 2.0 (TID 207) in 26991 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (211/220)
19/12/13 02:02:45 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 17
19/12/13 02:02:46 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 30
19/12/13 02:02:47 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 4
19/12/13 02:02:48 INFO scheduler.TaskSetManager: Finished task 217.0 in stage 2.0 (TID 200) in 33867 ms on hadoop-datanode-0054.corp.cootek.com (executor 15) (212/220)
19/12/13 02:02:49 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 5
19/12/13 02:02:49 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 22
19/12/13 02:02:50 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 40
19/12/13 02:02:50 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 48
19/12/13 02:02:50 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 8
19/12/13 02:02:51 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 44
19/12/13 02:02:51 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 36
19/12/13 02:02:51 INFO scheduler.TaskSetManager: Killing attempt 1 for task 133.1 in stage 2.0 (TID 229) on hadoop-datanode-0101.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0047.corp.cootek.com
19/12/13 02:02:51 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 2.0 (TID 173) in 46971 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (213/220)
19/12/13 02:02:51 WARN scheduler.TaskSetManager: Lost task 133.1 in stage 2.0 (TID 229, hadoop-datanode-0101.corp.cootek.com, executor 31): TaskKilled (another attempt succeeded)
19/12/13 02:02:51 INFO scheduler.TaskSetManager: Task 133.1 in stage 2.0 (TID 229) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:02:52 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 38
19/12/13 02:02:52 INFO scheduler.TaskSetManager: Killing attempt 1 for task 9.1 in stage 2.0 (TID 225) on hadoop-datanode-0035.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0043.corp.cootek.com
19/12/13 02:02:52 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 2.0 (TID 31) in 110808 ms on hadoop-datanode-0043.corp.cootek.com (executor 2) (214/220)
19/12/13 02:02:52 WARN scheduler.TaskSetManager: Lost task 9.1 in stage 2.0 (TID 225, hadoop-datanode-0035.corp.cootek.com, executor 6): TaskKilled (another attempt succeeded)
19/12/13 02:02:52 INFO scheduler.TaskSetManager: Task 9.1 in stage 2.0 (TID 225) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:02:53 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 47
19/12/13 02:02:54 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 2.0 (TID 208) in 35772 ms on hadoop-datanode-0031.corp.cootek.com (executor 16) (215/220)
19/12/13 02:02:55 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 7, 20
19/12/13 02:02:55 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 21
19/12/13 02:02:55 INFO scheduler.TaskSetManager: Marking task 185 in stage 2.0 (on hadoop-datanode-0076.corp.cootek.com) as speculatable because it ran more than 37396 ms
19/12/13 02:02:55 INFO scheduler.TaskSetManager: Starting task 185.1 in stage 2.0 (TID 230, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 185, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:55 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 37
19/12/13 02:02:56 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 46
19/12/13 02:02:57 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 19, 10
19/12/13 02:02:57 INFO scheduler.TaskSetManager: Marking task 197 in stage 2.0 (on hadoop-datanode-0129.corp.cootek.com) as speculatable because it ran more than 37396 ms
19/12/13 02:02:57 INFO scheduler.TaskSetManager: Starting task 197.1 in stage 2.0 (TID 231, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 197, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:57 INFO scheduler.TaskSetManager: Killing attempt 0 for task 99.0 in stage 2.0 (TID 158) on hadoop-datanode-0086.corp.cootek.com as the attempt 1 succeeded on hadoop-datanode-0039.corp.cootek.com
19/12/13 02:02:57 INFO scheduler.TaskSetManager: Finished task 99.1 in stage 2.0 (TID 223) in 22393 ms on hadoop-datanode-0039.corp.cootek.com (executor 9) (216/220)
19/12/13 02:02:57 WARN scheduler.TaskSetManager: Lost task 99.0 in stage 2.0 (TID 158, hadoop-datanode-0086.corp.cootek.com, executor 26): TaskKilled (another attempt succeeded)
19/12/13 02:02:57 INFO scheduler.TaskSetManager: Task 99.0 in stage 2.0 (TID 158) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:02:58 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 27
19/12/13 02:02:58 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 23
19/12/13 02:02:58 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 32
19/12/13 02:02:59 INFO scheduler.TaskSetManager: Marking task 205 in stage 2.0 (on hadoop-datanode-0080.corp.cootek.com) as speculatable because it ran more than 37619 ms
19/12/13 02:02:59 INFO scheduler.TaskSetManager: Starting task 205.1 in stage 2.0 (TID 232, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 205, RACK_LOCAL, 8086 bytes)
19/12/13 02:02:59 INFO scheduler.TaskSetManager: Killing attempt 1 for task 205.1 in stage 2.0 (TID 232) on hadoop-datanode-0095.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0080.corp.cootek.com
19/12/13 02:02:59 INFO scheduler.TaskSetManager: Finished task 205.0 in stage 2.0 (TID 217) in 38047 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (217/220)
19/12/13 02:02:59 WARN scheduler.TaskSetManager: Lost task 205.1 in stage 2.0 (TID 232, hadoop-datanode-0095.corp.cootek.com, executor 32): TaskKilled (another attempt succeeded)
19/12/13 02:02:59 INFO scheduler.TaskSetManager: Task 205.1 in stage 2.0 (TID 232) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:03:00 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 25
19/12/13 02:03:00 INFO scheduler.TaskSetManager: Marking task 213 in stage 2.0 (on hadoop-datanode-0063.corp.cootek.com) as speculatable because it ran more than 37842 ms
19/12/13 02:03:00 INFO scheduler.TaskSetManager: Starting task 213.1 in stage 2.0 (TID 233, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 213, RACK_LOCAL, 8086 bytes)
19/12/13 02:03:01 INFO scheduler.TaskSetManager: Killing attempt 1 for task 197.1 in stage 2.0 (TID 231) on hadoop-datanode-0053.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0129.corp.cootek.com
19/12/13 02:03:01 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 2.0 (TID 213) in 41721 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (218/220)
19/12/13 02:03:01 WARN scheduler.TaskSetManager: Lost task 197.1 in stage 2.0 (TID 231, hadoop-datanode-0053.corp.cootek.com, executor 14): TaskKilled (another attempt succeeded)
19/12/13 02:03:01 INFO scheduler.TaskSetManager: Task 197.1 in stage 2.0 (TID 231) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:03:03 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 15
19/12/13 02:03:06 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 50, 31
19/12/13 02:03:07 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 2
19/12/13 02:03:07 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 6
19/12/13 02:03:09 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 16
19/12/13 02:03:10 INFO scheduler.TaskSetManager: Killing attempt 1 for task 185.1 in stage 2.0 (TID 230) on hadoop-datanode-0127.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0076.corp.cootek.com
19/12/13 02:03:10 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 2.0 (TID 206) in 52171 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (219/220)
19/12/13 02:03:11 WARN scheduler.TaskSetManager: Lost task 185.1 in stage 2.0 (TID 230, hadoop-datanode-0127.corp.cootek.com, executor 30): TaskKilled (another attempt succeeded)
19/12/13 02:03:11 INFO scheduler.TaskSetManager: Task 185.1 in stage 2.0 (TID 230) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:03:12 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 9
19/12/13 02:03:12 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 26
19/12/13 02:03:14 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 13
19/12/13 02:03:16 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 14, 49
19/12/13 02:03:25 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 35
19/12/13 02:03:26 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 30
19/12/13 02:03:30 INFO scheduler.TaskSetManager: Killing attempt 1 for task 213.1 in stage 2.0 (TID 233) on hadoop-datanode-0095.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0063.corp.cootek.com
19/12/13 02:03:30 INFO scheduler.TaskSetManager: Finished task 213.0 in stage 2.0 (TID 219) in 67182 ms on hadoop-datanode-0063.corp.cootek.com (executor 1) (220/220)
19/12/13 02:03:30 INFO scheduler.DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 148.604 s
19/12/13 02:03:30 INFO scheduler.DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 148.702012 s
19/12/13 02:03:30 WARN scheduler.TaskSetManager: Lost task 213.1 in stage 2.0 (TID 233, hadoop-datanode-0095.corp.cootek.com, executor 32): TaskKilled (another attempt succeeded)
19/12/13 02:03:30 INFO scheduler.TaskSetManager: Task 213.1 in stage 2.0 (TID 233) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:03:30 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
19/12/13 02:03:30 WARN hdfs.DFSClient: Slow ReadProcessor read fields took 47689ms (threshold=30000ms); ack: seqno: 148 status: SUCCESS status: SUCCESS status: SUCCESS downstreamAckTimeNanos: 8578222, targets: [192.168.30.133:61004, 192.168.30.116:61004, 192.168.30.155:61004]
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 77
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 65
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 69
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 62
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 81
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 85
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 79
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 74
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 80
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 83
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 70
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.30:24425 in memory (size: 100.9 KB, free: 6.9 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0088.corp.cootek.com:31151 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0065.corp.cootek.com:45732 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0034.corp.cootek.com:50329 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0059.corp.cootek.com:14344 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0019.corp.cootek.com:58334 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0133.corp.cootek.com:24487 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0032.corp.cootek.com:40777 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0136.corp.cootek.com:15703 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0076.corp.cootek.com:61043 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0039.corp.cootek.com:14219 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0126.corp.cootek.com:31883 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0035.corp.cootek.com:57176 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0117.corp.cootek.com:20713 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0062.corp.cootek.com:57013 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0127.corp.cootek.com:20569 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0105.corp.cootek.com:37759 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0033.corp.cootek.com:42904 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0067.corp.cootek.com:40275 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0138.corp.cootek.com:31273 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0141.corp.cootek.com:27271 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0102.corp.cootek.com:14087 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0140.corp.cootek.com:33321 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0043.corp.cootek.com:38519 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0027.corp.cootek.com:29092 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0129.corp.cootek.com:35609 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0054.corp.cootek.com:53550 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0044.corp.cootek.com:33000 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0031.corp.cootek.com:54295 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0074.corp.cootek.com:62509 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0047.corp.cootek.com:23383 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0056.corp.cootek.com:30772 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0051.corp.cootek.com:10029 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0053.corp.cootek.com:57410 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0022.corp.cootek.com:51776 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0055.corp.cootek.com:11885 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0048.corp.cootek.com:63117 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0063.corp.cootek.com:60269 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0146.corp.cootek.com:28885 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0087.corp.cootek.com:32833 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0095.corp.cootek.com:31270 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0089.corp.cootek.com:20562 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0080.corp.cootek.com:32774 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0100.corp.cootek.com:29377 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0101.corp.cootek.com:31539 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0073.corp.cootek.com:31809 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0090.corp.cootek.com:36916 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0086.corp.cootek.com:24217 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0082.corp.cootek.com:37427 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0092.corp.cootek.com:36973 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on hadoop-datanode-0084.corp.cootek.com:63768 in memory (size: 100.9 KB, free: 4.5 GB)
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 66
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 78
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 76
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 75
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 68
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 67
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 71
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 82
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 64
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 72
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 61
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 73
19/12/13 02:03:30 INFO spark.ContextCleaner: Cleaned accumulator 63
19/12/13 02:03:31 INFO datasources.FileFormatWriter: Write Job 3746b0af-fd45-4e2c-86dc-29d12111c092 committed.
19/12/13 02:03:31 INFO datasources.FileFormatWriter: Finished processing stats for write job 3746b0af-fd45-4e2c-86dc-29d12111c092.
19/12/13 02:03:31 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19/12/13 02:03:31 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 373.7 KB, free 6.9 GB)
19/12/13 02:03:31 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.5 KB, free 6.9 GB)
19/12/13 02:03:31 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.30:24425 (size: 30.5 KB, free: 6.9 GB)
19/12/13 02:03:31 INFO spark.SparkContext: Created broadcast 6 from 
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816839 end=1576173816839 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816839 end=1576173816840 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816839 end=1576173816841 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816841 end=1576173816842 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816842 end=1576173816842 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816842 end=1576173816843 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816843 end=1576173816843 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816844 end=1576173816844 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816844 end=1576173816844 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816846 end=1576173816846 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816846 end=1576173816847 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816847 end=1576173816848 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816848 end=1576173816848 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816848 end=1576173816849 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816849 end=1576173816849 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816849 end=1576173816850 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816850 end=1576173816851 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816851 end=1576173816851 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816852 end=1576173816852 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816852 end=1576173816853 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816853 end=1576173816853 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816853 end=1576173816854 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816854 end=1576173816854 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816854 end=1576173816855 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816854 end=1576173816855 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816856 end=1576173816856 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816856 end=1576173816857 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816856 end=1576173816857 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816857 end=1576173816857 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816858 end=1576173816858 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816859 end=1576173816859 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816859 end=1576173816859 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816861 end=1576173816861 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816862 end=1576173816862 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816862 end=1576173816862 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816862 end=1576173816862 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816863 end=1576173816863 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816864 end=1576173816864 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816864 end=1576173816864 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816865 end=1576173816865 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816866 end=1576173816866 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816867 end=1576173816867 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816867 end=1576173816867 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816868 end=1576173816868 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816868 end=1576173816868 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816868 end=1576173816868 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816868 end=1576173816869 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816869 end=1576173816870 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816871 end=1576173816871 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816871 end=1576173816872 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816872 end=1576173816872 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816872 end=1576173816872 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816872 end=1576173816873 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816873 end=1576173816873 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816873 end=1576173816874 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816874 end=1576173816874 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816875 end=1576173816875 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816876 end=1576173816876 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816876 end=1576173816876 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816876 end=1576173816877 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816877 end=1576173816877 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816877 end=1576173816878 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816878 end=1576173816878 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816878 end=1576173816878 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816879 end=1576173816879 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816879 end=1576173816880 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816880 end=1576173816880 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816880 end=1576173816880 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816881 end=1576173816881 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816881 end=1576173816882 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816882 end=1576173816883 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816883 end=1576173816883 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816884 end=1576173816884 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816884 end=1576173816884 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816885 end=1576173816885 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816885 end=1576173816885 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816885 end=1576173816885 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816886 end=1576173816886 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816886 end=1576173816887 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816887 end=1576173816888 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816888 end=1576173816888 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816888 end=1576173816888 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816889 end=1576173816889 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816889 end=1576173816889 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816890 end=1576173816890 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816890 end=1576173816890 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816891 end=1576173816891 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816891 end=1576173816891 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816891 end=1576173816892 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816892 end=1576173816892 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816893 end=1576173816893 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816893 end=1576173816894 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816894 end=1576173816894 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816894 end=1576173816894 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816895 end=1576173816895 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816895 end=1576173816895 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816895 end=1576173816896 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816895 end=1576173816896 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816896 end=1576173816897 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816897 end=1576173816897 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816898 end=1576173816898 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816898 end=1576173816898 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816899 end=1576173816899 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816899 end=1576173816899 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816900 end=1576173816900 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816900 end=1576173816900 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816900 end=1576173816901 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816901 end=1576173816901 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816901 end=1576173816902 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816902 end=1576173816902 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816902 end=1576173816903 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816903 end=1576173816903 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816904 end=1576173816904 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816904 end=1576173816904 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816904 end=1576173816904 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816904 end=1576173816905 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816905 end=1576173816905 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816906 end=1576173816906 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816906 end=1576173816906 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816907 end=1576173816907 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816907 end=1576173816908 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816908 end=1576173816908 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816908 end=1576173816908 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816908 end=1576173816909 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816909 end=1576173816909 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816910 end=1576173816910 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816910 end=1576173816910 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816910 end=1576173816911 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816911 end=1576173816911 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816911 end=1576173816911 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816911 end=1576173816912 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816912 end=1576173816912 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816912 end=1576173816913 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816913 end=1576173816913 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816914 end=1576173816914 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816914 end=1576173816914 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816914 end=1576173816915 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816915 end=1576173816915 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816915 end=1576173816916 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816916 end=1576173816916 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816916 end=1576173816916 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816916 end=1576173816917 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816918 end=1576173816918 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816918 end=1576173816918 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816918 end=1576173816919 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816919 end=1576173816919 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816919 end=1576173816919 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816920 end=1576173816920 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816920 end=1576173816920 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816921 end=1576173816921 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816921 end=1576173816921 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816922 end=1576173816922 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816922 end=1576173816922 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816922 end=1576173816923 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816923 end=1576173816923 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816923 end=1576173816923 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816923 end=1576173816924 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816924 end=1576173816925 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816925 end=1576173816925 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816925 end=1576173816925 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816926 end=1576173816926 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816926 end=1576173816927 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816927 end=1576173816927 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816927 end=1576173816927 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816927 end=1576173816927 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816927 end=1576173816928 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816928 end=1576173816928 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816929 end=1576173816930 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816929 end=1576173816930 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816929 end=1576173816930 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816931 end=1576173816931 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816932 end=1576173816932 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816932 end=1576173816932 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816932 end=1576173816933 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816933 end=1576173816933 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816934 end=1576173816934 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816934 end=1576173816935 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816935 end=1576173816935 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816935 end=1576173816935 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816935 end=1576173816936 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816936 end=1576173816936 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816937 end=1576173816937 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816937 end=1576173816938 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816938 end=1576173816938 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816938 end=1576173816938 duration=0 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816938 end=1576173816939 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816938 end=1576173816939 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816938 end=1576173816939 duration=1 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816938 end=1576173816940 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816938 end=1576173816941 duration=3 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816941 end=1576173816943 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO orc.OrcInputFormat: FooterCacheHitRatio: 0/0
19/12/13 02:03:36 INFO log.PerfLogger: </PERFLOG method=OrcGetSplits start=1576173816944 end=1576173816946 duration=2 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
19/12/13 02:03:36 INFO spark.SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
19/12/13 02:03:36 INFO scheduler.DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 192 output partitions
19/12/13 02:03:36 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
19/12/13 02:03:36 INFO scheduler.DAGScheduler: Parents of final stage: List()
19/12/13 02:03:36 INFO scheduler.DAGScheduler: Missing parents: List()
19/12/13 02:03:36 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[2320] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
19/12/13 02:03:37 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 356.4 KB, free 6.9 GB)
19/12/13 02:03:37 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 99.4 KB, free 6.9 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.30:24425 (size: 99.4 KB, free: 6.9 GB)
19/12/13 02:03:37 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
19/12/13 02:03:37 INFO scheduler.DAGScheduler: Submitting 192 missing tasks from ResultStage 3 (MapPartitionsRDD[2320] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
19/12/13 02:03:37 INFO cluster.YarnScheduler: Adding task set 3.0 with 192 tasks
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 3.0 (TID 234, hadoop-datanode-0126.corp.cootek.com, executor 29, partition 31, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 3.0 (TID 235, hadoop-datanode-0031.corp.cootek.com, executor 16, partition 10, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 3.0 (TID 236, hadoop-datanode-0089.corp.cootek.com, executor 47, partition 67, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 3.0 (TID 237, hadoop-datanode-0063.corp.cootek.com, executor 1, partition 102, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 3.0 (TID 238, hadoop-datanode-0054.corp.cootek.com, executor 15, partition 70, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 3.0 (TID 239, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 24, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 3.0 (TID 240, hadoop-datanode-0088.corp.cootek.com, executor 28, partition 13, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 3.0 (TID 241, hadoop-datanode-0039.corp.cootek.com, executor 9, partition 40, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 3.0 (TID 242, hadoop-datanode-0043.corp.cootek.com, executor 2, partition 21, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 3.0 (TID 243, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 5, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 3.0 (TID 244, hadoop-datanode-0051.corp.cootek.com, executor 42, partition 60, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 3.0 (TID 245, hadoop-datanode-0086.corp.cootek.com, executor 26, partition 18, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 3.0 (TID 246, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 9, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 3.0 (TID 247, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 16, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 3.0 (TID 248, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 33, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 249, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 0, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 3.0 (TID 250, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 17, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 3.0 (TID 251, hadoop-datanode-0087.corp.cootek.com, executor 44, partition 22, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 3.0 (TID 252, hadoop-datanode-0074.corp.cootek.com, executor 17, partition 44, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 3.0 (TID 253, hadoop-datanode-0117.corp.cootek.com, executor 12, partition 69, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 3.0 (TID 254, hadoop-datanode-0101.corp.cootek.com, executor 31, partition 104, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 3.0 (TID 255, hadoop-datanode-0062.corp.cootek.com, executor 43, partition 6, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 3.0 (TID 256, hadoop-datanode-0090.corp.cootek.com, executor 23, partition 71, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 3.0 (TID 257, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 8, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 3.0 (TID 258, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 4, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 3.0 (TID 259, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 11, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 3.0 (TID 260, hadoop-datanode-0065.corp.cootek.com, executor 46, partition 28, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 3.0 (TID 261, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 37, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 3.0 (TID 262, hadoop-datanode-0044.corp.cootek.com, executor 48, partition 57, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 3.0 (TID 263, hadoop-datanode-0092.corp.cootek.com, executor 20, partition 74, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 3.0 (TID 264, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 27, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 265, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 2, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 3.0 (TID 266, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 49, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 3.0 (TID 267, hadoop-datanode-0019.corp.cootek.com, executor 4, partition 26, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 3.0 (TID 268, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 111, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 3.0 (TID 269, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 30, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 3.0 (TID 270, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 152, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 271, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 1, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 3.0 (TID 272, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 50, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 3.0 (TID 273, hadoop-datanode-0084.corp.cootek.com, executor 19, partition 92, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 3.0 (TID 274, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 66, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 3.0 (TID 275, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 83, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 3.0 (TID 276, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 52, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 277, hadoop-datanode-0034.corp.cootek.com, executor 27, partition 3, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 3.0 (TID 278, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 35, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 3.0 (TID 279, hadoop-datanode-0048.corp.cootek.com, executor 36, partition 45, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 3.0 (TID 280, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 61, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 3.0 (TID 281, hadoop-datanode-0027.corp.cootek.com, executor 3, partition 75, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 3.0 (TID 282, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 129, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0126.corp.cootek.com:31883 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0032.corp.cootek.com:40777 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0100.corp.cootek.com:29377 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0136.corp.cootek.com:15703 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0086.corp.cootek.com:24217 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0067.corp.cootek.com:40275 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0039.corp.cootek.com:14219 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 51)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0065.corp.cootek.com:45732 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0105.corp.cootek.com:37759 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0047.corp.cootek.com:23383 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0117.corp.cootek.com:20713 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0031.corp.cootek.com:54295 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0073.corp.cootek.com:31809 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0053.corp.cootek.com:57410 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0051.corp.cootek.com:10029 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0138.corp.cootek.com:31273 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0129.corp.cootek.com:35609 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0059.corp.cootek.com:14344 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0140.corp.cootek.com:33321 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0034.corp.cootek.com:50329 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0033.corp.cootek.com:42904 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0035.corp.cootek.com:57176 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0055.corp.cootek.com:11885 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0056.corp.cootek.com:30772 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0076.corp.cootek.com:61043 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0063.corp.cootek.com:60269 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0062.corp.cootek.com:57013 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0133.corp.cootek.com:24487 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0141.corp.cootek.com:27271 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0087.corp.cootek.com:32833 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0080.corp.cootek.com:32774 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0127.corp.cootek.com:20569 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0022.corp.cootek.com:51776 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0088.corp.cootek.com:31151 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0054.corp.cootek.com:53550 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0092.corp.cootek.com:36973 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0090.corp.cootek.com:36916 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0019.corp.cootek.com:58334 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0126.corp.cootek.com:31883 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0136.corp.cootek.com:15703 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0084.corp.cootek.com:63768 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 3.0 (TID 283, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 7, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0043.corp.cootek.com:38519 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0117.corp.cootek.com:20713 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0089.corp.cootek.com:20562 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0032.corp.cootek.com:40777 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0101.corp.cootek.com:31539 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0067.corp.cootek.com:40275 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0065.corp.cootek.com:45732 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0082.corp.cootek.com:37427 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0073.corp.cootek.com:31809 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0095.corp.cootek.com:31270 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0129.corp.cootek.com:35609 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0086.corp.cootek.com:24217 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0039.corp.cootek.com:14219 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0080.corp.cootek.com:32774 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0105.corp.cootek.com:37759 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0140.corp.cootek.com:33321 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0047.corp.cootek.com:23383 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0102.corp.cootek.com:14087 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0059.corp.cootek.com:14344 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0138.corp.cootek.com:31273 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0127.corp.cootek.com:20569 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0051.corp.cootek.com:10029 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0133.corp.cootek.com:24487 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0035.corp.cootek.com:57176 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0033.corp.cootek.com:42904 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0056.corp.cootek.com:30772 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0063.corp.cootek.com:60269 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0027.corp.cootek.com:29092 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0146.corp.cootek.com:28885 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0087.corp.cootek.com:32833 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0053.corp.cootek.com:57410 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0031.corp.cootek.com:54295 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0141.corp.cootek.com:27271 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0055.corp.cootek.com:11885 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0054.corp.cootek.com:53550 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0019.corp.cootek.com:58334 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0090.corp.cootek.com:36916 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0044.corp.cootek.com:33000 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0088.corp.cootek.com:31151 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0062.corp.cootek.com:57013 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0022.corp.cootek.com:51776 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0048.corp.cootek.com:63117 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0076.corp.cootek.com:61043 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0092.corp.cootek.com:36973 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0034.corp.cootek.com:50329 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0100.corp.cootek.com:29377 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0095.corp.cootek.com:31270 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0043.corp.cootek.com:38519 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0084.corp.cootek.com:63768 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0102.corp.cootek.com:14087 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0101.corp.cootek.com:31539 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0146.corp.cootek.com:28885 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0027.corp.cootek.com:29092 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0044.corp.cootek.com:33000 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 3.0 (TID 284, hadoop-datanode-0126.corp.cootek.com, executor 29, partition 48, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 3.0 (TID 234) in 166 ms on hadoop-datanode-0126.corp.cootek.com (executor 29) (1/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 3.0 (TID 285, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 100, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 3.0 (TID 243) in 167 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (2/192)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0089.corp.cootek.com:20562 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 3.0 (TID 286, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 108, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 3.0 (TID 247) in 182 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (3/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 3.0 (TID 287, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 115, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-datanode-0074.corp.cootek.com:62509 (size: 99.4 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 3.0 (TID 268) in 180 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (4/192)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0048.corp.cootek.com:63117 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 3.0 (TID 288, hadoop-datanode-0065.corp.cootek.com, executor 46, partition 51, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 3.0 (TID 260) in 195 ms on hadoop-datanode-0065.corp.cootek.com (executor 46) (5/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 3.0 (TID 289, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 63, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 3.0 (TID 248) in 211 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (6/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 3.0 (TID 290, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 86, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 265) in 212 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (7/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 3.0 (TID 291, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 182, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 3.0 (TID 264) in 214 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (8/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 3.0 (TID 292, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 78, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 3.0 (TID 274) in 213 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (9/192)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0082.corp.cootek.com:37427 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 3.0 (TID 293, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 110, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 3.0 (TID 239) in 222 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (10/192)
19/12/13 02:03:37 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-datanode-0074.corp.cootek.com:62509 (size: 30.5 KB, free: 4.5 GB)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 3.0 (TID 294, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 42, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 3.0 (TID 269) in 234 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (11/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 3.0 (TID 295, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 39, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 3.0 (TID 246) in 241 ms on hadoop-datanode-0053.corp.cootek.com (executor 14) (12/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 3.0 (TID 296, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 119, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 3.0 (TID 257) in 244 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (13/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 3.0 (TID 297, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 156, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 3.0 (TID 270) in 245 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (14/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 3.0 (TID 298, hadoop-datanode-0101.corp.cootek.com, executor 31, partition 117, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 3.0 (TID 254) in 249 ms on hadoop-datanode-0101.corp.cootek.com (executor 31) (15/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 3.0 (TID 299, hadoop-datanode-0062.corp.cootek.com, executor 43, partition 172, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 3.0 (TID 255) in 250 ms on hadoop-datanode-0062.corp.cootek.com (executor 43) (16/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 3.0 (TID 300, hadoop-datanode-0087.corp.cootek.com, executor 44, partition 38, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 3.0 (TID 251) in 253 ms on hadoop-datanode-0087.corp.cootek.com (executor 44) (17/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 3.0 (TID 301, hadoop-datanode-0088.corp.cootek.com, executor 28, partition 123, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 3.0 (TID 240) in 262 ms on hadoop-datanode-0088.corp.cootek.com (executor 28) (18/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 3.0 (TID 302, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 25, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 3.0 (TID 259) in 263 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (19/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 3.0 (TID 303, hadoop-datanode-0126.corp.cootek.com, executor 29, partition 132, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 3.0 (TID 284) in 110 ms on hadoop-datanode-0126.corp.cootek.com (executor 29) (20/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 3.0 (TID 304, hadoop-datanode-0027.corp.cootek.com, executor 3, partition 81, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 3.0 (TID 281) in 268 ms on hadoop-datanode-0027.corp.cootek.com (executor 3) (21/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 3.0 (TID 305, hadoop-datanode-0092.corp.cootek.com, executor 20, partition 125, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 3.0 (TID 263) in 272 ms on hadoop-datanode-0092.corp.cootek.com (executor 20) (22/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 3.0 (TID 306, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 41, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 3.0 (TID 258) in 289 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (23/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 3.0 (TID 307, hadoop-datanode-0054.corp.cootek.com, executor 15, partition 138, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 3.0 (TID 238) in 294 ms on hadoop-datanode-0054.corp.cootek.com (executor 15) (24/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 3.0 (TID 308, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 160, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 249) in 300 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (25/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 3.0 (TID 309, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 97, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 3.0 (TID 280) in 306 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (26/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 3.0 (TID 310, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 113, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 3.0 (TID 275) in 310 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (27/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 3.0 (TID 311, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 124, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 271) in 317 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (28/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 3.0 (TID 312, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 12, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 3.0 (TID 283) in 271 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (29/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 3.0 (TID 313, hadoop-datanode-0080.corp.cootek.com, executor 13, partition 179, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 3.0 (TID 290) in 113 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (30/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 3.0 (TID 314, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 19, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 3.0 (TID 250) in 331 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (31/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 3.0 (TID 315, hadoop-datanode-0039.corp.cootek.com, executor 9, partition 53, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 3.0 (TID 241) in 348 ms on hadoop-datanode-0039.corp.cootek.com (executor 9) (32/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 3.0 (TID 316, hadoop-datanode-0034.corp.cootek.com, executor 27, partition 80, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 277) in 344 ms on hadoop-datanode-0034.corp.cootek.com (executor 27) (33/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 3.0 (TID 317, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 59, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 3.0 (TID 261) in 350 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (34/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 3.0 (TID 318, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 186, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 3.0 (TID 291) in 146 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (35/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 3.0 (TID 319, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 144, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 3.0 (TID 293) in 143 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (36/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 3.0 (TID 320, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 62, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 3.0 (TID 276) in 359 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (37/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 3.0 (TID 236) in 366 ms on hadoop-datanode-0089.corp.cootek.com (executor 47) (38/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 3.0 (TID 321, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 36, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 3.0 (TID 278) in 368 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (39/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 3.0 (TID 288) in 178 ms on hadoop-datanode-0065.corp.cootek.com (executor 46) (40/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 3.0 (TID 322, hadoop-datanode-0086.corp.cootek.com, executor 26, partition 96, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 3.0 (TID 245) in 377 ms on hadoop-datanode-0086.corp.cootek.com (executor 26) (41/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 3.0 (TID 323, hadoop-datanode-0044.corp.cootek.com, executor 48, partition 89, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 3.0 (TID 262) in 374 ms on hadoop-datanode-0044.corp.cootek.com (executor 48) (42/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 3.0 (TID 324, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 153, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 3.0 (TID 296) in 133 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (43/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 3.0 (TID 325, hadoop-datanode-0090.corp.cootek.com, executor 23, partition 91, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 3.0 (TID 256) in 379 ms on hadoop-datanode-0090.corp.cootek.com (executor 23) (44/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 3.0 (TID 326, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 94, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 3.0 (TID 306) in 91 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (45/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 3.0 (TID 327, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 135, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 3.0 (TID 287) in 200 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (46/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 3.0 (TID 298) in 135 ms on hadoop-datanode-0101.corp.cootek.com (executor 31) (47/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 3.0 (TID 328, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 88, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 3.0 (TID 289) in 175 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (48/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 3.0 (TID 329, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 114, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 3.0 (TID 285) in 253 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (49/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 3.0 (TID 330, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 159, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 3.0 (TID 302) in 164 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (50/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 3.0 (TID 331, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 173, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 3.0 (TID 297) in 185 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (51/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 3.0 (TID 303) in 162 ms on hadoop-datanode-0126.corp.cootek.com (executor 29) (52/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 3.0 (TID 332, hadoop-datanode-0074.corp.cootek.com, executor 17, partition 128, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 3.0 (TID 252) in 438 ms on hadoop-datanode-0074.corp.cootek.com (executor 17) (53/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 3.0 (TID 333, hadoop-datanode-0092.corp.cootek.com, executor 20, partition 154, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 3.0 (TID 305) in 171 ms on hadoop-datanode-0092.corp.cootek.com (executor 20) (54/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 3.0 (TID 334, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 14, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 3.0 (TID 309) in 136 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (55/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 3.0 (TID 335, hadoop-datanode-0088.corp.cootek.com, executor 28, partition 15, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 3.0 (TID 336, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 134, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 3.0 (TID 301) in 188 ms on hadoop-datanode-0088.corp.cootek.com (executor 28) (56/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 3.0 (TID 272) in 445 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (57/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 3.0 (TID 337, hadoop-datanode-0062.corp.cootek.com, executor 43, partition 180, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 3.0 (TID 299) in 206 ms on hadoop-datanode-0062.corp.cootek.com (executor 43) (58/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 3.0 (TID 338, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 120, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 3.0 (TID 286) in 275 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (59/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 3.0 (TID 300) in 204 ms on hadoop-datanode-0087.corp.cootek.com (executor 44) (60/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 3.0 (TID 339, hadoop-datanode-0043.corp.cootek.com, executor 2, partition 90, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 3.0 (TID 242) in 462 ms on hadoop-datanode-0043.corp.cootek.com (executor 2) (61/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 3.0 (TID 340, hadoop-datanode-0027.corp.cootek.com, executor 3, partition 95, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 3.0 (TID 304) in 194 ms on hadoop-datanode-0027.corp.cootek.com (executor 3) (62/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 3.0 (TID 341, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 73, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 3.0 (TID 314) in 143 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (63/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 3.0 (TID 342, hadoop-datanode-0022.corp.cootek.com, executor 24, partition 171, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 3.0 (TID 308) in 178 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (64/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 3.0 (TID 343, hadoop-datanode-0031.corp.cootek.com, executor 16, partition 29, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 3.0 (TID 235) in 483 ms on hadoop-datanode-0031.corp.cootek.com (executor 16) (65/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 3.0 (TID 307) in 189 ms on hadoop-datanode-0054.corp.cootek.com (executor 15) (66/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 3.0 (TID 344, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 136, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 3.0 (TID 310) in 167 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (67/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 3.0 (TID 345, hadoop-datanode-0051.corp.cootek.com, executor 42, partition 105, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 3.0 (TID 244) in 485 ms on hadoop-datanode-0051.corp.cootek.com (executor 42) (68/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 3.0 (TID 346, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 87, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 3.0 (TID 294) in 254 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (69/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 3.0 (TID 313) in 166 ms on hadoop-datanode-0080.corp.cootek.com (executor 13) (70/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 3.0 (TID 347, hadoop-datanode-0034.corp.cootek.com, executor 27, partition 109, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 3.0 (TID 316) in 151 ms on hadoop-datanode-0034.corp.cootek.com (executor 27) (71/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 3.0 (TID 348, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 140, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 3.0 (TID 311) in 186 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (72/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 3.0 (TID 349, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 56, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 3.0 (TID 295) in 268 ms on hadoop-datanode-0053.corp.cootek.com (executor 14) (73/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 3.0 (TID 273) in 504 ms on hadoop-datanode-0084.corp.cootek.com (executor 19) (74/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 3.0 (TID 350, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 151, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 3.0 (TID 319) in 149 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (75/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 3.0 (TID 351, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 126, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 3.0 (TID 328) in 136 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (76/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 3.0 (TID 352, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 158, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 3.0 (TID 317) in 174 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (77/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 3.0 (TID 353, hadoop-datanode-0039.corp.cootek.com, executor 9, partition 79, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 3.0 (TID 315) in 181 ms on hadoop-datanode-0039.corp.cootek.com (executor 9) (78/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 3.0 (TID 354, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 112, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 3.0 (TID 320) in 169 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (79/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 3.0 (TID 355, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 162, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 3.0 (TID 329) in 120 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (80/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 3.0 (TID 356, hadoop-datanode-0141.corp.cootek.com, executor 11, partition 177, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 3.0 (TID 324) in 161 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (81/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 3.0 (TID 357, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 20, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 3.0 (TID 327) in 160 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (82/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 3.0 (TID 358, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 146, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 3.0 (TID 282) in 541 ms on hadoop-datanode-0082.corp.cootek.com (executor 7) (83/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 3.0 (TID 359, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 23, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 3.0 (TID 318) in 189 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (84/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 3.0 (TID 360, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 32, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 3.0 (TID 312) in 227 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (85/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 3.0 (TID 361, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 175, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 3.0 (TID 336) in 113 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (86/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 3.0 (TID 322) in 185 ms on hadoop-datanode-0086.corp.cootek.com (executor 26) (87/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 3.0 (TID 362, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 185, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 3.0 (TID 331) in 147 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (88/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 3.0 (TID 363, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 93, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 3.0 (TID 292) in 379 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (89/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 3.0 (TID 364, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 155, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 3.0 (TID 338) in 141 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (90/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 3.0 (TID 333) in 155 ms on hadoop-datanode-0092.corp.cootek.com (executor 20) (91/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 3.0 (TID 365, hadoop-datanode-0090.corp.cootek.com, executor 23, partition 34, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 3.0 (TID 325) in 223 ms on hadoop-datanode-0090.corp.cootek.com (executor 23) (92/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 3.0 (TID 366, hadoop-datanode-0074.corp.cootek.com, executor 17, partition 43, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 3.0 (TID 332) in 174 ms on hadoop-datanode-0074.corp.cootek.com (executor 17) (93/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 3.0 (TID 367, hadoop-datanode-0019.corp.cootek.com, executor 4, partition 189, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 3.0 (TID 267) in 610 ms on hadoop-datanode-0019.corp.cootek.com (executor 4) (94/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 3.0 (TID 337) in 159 ms on hadoop-datanode-0062.corp.cootek.com (executor 43) (95/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 3.0 (TID 368, hadoop-datanode-0051.corp.cootek.com, executor 42, partition 46, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 3.0 (TID 345) in 140 ms on hadoop-datanode-0051.corp.cootek.com (executor 42) (96/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 3.0 (TID 369, hadoop-datanode-0044.corp.cootek.com, executor 48, partition 143, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 3.0 (TID 323) in 247 ms on hadoop-datanode-0044.corp.cootek.com (executor 48) (97/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 3.0 (TID 370, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 47, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 3.0 (TID 344) in 150 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (98/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 3.0 (TID 371, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 54, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 3.0 (TID 372, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 127, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 3.0 (TID 346) in 145 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (99/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 3.0 (TID 354) in 104 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (100/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 3.0 (TID 373, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 101, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 3.0 (TID 349) in 133 ms on hadoop-datanode-0053.corp.cootek.com (executor 14) (101/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 3.0 (TID 374, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 165, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 3.0 (TID 352) in 122 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (102/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 3.0 (TID 356) in 109 ms on hadoop-datanode-0141.corp.cootek.com (executor 11) (103/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 3.0 (TID 375, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 176, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 3.0 (TID 355) in 116 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (104/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 3.0 (TID 342) in 177 ms on hadoop-datanode-0022.corp.cootek.com (executor 24) (105/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 3.0 (TID 376, hadoop-datanode-0027.corp.cootek.com, executor 3, partition 184, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 3.0 (TID 340) in 192 ms on hadoop-datanode-0027.corp.cootek.com (executor 3) (106/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 3.0 (TID 377, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 149, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 3.0 (TID 351) in 142 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (107/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 3.0 (TID 378, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 55, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 3.0 (TID 330) in 241 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (108/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 3.0 (TID 379, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 58, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 3.0 (TID 350) in 164 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (109/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 3.0 (TID 380, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 122, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 3.0 (TID 321) in 318 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (110/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 3.0 (TID 381, hadoop-datanode-0095.corp.cootek.com, executor 32, partition 64, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 3.0 (TID 334) in 249 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (111/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 3.0 (TID 382, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 187, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 3.0 (TID 341) in 236 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (112/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 3.0 (TID 383, hadoop-datanode-0031.corp.cootek.com, executor 16, partition 147, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 3.0 (TID 343) in 239 ms on hadoop-datanode-0031.corp.cootek.com (executor 16) (113/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 3.0 (TID 384, hadoop-datanode-0067.corp.cootek.com, executor 25, partition 65, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 3.0 (TID 364) in 131 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (114/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 3.0 (TID 385, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 191, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 3.0 (TID 362) in 153 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (115/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 3.0 (TID 386, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 163, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 3.0 (TID 348) in 228 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (116/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 3.0 (TID 387, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 139, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 3.0 (TID 372) in 98 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (117/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 3.0 (TID 369) in 113 ms on hadoop-datanode-0044.corp.cootek.com (executor 48) (118/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 3.0 (TID 388, hadoop-datanode-0090.corp.cootek.com, executor 23, partition 68, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 3.0 (TID 365) in 137 ms on hadoop-datanode-0090.corp.cootek.com (executor 23) (119/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 3.0 (TID 389, hadoop-datanode-0088.corp.cootek.com, executor 28, partition 72, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 3.0 (TID 335) in 297 ms on hadoop-datanode-0088.corp.cootek.com (executor 28) (120/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 3.0 (TID 390, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 76, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 3.0 (TID 363) in 157 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (121/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 3.0 (TID 391, hadoop-datanode-0105.corp.cootek.com, executor 45, partition 118, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 3.0 (TID 326) in 372 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (122/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 3.0 (TID 392, hadoop-datanode-0043.corp.cootek.com, executor 2, partition 168, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 3.0 (TID 339) in 308 ms on hadoop-datanode-0043.corp.cootek.com (executor 2) (123/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 3.0 (TID 366) in 158 ms on hadoop-datanode-0074.corp.cootek.com (executor 17) (124/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 3.0 (TID 368) in 148 ms on hadoop-datanode-0051.corp.cootek.com (executor 42) (125/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 3.0 (TID 393, hadoop-datanode-0039.corp.cootek.com, executor 9, partition 166, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 3.0 (TID 353) in 244 ms on hadoop-datanode-0039.corp.cootek.com (executor 9) (126/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 3.0 (TID 347) in 274 ms on hadoop-datanode-0034.corp.cootek.com (executor 27) (127/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 3.0 (TID 394, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 77, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 3.0 (TID 359) in 226 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (128/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 3.0 (TID 395, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 82, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 3.0 (TID 370) in 147 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (129/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 3.0 (TID 396, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 85, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 3.0 (TID 266) in 777 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (130/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 3.0 (TID 397, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 84, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 3.0 (TID 374) in 145 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (131/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 3.0 (TID 398, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 170, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 3.0 (TID 380) in 105 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (132/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 3.0 (TID 399, hadoop-datanode-0032.corp.cootek.com, executor 18, partition 98, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 3.0 (TID 377) in 154 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (133/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 3.0 (TID 400, hadoop-datanode-0055.corp.cootek.com, executor 10, partition 99, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 3.0 (TID 386) in 89 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (134/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 3.0 (TID 401, hadoop-datanode-0027.corp.cootek.com, executor 3, partition 103, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 3.0 (TID 376) in 170 ms on hadoop-datanode-0027.corp.cootek.com (executor 3) (135/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 3.0 (TID 402, hadoop-datanode-0048.corp.cootek.com, executor 36, partition 121, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 3.0 (TID 279) in 828 ms on hadoop-datanode-0048.corp.cootek.com (executor 36) (136/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 3.0 (TID 403, hadoop-datanode-0133.corp.cootek.com, executor 41, partition 106, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 3.0 (TID 361) in 277 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (137/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 3.0 (TID 404, hadoop-datanode-0053.corp.cootek.com, executor 14, partition 107, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 3.0 (TID 373) in 199 ms on hadoop-datanode-0053.corp.cootek.com (executor 14) (138/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 3.0 (TID 405, hadoop-datanode-0031.corp.cootek.com, executor 16, partition 148, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 3.0 (TID 383) in 124 ms on hadoop-datanode-0031.corp.cootek.com (executor 16) (139/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 3.0 (TID 406, hadoop-datanode-0117.corp.cootek.com, executor 12, partition 178, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 3.0 (TID 253) in 849 ms on hadoop-datanode-0117.corp.cootek.com (executor 12) (140/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 3.0 (TID 407, hadoop-datanode-0059.corp.cootek.com, executor 5, partition 116, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 3.0 (TID 357) in 318 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (141/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 3.0 (TID 408, hadoop-datanode-0146.corp.cootek.com, executor 8, partition 130, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 3.0 (TID 360) in 312 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (142/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 3.0 (TID 409, hadoop-datanode-0076.corp.cootek.com, executor 35, partition 181, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 3.0 (TID 387) in 142 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (143/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 3.0 (TID 384) in 151 ms on hadoop-datanode-0067.corp.cootek.com (executor 25) (144/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 3.0 (TID 410, hadoop-datanode-0136.corp.cootek.com, executor 40, partition 131, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 3.0 (TID 375) in 233 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (145/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 3.0 (TID 411, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 133, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 3.0 (TID 358) in 354 ms on hadoop-datanode-0082.corp.cootek.com (executor 7) (146/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 3.0 (TID 412, hadoop-datanode-0100.corp.cootek.com, executor 34, partition 137, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 3.0 (TID 379) in 228 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (147/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 3.0 (TID 413, hadoop-datanode-0035.corp.cootek.com, executor 6, partition 141, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 3.0 (TID 395) in 129 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (148/192)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 3.0 (TID 414, hadoop-datanode-0102.corp.cootek.com, executor 39, partition 142, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:37 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 3.0 (TID 398) in 115 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (149/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 3.0 (TID 415, hadoop-datanode-0056.corp.cootek.com, executor 38, partition 145, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 3.0 (TID 371) in 276 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (150/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 3.0 (TID 416, hadoop-datanode-0129.corp.cootek.com, executor 49, partition 190, NODE_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 3.0 (TID 396) in 133 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (151/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 3.0 (TID 417, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 150, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 3.0 (TID 397) in 128 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (152/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 3.0 (TID 418, hadoop-datanode-0138.corp.cootek.com, executor 37, partition 157, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 3.0 (TID 378) in 253 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (153/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 3.0 (TID 419, hadoop-datanode-0140.corp.cootek.com, executor 21, partition 161, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 3.0 (TID 385) in 192 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (154/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 3.0 (TID 420, hadoop-datanode-0047.corp.cootek.com, executor 50, partition 164, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 3.0 (TID 382) in 221 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (155/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 3.0 (TID 421, hadoop-datanode-0090.corp.cootek.com, executor 23, partition 167, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 3.0 (TID 388) in 204 ms on hadoop-datanode-0090.corp.cootek.com (executor 23) (156/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 3.0 (TID 422, hadoop-datanode-0117.corp.cootek.com, executor 12, partition 169, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 3.0 (TID 406) in 102 ms on hadoop-datanode-0117.corp.cootek.com (executor 12) (157/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 3.0 (TID 423, hadoop-datanode-0127.corp.cootek.com, executor 30, partition 174, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 3.0 (TID 390) in 200 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (158/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 3.0 (TID 424, hadoop-datanode-0019.corp.cootek.com, executor 4, partition 183, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 3.0 (TID 367) in 362 ms on hadoop-datanode-0019.corp.cootek.com (executor 4) (159/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 3.0 (TID 425, hadoop-datanode-0073.corp.cootek.com, executor 22, partition 188, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 3.0 (TID 394) in 208 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (160/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 3.0 (TID 405) in 161 ms on hadoop-datanode-0031.corp.cootek.com (executor 16) (161/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 3.0 (TID 392) in 239 ms on hadoop-datanode-0043.corp.cootek.com (executor 2) (162/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 3.0 (TID 404) in 173 ms on hadoop-datanode-0053.corp.cootek.com (executor 14) (163/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 3.0 (TID 407) in 155 ms on hadoop-datanode-0059.corp.cootek.com (executor 5) (164/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 3.0 (TID 389) in 272 ms on hadoop-datanode-0088.corp.cootek.com (executor 28) (165/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 3.0 (TID 401) in 189 ms on hadoop-datanode-0027.corp.cootek.com (executor 3) (166/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 3.0 (TID 408) in 157 ms on hadoop-datanode-0146.corp.cootek.com (executor 8) (167/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 3.0 (TID 409) in 144 ms on hadoop-datanode-0076.corp.cootek.com (executor 35) (168/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 3.0 (TID 393) in 252 ms on hadoop-datanode-0039.corp.cootek.com (executor 9) (169/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 3.0 (TID 400) in 201 ms on hadoop-datanode-0055.corp.cootek.com (executor 10) (170/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 3.0 (TID 403) in 194 ms on hadoop-datanode-0133.corp.cootek.com (executor 41) (171/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 3.0 (TID 412) in 146 ms on hadoop-datanode-0100.corp.cootek.com (executor 34) (172/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 3.0 (TID 414) in 140 ms on hadoop-datanode-0102.corp.cootek.com (executor 39) (173/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 3.0 (TID 418) in 131 ms on hadoop-datanode-0138.corp.cootek.com (executor 37) (174/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 3.0 (TID 421) in 129 ms on hadoop-datanode-0090.corp.cootek.com (executor 23) (175/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 3.0 (TID 391) in 331 ms on hadoop-datanode-0105.corp.cootek.com (executor 45) (176/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 3.0 (TID 420) in 160 ms on hadoop-datanode-0047.corp.cootek.com (executor 50) (177/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 3.0 (TID 410) in 201 ms on hadoop-datanode-0136.corp.cootek.com (executor 40) (178/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 3.0 (TID 417) in 170 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (179/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 3.0 (TID 419) in 166 ms on hadoop-datanode-0140.corp.cootek.com (executor 21) (180/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 3.0 (TID 411) in 198 ms on hadoop-datanode-0082.corp.cootek.com (executor 7) (181/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 3.0 (TID 413) in 196 ms on hadoop-datanode-0035.corp.cootek.com (executor 6) (182/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Marking task 64 in stage 3.0 (on hadoop-datanode-0095.corp.cootek.com) as speculatable because it ran more than 388 ms
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Marking task 102 in stage 3.0 (on hadoop-datanode-0063.corp.cootek.com) as speculatable because it ran more than 388 ms
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 102.1 in stage 3.0 (TID 426, hadoop-datanode-0092.corp.cootek.com, executor 20, partition 102, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 64.1 in stage 3.0 (TID 427, hadoop-datanode-0062.corp.cootek.com, executor 43, partition 64, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 3.0 (TID 423) in 172 ms on hadoop-datanode-0127.corp.cootek.com (executor 30) (183/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 3.0 (TID 415) in 230 ms on hadoop-datanode-0056.corp.cootek.com (executor 38) (184/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 3.0 (TID 424) in 202 ms on hadoop-datanode-0019.corp.cootek.com (executor 4) (185/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 3.0 (TID 425) in 210 ms on hadoop-datanode-0073.corp.cootek.com (executor 22) (186/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 3.0 (TID 416) in 285 ms on hadoop-datanode-0129.corp.cootek.com (executor 49) (187/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Marking task 98 in stage 3.0 (on hadoop-datanode-0032.corp.cootek.com) as speculatable because it ran more than 390 ms
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 98.1 in stage 3.0 (TID 428, hadoop-datanode-0082.corp.cootek.com, executor 7, partition 98, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 3.0 (TID 422) in 270 ms on hadoop-datanode-0117.corp.cootek.com (executor 12) (188/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Killing attempt 1 for task 64.1 in stage 3.0 (TID 427) on hadoop-datanode-0062.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0095.corp.cootek.com
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 3.0 (TID 381) in 544 ms on hadoop-datanode-0095.corp.cootek.com (executor 32) (189/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Killing attempt 1 for task 98.1 in stage 3.0 (TID 428) on hadoop-datanode-0082.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0032.corp.cootek.com
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 3.0 (TID 399) in 435 ms on hadoop-datanode-0032.corp.cootek.com (executor 18) (190/192)
19/12/13 02:03:38 WARN scheduler.TaskSetManager: Lost task 64.1 in stage 3.0 (TID 427, hadoop-datanode-0062.corp.cootek.com, executor 43): TaskKilled (another attempt succeeded)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Task 64.1 in stage 3.0 (TID 427) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Killing attempt 1 for task 102.1 in stage 3.0 (TID 426) on hadoop-datanode-0092.corp.cootek.com as the attempt 0 succeeded on hadoop-datanode-0063.corp.cootek.com
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 3.0 (TID 237) in 1317 ms on hadoop-datanode-0063.corp.cootek.com (executor 1) (191/192)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Marking task 121 in stage 3.0 (on hadoop-datanode-0048.corp.cootek.com) as speculatable because it ran more than 396 ms
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Starting task 121.1 in stage 3.0 (TID 429, hadoop-datanode-0033.corp.cootek.com, executor 33, partition 121, RACK_LOCAL, 8090 bytes)
19/12/13 02:03:38 WARN scheduler.TaskSetManager: Lost task 102.1 in stage 3.0 (TID 426, hadoop-datanode-0092.corp.cootek.com, executor 20): TaskKilled (another attempt succeeded)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Task 102.1 in stage 3.0 (TID 426) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:03:38 WARN scheduler.TaskSetManager: Lost task 98.1 in stage 3.0 (TID 428, hadoop-datanode-0082.corp.cootek.com, executor 7): TaskKilled (another attempt succeeded)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Task 98.1 in stage 3.0 (TID 428) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Killing attempt 0 for task 121.0 in stage 3.0 (TID 402) on hadoop-datanode-0048.corp.cootek.com as the attempt 1 succeeded on hadoop-datanode-0033.corp.cootek.com
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Finished task 121.1 in stage 3.0 (TID 429) in 213 ms on hadoop-datanode-0033.corp.cootek.com (executor 33) (192/192)
19/12/13 02:03:38 INFO scheduler.DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 1.612 s
19/12/13 02:03:38 INFO scheduler.DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 1.647534 s
19/12/13 02:03:38 WARN scheduler.TaskSetManager: Lost task 121.0 in stage 3.0 (TID 402, hadoop-datanode-0048.corp.cootek.com, executor 36): TaskKilled (another attempt succeeded)
19/12/13 02:03:38 INFO scheduler.TaskSetManager: Task 121.0 in stage 3.0 (TID 402) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
19/12/13 02:03:38 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
19/12/13 02:03:40 INFO datasources.FileFormatWriter: Write Job 4664c5c6-257b-4cfa-9196-e150139aca20 committed.
19/12/13 02:03:40 INFO datasources.FileFormatWriter: Finished processing stats for write job 4664c5c6-257b-4cfa-9196-e150139aca20.
19/12/13 02:03:40 INFO spark.SparkContext: Invoking stop() from shutdown hook
19/12/13 02:03:40 INFO server.AbstractConnector: Stopped Spark@198b1405{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
19/12/13 02:03:40 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.0.30:34275
19/12/13 02:03:40 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
19/12/13 02:03:40 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
19/12/13 02:03:40 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
19/12/13 02:03:40 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
19/12/13 02:03:40 INFO cluster.YarnClientSchedulerBackend: Stopped
19/12/13 02:03:40 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/12/13 02:03:40 INFO memory.MemoryStore: MemoryStore cleared
19/12/13 02:03:40 INFO storage.BlockManager: BlockManager stopped
19/12/13 02:03:40 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
19/12/13 02:03:40 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/12/13 02:03:40 INFO spark.SparkContext: Successfully stopped SparkContext
19/12/13 02:03:40 INFO util.ShutdownHookManager: Shutdown hook called
19/12/13 02:03:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-20361a15-ea86-4df2-9cbd-33580dd2df88/pyspark-420fd9fe-3f0f-42c4-b157-8b775d94cdab
19/12/13 02:03:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-20361a15-ea86-4df2-9cbd-33580dd2df88
19/12/13 02:03:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c8a4f845-c42c-4aa2-96a1-27eed4dc4f03
+ [[ 0 -ne 0 ]]
+ cd /home/ad_user/personal/ling.fang//cvr_space/click_join_trans
+ bash -x run_click_join_trans_new.sh
+ bash /home/ad_user/kinit_ad_user.sh
+ USER_NAME=ad_user
+ HADOOP_HOME=/usr/local/hadoop-2.6.3
+ common_file=/home/ling.fang/script/tools/common.sh
+ source /home/ling.fang/script/tools/common.sh
++ bash /home/ling.fang/kinit_ad_user.sh
kinit: Key table file '/home/ling.fang/ad_user.keytab' not found while getting initial credentials
++ HADOOP_BIN=/usr/local/hadoop-2.6.3/bin/hadoop
+++ pwd
++ ROOT_DIR=/home/ad_user/personal/ling.fang/cvr_space/click_join_trans
++ pwd
+ ROOT_DIR=/home/ad_user/personal/ling.fang/cvr_space/click_join_trans
+ '[' 0 -eq 1 ']'
++ date -d ' 1 days ago ' +%Y%m%d
+ DATE=20191212
++ seq 0 7
+ for idx in '`seq 0 7`'
++ date -d '0 days ago 20191212' +%Y%m%d
+ day=20191212
+ INPUTDIR='-input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ for idx in '`seq 0 7`'
++ date -d '1 days ago 20191212' +%Y%m%d
+ day=20191211
+ INPUTDIR='-input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ for idx in '`seq 0 7`'
++ date -d '2 days ago 20191212' +%Y%m%d
+ day=20191210
+ INPUTDIR='-input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ for idx in '`seq 0 7`'
++ date -d '3 days ago 20191212' +%Y%m%d
+ day=20191209
+ INPUTDIR='-input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ for idx in '`seq 0 7`'
++ date -d '4 days ago 20191212' +%Y%m%d
+ day=20191208
+ INPUTDIR='-input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ for idx in '`seq 0 7`'
++ date -d '5 days ago 20191212' +%Y%m%d
+ day=20191207
+ INPUTDIR='-input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ for idx in '`seq 0 7`'
++ date -d '6 days ago 20191212' +%Y%m%d
+ day=20191206
+ INPUTDIR='-input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/action/20191206/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/sdk_action/20191206/*" -input "/user/ad_user/car_space/action/20191206/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ for idx in '`seq 0 7`'
++ date -d '7 days ago 20191212' +%Y%m%d
+ day=20191205
+ INPUTDIR='-input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191205/*" -input "/user/ad_user/car_space/sdk_action/20191206/*" -input "/user/ad_user/car_space/action/20191206/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/action/20191205/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191205/*" -input "/user/ad_user/car_space/sdk_action/20191206/*" -input "/user/ad_user/car_space/action/20191206/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/car_space/sdk_action/20191205/*" -input "/user/ad_user/car_space/action/20191205/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191205/*" -input "/user/ad_user/car_space/sdk_action/20191206/*" -input "/user/ad_user/car_space/action/20191206/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/ocpc/click_join_trans/clk_path/" -input "/user/ad_user/car_space/sdk_action/20191205/*" -input "/user/ad_user/car_space/action/20191205/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191205/*" -input "/user/ad_user/car_space/sdk_action/20191206/*" -input "/user/ad_user/car_space/action/20191206/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ INPUTDIR='-input "/user/ad_user/ocpc/click_join_trans/trans_path/" -input "/user/ad_user/ocpc/click_join_trans/clk_path/" -input "/user/ad_user/car_space/sdk_action/20191205/*" -input "/user/ad_user/car_space/action/20191205/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191205/*" -input "/user/ad_user/car_space/sdk_action/20191206/*" -input "/user/ad_user/car_space/action/20191206/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*" '
+ echo -input '"/user/ad_user/ocpc/click_join_trans/trans_path/"' -input '"/user/ad_user/ocpc/click_join_trans/clk_path/"' -input '"/user/ad_user/car_space/sdk_action/20191205/*"' -input '"/user/ad_user/car_space/action/20191205/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191205/*"' -input '"/user/ad_user/car_space/sdk_action/20191206/*"' -input '"/user/ad_user/car_space/action/20191206/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*"' -input '"/user/ad_user/car_space/sdk_action/20191207/*"' -input '"/user/ad_user/car_space/action/20191207/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*"' -input '"/user/ad_user/car_space/sdk_action/20191208/*"' -input '"/user/ad_user/car_space/action/20191208/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*"' -input '"/user/ad_user/car_space/sdk_action/20191209/*"' -input '"/user/ad_user/car_space/action/20191209/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*"' -input '"/user/ad_user/car_space/sdk_action/20191210/*"' -input '"/user/ad_user/car_space/action/20191210/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*"' -input '"/user/ad_user/car_space/sdk_action/20191211/*"' -input '"/user/ad_user/car_space/action/20191211/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*"' -input '"/user/ad_user/car_space/sdk_action/20191212/*"' -input '"/user/ad_user/car_space/action/20191212/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*"'
-input "/user/ad_user/ocpc/click_join_trans/trans_path/" -input "/user/ad_user/ocpc/click_join_trans/clk_path/" -input "/user/ad_user/car_space/sdk_action/20191205/*" -input "/user/ad_user/car_space/action/20191205/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191205/*" -input "/user/ad_user/car_space/sdk_action/20191206/*" -input "/user/ad_user/car_space/action/20191206/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*" -input "/user/ad_user/car_space/sdk_action/20191207/*" -input "/user/ad_user/car_space/action/20191207/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*" -input "/user/ad_user/car_space/sdk_action/20191208/*" -input "/user/ad_user/car_space/action/20191208/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*" -input "/user/ad_user/car_space/sdk_action/20191209/*" -input "/user/ad_user/car_space/action/20191209/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*" -input "/user/ad_user/car_space/sdk_action/20191210/*" -input "/user/ad_user/car_space/action/20191210/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*" -input "/user/ad_user/car_space/sdk_action/20191211/*" -input "/user/ad_user/car_space/action/20191211/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*" -input "/user/ad_user/car_space/sdk_action/20191212/*" -input "/user/ad_user/car_space/action/20191212/*" -input "/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*"
+ OUTPUTDIR=/user/ad_user/ocpc/click_join_trans/20191212/
+ FLAG_PREFIX=flag_ods_usage_data_h_usage_naga_dsp
+ log_type='click transform'
++ seq 0 1
+ for idx in '`seq 0 1`'
++ date -d '0 days ago 20191212' +%Y%m%d
+ day=20191212
+ for tp in '$log_type'
+ done_file=flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212
+ loop_check flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 140 2
+ file=flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212
+ ready=0
+ check_time=140
+ mode=2
+ echo 'input params:flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 140 2'
input params:flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 140 2
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 2 -eq 1 ']'
+ '[' 2 -eq 2 ']'
+ redis_check_ready
++ python /home/ling.fang/script/tools/get_flag.py flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212
+ res_code=True
+ [[ XTrue == X\T\r\u\e ]]
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog 'flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:41
+ echo '[20191213-02:03:41] flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212' is 'ready!'
[20191213-02:03:41] flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 is ready!
+ return 0
+ for tp in '$log_type'
+ done_file=flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212
+ loop_check flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 140 2
+ file=flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212
+ ready=0
+ check_time=140
+ mode=2
+ echo 'input params:flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 140 2'
input params:flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 140 2
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 2 -eq 1 ']'
+ '[' 2 -eq 2 ']'
+ redis_check_ready
++ python /home/ling.fang/script/tools/get_flag.py flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212
+ res_code=True
+ [[ XTrue == X\T\r\u\e ]]
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog 'flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:42
+ echo '[20191213-02:03:42] flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212' is 'ready!'
[20191213-02:03:42] flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 is ready!
+ return 0
+ for idx in '`seq 0 1`'
++ date -d '1 days ago 20191212' +%Y%m%d
+ day=20191211
+ for tp in '$log_type'
+ done_file=flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212
+ loop_check flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 140 2
+ file=flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212
+ ready=0
+ check_time=140
+ mode=2
+ echo 'input params:flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 140 2'
input params:flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 140 2
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 2 -eq 1 ']'
+ '[' 2 -eq 2 ']'
+ redis_check_ready
++ python /home/ling.fang/script/tools/get_flag.py flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212
+ res_code=True
+ [[ XTrue == X\T\r\u\e ]]
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog 'flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:43
+ echo '[20191213-02:03:43] flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212' is 'ready!'
[20191213-02:03:43] flag_ods_usage_data_h_usage_naga_dsp_click_cn##20191212 is ready!
+ return 0
+ for tp in '$log_type'
+ done_file=flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212
+ loop_check flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 140 2
+ file=flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212
+ ready=0
+ check_time=140
+ mode=2
+ echo 'input params:flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 140 2'
input params:flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 140 2
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 2 -eq 1 ']'
+ '[' 2 -eq 2 ']'
+ redis_check_ready
++ python /home/ling.fang/script/tools/get_flag.py flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212
+ res_code=True
+ [[ XTrue == X\T\r\u\e ]]
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog 'flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:43
+ echo '[20191213-02:03:43] flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212' is 'ready!'
[20191213-02:03:43] flag_ods_usage_data_h_usage_naga_dsp_transform_cn##20191212 is ready!
+ return 0
++ seq 0 7
+ for idx in '`seq 0 7`'
++ date -d '0 days ago 20191212' +%Y%m%d
+ day=20191212
+ done_file=/user/ad_user/car_space/action/20191212/_SUCCESS
+ loop_check /user/ad_user/car_space/action/20191212/_SUCCESS 140 1
+ file=/user/ad_user/car_space/action/20191212/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/action/20191212/_SUCCESS 140 1'
input params:/user/ad_user/car_space/action/20191212/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/action/20191212/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/action/20191212/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:45
+ echo '[20191213-02:03:45] /user/ad_user/car_space/action/20191212/_SUCCESS' is 'ready!'
[20191213-02:03:45] /user/ad_user/car_space/action/20191212/_SUCCESS is ready!
+ return 0
+ done_file=/user/ad_user/car_space/sdk_action/20191212/_SUCCESS
+ loop_check /user/ad_user/car_space/sdk_action/20191212/_SUCCESS 140 1
+ file=/user/ad_user/car_space/sdk_action/20191212/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/sdk_action/20191212/_SUCCESS 140 1'
input params:/user/ad_user/car_space/sdk_action/20191212/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/sdk_action/20191212/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/sdk_action/20191212/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:47
+ echo '[20191213-02:03:47] /user/ad_user/car_space/sdk_action/20191212/_SUCCESS' is 'ready!'
[20191213-02:03:47] /user/ad_user/car_space/sdk_action/20191212/_SUCCESS is ready!
+ return 0
+ for idx in '`seq 0 7`'
++ date -d '1 days ago 20191212' +%Y%m%d
+ day=20191211
+ done_file=/user/ad_user/car_space/action/20191211/_SUCCESS
+ loop_check /user/ad_user/car_space/action/20191211/_SUCCESS 140 1
+ file=/user/ad_user/car_space/action/20191211/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/action/20191211/_SUCCESS 140 1'
input params:/user/ad_user/car_space/action/20191211/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/action/20191211/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/action/20191211/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:49
+ echo '[20191213-02:03:49] /user/ad_user/car_space/action/20191211/_SUCCESS' is 'ready!'
[20191213-02:03:49] /user/ad_user/car_space/action/20191211/_SUCCESS is ready!
+ return 0
+ done_file=/user/ad_user/car_space/sdk_action/20191211/_SUCCESS
+ loop_check /user/ad_user/car_space/sdk_action/20191211/_SUCCESS 140 1
+ file=/user/ad_user/car_space/sdk_action/20191211/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/sdk_action/20191211/_SUCCESS 140 1'
input params:/user/ad_user/car_space/sdk_action/20191211/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/sdk_action/20191211/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/sdk_action/20191211/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:51
+ echo '[20191213-02:03:51] /user/ad_user/car_space/sdk_action/20191211/_SUCCESS' is 'ready!'
[20191213-02:03:51] /user/ad_user/car_space/sdk_action/20191211/_SUCCESS is ready!
+ return 0
+ for idx in '`seq 0 7`'
++ date -d '2 days ago 20191212' +%Y%m%d
+ day=20191210
+ done_file=/user/ad_user/car_space/action/20191210/_SUCCESS
+ loop_check /user/ad_user/car_space/action/20191210/_SUCCESS 140 1
+ file=/user/ad_user/car_space/action/20191210/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/action/20191210/_SUCCESS 140 1'
input params:/user/ad_user/car_space/action/20191210/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/action/20191210/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/action/20191210/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:53
+ echo '[20191213-02:03:53] /user/ad_user/car_space/action/20191210/_SUCCESS' is 'ready!'
[20191213-02:03:53] /user/ad_user/car_space/action/20191210/_SUCCESS is ready!
+ return 0
+ done_file=/user/ad_user/car_space/sdk_action/20191210/_SUCCESS
+ loop_check /user/ad_user/car_space/sdk_action/20191210/_SUCCESS 140 1
+ file=/user/ad_user/car_space/sdk_action/20191210/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/sdk_action/20191210/_SUCCESS 140 1'
input params:/user/ad_user/car_space/sdk_action/20191210/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/sdk_action/20191210/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/sdk_action/20191210/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:55
+ echo '[20191213-02:03:55] /user/ad_user/car_space/sdk_action/20191210/_SUCCESS' is 'ready!'
[20191213-02:03:55] /user/ad_user/car_space/sdk_action/20191210/_SUCCESS is ready!
+ return 0
+ for idx in '`seq 0 7`'
++ date -d '3 days ago 20191212' +%Y%m%d
+ day=20191209
+ done_file=/user/ad_user/car_space/action/20191209/_SUCCESS
+ loop_check /user/ad_user/car_space/action/20191209/_SUCCESS 140 1
+ file=/user/ad_user/car_space/action/20191209/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/action/20191209/_SUCCESS 140 1'
input params:/user/ad_user/car_space/action/20191209/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/action/20191209/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/action/20191209/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:57
+ echo '[20191213-02:03:57] /user/ad_user/car_space/action/20191209/_SUCCESS' is 'ready!'
[20191213-02:03:57] /user/ad_user/car_space/action/20191209/_SUCCESS is ready!
+ return 0
+ done_file=/user/ad_user/car_space/sdk_action/20191209/_SUCCESS
+ loop_check /user/ad_user/car_space/sdk_action/20191209/_SUCCESS 140 1
+ file=/user/ad_user/car_space/sdk_action/20191209/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/sdk_action/20191209/_SUCCESS 140 1'
input params:/user/ad_user/car_space/sdk_action/20191209/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/sdk_action/20191209/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/sdk_action/20191209/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:03:59
+ echo '[20191213-02:03:59] /user/ad_user/car_space/sdk_action/20191209/_SUCCESS' is 'ready!'
[20191213-02:03:59] /user/ad_user/car_space/sdk_action/20191209/_SUCCESS is ready!
+ return 0
+ for idx in '`seq 0 7`'
++ date -d '4 days ago 20191212' +%Y%m%d
+ day=20191208
+ done_file=/user/ad_user/car_space/action/20191208/_SUCCESS
+ loop_check /user/ad_user/car_space/action/20191208/_SUCCESS 140 1
+ file=/user/ad_user/car_space/action/20191208/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/action/20191208/_SUCCESS 140 1'
input params:/user/ad_user/car_space/action/20191208/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/action/20191208/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/action/20191208/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:04:01
+ echo '[20191213-02:04:01] /user/ad_user/car_space/action/20191208/_SUCCESS' is 'ready!'
[20191213-02:04:01] /user/ad_user/car_space/action/20191208/_SUCCESS is ready!
+ return 0
+ done_file=/user/ad_user/car_space/sdk_action/20191208/_SUCCESS
+ loop_check /user/ad_user/car_space/sdk_action/20191208/_SUCCESS 140 1
+ file=/user/ad_user/car_space/sdk_action/20191208/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/sdk_action/20191208/_SUCCESS 140 1'
input params:/user/ad_user/car_space/sdk_action/20191208/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/sdk_action/20191208/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/sdk_action/20191208/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:04:03
+ echo '[20191213-02:04:03] /user/ad_user/car_space/sdk_action/20191208/_SUCCESS' is 'ready!'
[20191213-02:04:03] /user/ad_user/car_space/sdk_action/20191208/_SUCCESS is ready!
+ return 0
+ for idx in '`seq 0 7`'
++ date -d '5 days ago 20191212' +%Y%m%d
+ day=20191207
+ done_file=/user/ad_user/car_space/action/20191207/_SUCCESS
+ loop_check /user/ad_user/car_space/action/20191207/_SUCCESS 140 1
+ file=/user/ad_user/car_space/action/20191207/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/action/20191207/_SUCCESS 140 1'
input params:/user/ad_user/car_space/action/20191207/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/action/20191207/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/action/20191207/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:04:07
+ echo '[20191213-02:04:07] /user/ad_user/car_space/action/20191207/_SUCCESS' is 'ready!'
[20191213-02:04:07] /user/ad_user/car_space/action/20191207/_SUCCESS is ready!
+ return 0
+ done_file=/user/ad_user/car_space/sdk_action/20191207/_SUCCESS
+ loop_check /user/ad_user/car_space/sdk_action/20191207/_SUCCESS 140 1
+ file=/user/ad_user/car_space/sdk_action/20191207/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/sdk_action/20191207/_SUCCESS 140 1'
input params:/user/ad_user/car_space/sdk_action/20191207/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/sdk_action/20191207/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/sdk_action/20191207/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:04:09
+ echo '[20191213-02:04:09] /user/ad_user/car_space/sdk_action/20191207/_SUCCESS' is 'ready!'
[20191213-02:04:09] /user/ad_user/car_space/sdk_action/20191207/_SUCCESS is ready!
+ return 0
+ for idx in '`seq 0 7`'
++ date -d '6 days ago 20191212' +%Y%m%d
+ day=20191206
+ done_file=/user/ad_user/car_space/action/20191206/_SUCCESS
+ loop_check /user/ad_user/car_space/action/20191206/_SUCCESS 140 1
+ file=/user/ad_user/car_space/action/20191206/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/action/20191206/_SUCCESS 140 1'
input params:/user/ad_user/car_space/action/20191206/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/action/20191206/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/action/20191206/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:04:11
+ echo '[20191213-02:04:11] /user/ad_user/car_space/action/20191206/_SUCCESS' is 'ready!'
[20191213-02:04:11] /user/ad_user/car_space/action/20191206/_SUCCESS is ready!
+ return 0
+ done_file=/user/ad_user/car_space/sdk_action/20191206/_SUCCESS
+ loop_check /user/ad_user/car_space/sdk_action/20191206/_SUCCESS 140 1
+ file=/user/ad_user/car_space/sdk_action/20191206/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/sdk_action/20191206/_SUCCESS 140 1'
input params:/user/ad_user/car_space/sdk_action/20191206/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/sdk_action/20191206/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/sdk_action/20191206/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:04:13
+ echo '[20191213-02:04:13] /user/ad_user/car_space/sdk_action/20191206/_SUCCESS' is 'ready!'
[20191213-02:04:13] /user/ad_user/car_space/sdk_action/20191206/_SUCCESS is ready!
+ return 0
+ for idx in '`seq 0 7`'
++ date -d '7 days ago 20191212' +%Y%m%d
+ day=20191205
+ done_file=/user/ad_user/car_space/action/20191205/_SUCCESS
+ loop_check /user/ad_user/car_space/action/20191205/_SUCCESS 140 1
+ file=/user/ad_user/car_space/action/20191205/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/action/20191205/_SUCCESS 140 1'
input params:/user/ad_user/car_space/action/20191205/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/action/20191205/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/action/20191205/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:04:15
+ echo '[20191213-02:04:15] /user/ad_user/car_space/action/20191205/_SUCCESS' is 'ready!'
[20191213-02:04:15] /user/ad_user/car_space/action/20191205/_SUCCESS is ready!
+ return 0
+ done_file=/user/ad_user/car_space/sdk_action/20191205/_SUCCESS
+ loop_check /user/ad_user/car_space/sdk_action/20191205/_SUCCESS 140 1
+ file=/user/ad_user/car_space/sdk_action/20191205/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/car_space/sdk_action/20191205/_SUCCESS 140 1'
input params:/user/ad_user/car_space/sdk_action/20191205/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/car_space/sdk_action/20191205/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/car_space/sdk_action/20191205/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:04:17
+ echo '[20191213-02:04:17] /user/ad_user/car_space/sdk_action/20191205/_SUCCESS' is 'ready!'
[20191213-02:04:17] /user/ad_user/car_space/sdk_action/20191205/_SUCCESS is ready!
+ return 0
+ TASK_NAME=click_join_trans
+ hadoop_run
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -rm -r /user/ad_user/ocpc/click_join_trans/20191212/
rm: `/user/ad_user/ocpc/click_join_trans/20191212/': No such file or directory
+ /usr/local/hadoop-2.6.3/bin/hadoop jar /usr/local/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar -libjars /home/ling.fang/script/tools/mr_plugins-1.0.jar -D map.output.key.field.separator=# -D num.key.fields.for.partition=1 -D mapreduce.reduce.memory.mb=6144 -D mapreduce.reduce.java.opts=-Xmx3276m -Dmapred.map.tasks.speculative.execution=false -Dmapred.reduce.tasks.speculative.execution=false -Dstream.non.zero.exit.is.failure=false -Dmapred.job.priority=HIGH -Dmapreduce.job.queuename=root.ad-root.etl.dailyetl.high -Dmapred.map.tasks=500 -Dmapred.reduce.tasks=20 -Dmapred.job.map.capacity=500 -Dmapred.job.reduce.capacity=20 -Dmapred.job.name=click_join_trans_20191212 -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner -outputformat org.apache.hadoop.mapred.lib.SuffixMultipleTextOutputFormat -file mapper_new.py -file reducer_new.py -mapper 'python mapper_new.py' -reducer 'python reducer_new.py 20191212' -input '"/user/ad_user/ocpc/click_join_trans/trans_path/"' -input '"/user/ad_user/ocpc/click_join_trans/clk_path/"' -input '"/user/ad_user/car_space/sdk_action/20191205/*"' -input '"/user/ad_user/car_space/action/20191205/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191205/*"' -input '"/user/ad_user/car_space/sdk_action/20191206/*"' -input '"/user/ad_user/car_space/action/20191206/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191206/*"' -input '"/user/ad_user/car_space/sdk_action/20191207/*"' -input '"/user/ad_user/car_space/action/20191207/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191207/*"' -input '"/user/ad_user/car_space/sdk_action/20191208/*"' -input '"/user/ad_user/car_space/action/20191208/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191208/*"' -input '"/user/ad_user/car_space/sdk_action/20191209/*"' -input '"/user/ad_user/car_space/action/20191209/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191209/*"' -input '"/user/ad_user/car_space/sdk_action/20191210/*"' -input '"/user/ad_user/car_space/action/20191210/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191210/*"' -input '"/user/ad_user/car_space/sdk_action/20191211/*"' -input '"/user/ad_user/car_space/action/20191211/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191211/*"' -input '"/user/ad_user/car_space/sdk_action/20191212/*"' -input '"/user/ad_user/car_space/action/20191212/*"' -input '"/data/external/dw/dw_usage_naga_adx_sspstat_h/dt=20191212/*"' -output /user/ad_user/ocpc/click_join_trans/20191212/
19/12/13 02:04:20 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator
19/12/13 02:04:20 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
19/12/13 02:04:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
19/12/13 02:04:20 INFO Configuration.deprecation: mapred.job.priority is deprecated. Instead, use mapreduce.job.priority
19/12/13 02:04:20 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
19/12/13 02:04:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
19/12/13 02:04:20 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name
19/12/13 02:04:20 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
packageJobJar: [mapper_new.py, reducer_new.py, /tmp/hadoop-unjar4651369529005864177/] [] /tmp/streamjob1087249376560965866.jar tmpDir=null
19/12/13 02:04:20 INFO client.RMProxy: Connecting to ResourceManager at hadoop2-namenode.corp.cootek.com/192.168.30.10:8032
19/12/13 02:04:21 INFO client.RMProxy: Connecting to ResourceManager at hadoop2-namenode.corp.cootek.com/192.168.30.10:8032
19/12/13 02:04:21 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5998182 for ad_user on ha-hdfs:cootek
19/12/13 02:04:21 INFO security.TokenCache: Got dt for hdfs://cootek; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:cootek, Ident: (HDFS_DELEGATION_TOKEN token 5998182 for ad_user)
19/12/13 02:04:21 WARN token.Token: Cannot find class for token kind kms-dt
19/12/13 02:04:21 INFO security.TokenCache: Got dt for hdfs://cootek; Kind: kms-dt, Service: 192.168.30.152:16000, Ident: 00 07 61 64 5f 75 73 65 72 06 68 61 64 6f 6f 70 00 8a 01 6e fb 48 7a 71 8a 01 6f 1f 54 fe 71 8d 41 ef ac 8e 02 a4
19/12/13 02:04:22 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries
19/12/13 02:04:22 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 52decc77982b58949890770d22720a91adce0c3f]
19/12/13 02:04:24 INFO mapred.FileInputFormat: Total input paths to process : 7746
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.104:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.111:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.220:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.132:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.106:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.44:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.31:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.75:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.118:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.131:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.130:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.19:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.22:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.109:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.40:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.114:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.219:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.225:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.53:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.113:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.129:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.61:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.79:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.67:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.23:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.51:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.74:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.41:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.107:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.232:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.32:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.215:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.49:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.211:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.227:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.204:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.105:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.34:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.39:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.231:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.119:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.30:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.224:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.37:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.128:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.24:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.58:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.45:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.50:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.73:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.122:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.78:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.27:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.117:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.77:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.226:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.68:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.36:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.42:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.115:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.43:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.216:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.206:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.230:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.21:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.217:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.64:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.112:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.88:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.18:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.221:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.213:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.69:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.209:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.203:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.210:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.89:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.229:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.205:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.59:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.155:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.70:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.72:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.20:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.65:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.125:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.228:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.218:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.62:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.223:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.90:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.108:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.33:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.28:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.124:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.208:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.29:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.60:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.63:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.154:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.25:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.57:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.127:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.56:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.87:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.48:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.156:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.103:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.66:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.222:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.120:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.214:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.55:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.35:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.52:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.133:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.123:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.126:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.86:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.76:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.38:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.47:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.207:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.71:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.116:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.212:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.121:61004
19/12/13 02:04:24 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.54:61004
19/12/13 02:04:25 INFO mapreduce.JobSubmitter: number of splits:7993
19/12/13 02:04:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1573729450353_315993
19/12/13 02:04:25 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:cootek, Ident: (HDFS_DELEGATION_TOKEN token 5998182 for ad_user)
19/12/13 02:04:25 WARN token.Token: Cannot find class for token kind kms-dt
19/12/13 02:04:25 WARN token.Token: Cannot find class for token kind kms-dt
Kind: kms-dt, Service: 192.168.30.152:16000, Ident: 00 07 61 64 5f 75 73 65 72 06 68 61 64 6f 6f 70 00 8a 01 6e fb 48 7a 71 8a 01 6f 1f 54 fe 71 8d 41 ef ac 8e 02 a4
19/12/13 02:04:25 INFO impl.YarnClientImpl: Submitted application application_1573729450353_315993
19/12/13 02:04:25 INFO mapreduce.Job: The url to track the job: http://hadoop2-namenode.corp.cootek.com:8088/proxy/application_1573729450353_315993/
19/12/13 02:04:25 INFO mapreduce.Job: Running job: job_1573729450353_315993
19/12/13 02:04:40 INFO mapreduce.Job: Job job_1573729450353_315993 running in uber mode : false
19/12/13 02:04:40 INFO mapreduce.Job:  map 0% reduce 0%
19/12/13 02:05:27 INFO mapreduce.Job:  map 1% reduce 0%
19/12/13 02:05:40 INFO mapreduce.Job:  map 2% reduce 0%
19/12/13 02:06:05 INFO mapreduce.Job:  map 3% reduce 0%
19/12/13 02:08:12 INFO mapreduce.Job:  map 4% reduce 0%
19/12/13 02:09:05 INFO mapreduce.Job:  map 5% reduce 0%
19/12/13 02:11:08 INFO mapreduce.Job:  map 6% reduce 0%
19/12/13 02:12:18 INFO mapreduce.Job:  map 7% reduce 0%
19/12/13 02:13:02 INFO mapreduce.Job:  map 8% reduce 0%
19/12/13 02:13:27 INFO mapreduce.Job:  map 9% reduce 0%
19/12/13 02:13:44 INFO mapreduce.Job:  map 10% reduce 0%
19/12/13 02:14:01 INFO mapreduce.Job:  map 11% reduce 0%
19/12/13 02:14:18 INFO mapreduce.Job:  map 12% reduce 0%
19/12/13 02:14:49 INFO mapreduce.Job:  map 13% reduce 0%
19/12/13 02:14:58 INFO mapreduce.Job:  map 14% reduce 0%
19/12/13 02:15:06 INFO mapreduce.Job:  map 15% reduce 0%
19/12/13 02:15:11 INFO mapreduce.Job:  map 16% reduce 0%
19/12/13 02:15:17 INFO mapreduce.Job:  map 17% reduce 0%
19/12/13 02:15:29 INFO mapreduce.Job:  map 18% reduce 0%
19/12/13 02:15:46 INFO mapreduce.Job:  map 19% reduce 0%
19/12/13 02:15:59 INFO mapreduce.Job:  map 20% reduce 0%
19/12/13 02:16:12 INFO mapreduce.Job:  map 21% reduce 0%
19/12/13 02:16:32 INFO mapreduce.Job:  map 22% reduce 0%
19/12/13 02:17:09 INFO mapreduce.Job:  map 23% reduce 0%
19/12/13 02:19:16 INFO mapreduce.Job:  map 24% reduce 0%
19/12/13 02:19:33 INFO mapreduce.Job:  map 25% reduce 0%
19/12/13 02:19:45 INFO mapreduce.Job:  map 26% reduce 0%
19/12/13 02:19:57 INFO mapreduce.Job:  map 27% reduce 0%
19/12/13 02:20:06 INFO mapreduce.Job:  map 28% reduce 0%
19/12/13 02:20:14 INFO mapreduce.Job:  map 29% reduce 0%
19/12/13 02:20:20 INFO mapreduce.Job:  map 30% reduce 0%
19/12/13 02:20:26 INFO mapreduce.Job:  map 31% reduce 0%
19/12/13 02:20:30 INFO mapreduce.Job:  map 32% reduce 0%
19/12/13 02:20:35 INFO mapreduce.Job:  map 33% reduce 0%
19/12/13 02:20:39 INFO mapreduce.Job:  map 34% reduce 0%
19/12/13 02:20:43 INFO mapreduce.Job:  map 35% reduce 0%
19/12/13 02:20:47 INFO mapreduce.Job:  map 36% reduce 0%
19/12/13 02:20:52 INFO mapreduce.Job:  map 37% reduce 0%
19/12/13 02:20:56 INFO mapreduce.Job:  map 38% reduce 0%
19/12/13 02:21:00 INFO mapreduce.Job:  map 39% reduce 0%
19/12/13 02:21:04 INFO mapreduce.Job:  map 40% reduce 0%
19/12/13 02:21:09 INFO mapreduce.Job:  map 41% reduce 0%
19/12/13 02:21:13 INFO mapreduce.Job:  map 42% reduce 0%
19/12/13 02:21:18 INFO mapreduce.Job:  map 43% reduce 0%
19/12/13 02:21:22 INFO mapreduce.Job:  map 44% reduce 0%
19/12/13 02:21:26 INFO mapreduce.Job:  map 45% reduce 0%
19/12/13 02:21:32 INFO mapreduce.Job:  map 46% reduce 0%
19/12/13 02:21:36 INFO mapreduce.Job:  map 47% reduce 0%
19/12/13 02:21:40 INFO mapreduce.Job:  map 48% reduce 0%
19/12/13 02:21:45 INFO mapreduce.Job:  map 49% reduce 0%
19/12/13 02:21:48 INFO mapreduce.Job:  map 50% reduce 0%
19/12/13 02:21:51 INFO mapreduce.Job:  map 51% reduce 0%
19/12/13 02:21:54 INFO mapreduce.Job:  map 52% reduce 0%
19/12/13 02:21:58 INFO mapreduce.Job:  map 53% reduce 0%
19/12/13 02:22:02 INFO mapreduce.Job:  map 54% reduce 0%
19/12/13 02:22:04 INFO mapreduce.Job:  map 55% reduce 0%
19/12/13 02:22:10 INFO mapreduce.Job:  map 56% reduce 0%
19/12/13 02:22:13 INFO mapreduce.Job:  map 57% reduce 0%
19/12/13 02:22:15 INFO mapreduce.Job:  map 58% reduce 0%
19/12/13 02:22:17 INFO mapreduce.Job:  map 59% reduce 0%
19/12/13 02:22:21 INFO mapreduce.Job:  map 60% reduce 0%
19/12/13 02:22:24 INFO mapreduce.Job:  map 61% reduce 0%
19/12/13 02:22:26 INFO mapreduce.Job:  map 62% reduce 0%
19/12/13 02:22:28 INFO mapreduce.Job:  map 63% reduce 0%
19/12/13 02:22:32 INFO mapreduce.Job:  map 64% reduce 0%
19/12/13 02:22:35 INFO mapreduce.Job:  map 65% reduce 0%
19/12/13 02:22:37 INFO mapreduce.Job:  map 66% reduce 0%
19/12/13 02:22:39 INFO mapreduce.Job:  map 67% reduce 0%
19/12/13 02:22:42 INFO mapreduce.Job:  map 68% reduce 0%
19/12/13 02:22:45 INFO mapreduce.Job:  map 69% reduce 0%
19/12/13 02:22:47 INFO mapreduce.Job:  map 70% reduce 0%
19/12/13 02:22:49 INFO mapreduce.Job:  map 71% reduce 0%
19/12/13 02:22:52 INFO mapreduce.Job:  map 72% reduce 0%
19/12/13 02:22:54 INFO mapreduce.Job:  map 73% reduce 0%
19/12/13 02:22:56 INFO mapreduce.Job:  map 74% reduce 0%
19/12/13 02:22:58 INFO mapreduce.Job:  map 75% reduce 0%
19/12/13 02:23:01 INFO mapreduce.Job:  map 76% reduce 0%
19/12/13 02:23:03 INFO mapreduce.Job:  map 77% reduce 0%
19/12/13 02:23:06 INFO mapreduce.Job:  map 78% reduce 0%
19/12/13 02:23:09 INFO mapreduce.Job:  map 79% reduce 0%
19/12/13 02:23:11 INFO mapreduce.Job:  map 80% reduce 0%
19/12/13 02:23:13 INFO mapreduce.Job:  map 81% reduce 0%
19/12/13 02:23:17 INFO mapreduce.Job:  map 82% reduce 0%
19/12/13 02:23:19 INFO mapreduce.Job:  map 83% reduce 0%
19/12/13 02:23:22 INFO mapreduce.Job:  map 84% reduce 0%
19/12/13 02:23:24 INFO mapreduce.Job:  map 85% reduce 0%
19/12/13 02:23:25 INFO mapreduce.Job:  map 85% reduce 1%
19/12/13 02:23:26 INFO mapreduce.Job:  map 85% reduce 2%
19/12/13 02:23:27 INFO mapreduce.Job:  map 85% reduce 3%
19/12/13 02:23:29 INFO mapreduce.Job:  map 85% reduce 4%
19/12/13 02:23:32 INFO mapreduce.Job:  map 85% reduce 5%
19/12/13 02:23:34 INFO mapreduce.Job:  map 86% reduce 5%
19/12/13 02:23:36 INFO mapreduce.Job:  map 86% reduce 6%
19/12/13 02:23:40 INFO mapreduce.Job:  map 86% reduce 7%
19/12/13 02:23:43 INFO mapreduce.Job:  map 86% reduce 8%
19/12/13 02:23:46 INFO mapreduce.Job:  map 86% reduce 9%
19/12/13 02:23:51 INFO mapreduce.Job:  map 86% reduce 10%
19/12/13 02:23:55 INFO mapreduce.Job:  map 86% reduce 11%
19/12/13 02:23:59 INFO mapreduce.Job:  map 86% reduce 12%
19/12/13 02:24:03 INFO mapreduce.Job:  map 86% reduce 13%
19/12/13 02:24:06 INFO mapreduce.Job:  map 86% reduce 14%
19/12/13 02:24:11 INFO mapreduce.Job:  map 86% reduce 15%
19/12/13 02:24:17 INFO mapreduce.Job:  map 86% reduce 16%
19/12/13 02:24:19 INFO mapreduce.Job:  map 86% reduce 17%
19/12/13 02:24:20 INFO mapreduce.Job:  map 86% reduce 18%
19/12/13 02:24:23 INFO mapreduce.Job:  map 86% reduce 19%
19/12/13 02:24:26 INFO mapreduce.Job:  map 86% reduce 20%
19/12/13 02:24:30 INFO mapreduce.Job:  map 86% reduce 21%
19/12/13 02:24:32 INFO mapreduce.Job:  map 87% reduce 21%
19/12/13 02:24:36 INFO mapreduce.Job:  map 87% reduce 22%
19/12/13 02:24:39 INFO mapreduce.Job:  map 88% reduce 23%
19/12/13 02:24:45 INFO mapreduce.Job:  map 89% reduce 24%
19/12/13 02:24:49 INFO mapreduce.Job:  map 89% reduce 25%
19/12/13 02:24:53 INFO mapreduce.Job:  map 90% reduce 25%
19/12/13 02:24:56 INFO mapreduce.Job:  map 90% reduce 26%
19/12/13 02:24:59 INFO mapreduce.Job:  map 91% reduce 26%
19/12/13 02:25:01 INFO mapreduce.Job:  map 91% reduce 27%
19/12/13 02:25:06 INFO mapreduce.Job:  map 92% reduce 28%
19/12/13 02:25:12 INFO mapreduce.Job:  map 92% reduce 29%
19/12/13 02:25:18 INFO mapreduce.Job:  map 93% reduce 29%
19/12/13 02:25:19 INFO mapreduce.Job:  map 93% reduce 30%
19/12/13 02:25:29 INFO mapreduce.Job:  map 93% reduce 31%
19/12/13 02:25:34 INFO mapreduce.Job:  map 94% reduce 31%
19/12/13 02:25:45 INFO mapreduce.Job:  map 95% reduce 31%
19/12/13 02:25:47 INFO mapreduce.Job:  map 95% reduce 32%
19/12/13 02:25:57 INFO mapreduce.Job:  map 96% reduce 32%
19/12/13 02:26:07 INFO mapreduce.Job:  map 97% reduce 32%
19/12/13 02:27:17 INFO mapreduce.Job:  map 97% reduce 0%
19/12/13 02:27:27 INFO mapreduce.Job:  map 98% reduce 0%
19/12/13 02:27:28 INFO mapreduce.Job:  map 98% reduce 1%
19/12/13 02:27:31 INFO mapreduce.Job:  map 98% reduce 2%
19/12/13 02:27:40 INFO mapreduce.Job:  map 98% reduce 3%
19/12/13 02:27:43 INFO mapreduce.Job:  map 99% reduce 4%
19/12/13 02:27:48 INFO mapreduce.Job:  map 99% reduce 5%
19/12/13 02:27:52 INFO mapreduce.Job:  map 99% reduce 6%
19/12/13 02:27:58 INFO mapreduce.Job:  map 99% reduce 7%
19/12/13 02:28:05 INFO mapreduce.Job:  map 99% reduce 8%
19/12/13 02:28:11 INFO mapreduce.Job:  map 99% reduce 9%
19/12/13 02:28:17 INFO mapreduce.Job:  map 99% reduce 10%
19/12/13 02:28:23 INFO mapreduce.Job:  map 99% reduce 11%
19/12/13 02:28:27 INFO mapreduce.Job:  map 99% reduce 12%
19/12/13 02:28:33 INFO mapreduce.Job:  map 99% reduce 13%
19/12/13 02:28:40 INFO mapreduce.Job:  map 99% reduce 14%
19/12/13 02:28:46 INFO mapreduce.Job:  map 99% reduce 15%
19/12/13 02:29:01 INFO mapreduce.Job:  map 99% reduce 16%
19/12/13 02:29:04 INFO mapreduce.Job:  map 100% reduce 16%
19/12/13 02:29:15 INFO mapreduce.Job:  map 100% reduce 24%
19/12/13 02:29:16 INFO mapreduce.Job:  map 100% reduce 25%
19/12/13 02:29:17 INFO mapreduce.Job:  map 100% reduce 27%
19/12/13 02:29:18 INFO mapreduce.Job:  map 100% reduce 30%
19/12/13 02:29:19 INFO mapreduce.Job:  map 100% reduce 31%
19/12/13 02:29:22 INFO mapreduce.Job:  map 100% reduce 32%
19/12/13 02:29:28 INFO mapreduce.Job:  map 100% reduce 33%
19/12/13 02:29:32 INFO mapreduce.Job:  map 100% reduce 34%
19/12/13 02:29:36 INFO mapreduce.Job:  map 100% reduce 35%
19/12/13 02:29:41 INFO mapreduce.Job:  map 100% reduce 37%
19/12/13 02:29:43 INFO mapreduce.Job:  map 100% reduce 38%
19/12/13 02:29:51 INFO mapreduce.Job:  map 100% reduce 39%
19/12/13 02:29:57 INFO mapreduce.Job:  map 100% reduce 40%
19/12/13 02:30:04 INFO mapreduce.Job:  map 100% reduce 41%
19/12/13 02:30:10 INFO mapreduce.Job:  map 100% reduce 42%
19/12/13 02:30:15 INFO mapreduce.Job:  map 100% reduce 43%
19/12/13 02:30:21 INFO mapreduce.Job:  map 100% reduce 44%
19/12/13 02:30:26 INFO mapreduce.Job:  map 100% reduce 45%
19/12/13 02:30:27 INFO mapreduce.Job:  map 100% reduce 46%
19/12/13 02:30:29 INFO mapreduce.Job:  map 100% reduce 47%
19/12/13 02:30:30 INFO mapreduce.Job:  map 100% reduce 48%
19/12/13 02:30:31 INFO mapreduce.Job:  map 100% reduce 49%
19/12/13 02:30:34 INFO mapreduce.Job:  map 100% reduce 50%
19/12/13 02:30:36 INFO mapreduce.Job:  map 100% reduce 52%
19/12/13 02:30:42 INFO mapreduce.Job:  map 100% reduce 53%
19/12/13 02:30:50 INFO mapreduce.Job:  map 100% reduce 55%
19/12/13 02:30:54 INFO mapreduce.Job:  map 100% reduce 57%
19/12/13 02:30:58 INFO mapreduce.Job:  map 100% reduce 58%
19/12/13 02:31:02 INFO mapreduce.Job:  map 100% reduce 60%
19/12/13 02:31:12 INFO mapreduce.Job:  map 100% reduce 61%
19/12/13 02:31:20 INFO mapreduce.Job:  map 100% reduce 62%
19/12/13 02:31:23 INFO mapreduce.Job:  map 100% reduce 63%
19/12/13 02:31:25 INFO mapreduce.Job:  map 100% reduce 64%
19/12/13 02:31:32 INFO mapreduce.Job:  map 100% reduce 66%
19/12/13 02:31:37 INFO mapreduce.Job:  map 100% reduce 68%
19/12/13 02:31:43 INFO mapreduce.Job:  map 100% reduce 69%
19/12/13 02:31:46 INFO mapreduce.Job:  map 100% reduce 70%
19/12/13 02:31:52 INFO mapreduce.Job:  map 100% reduce 71%
19/12/13 02:32:19 INFO mapreduce.Job:  map 100% reduce 72%
19/12/13 02:32:44 INFO mapreduce.Job:  map 100% reduce 73%
19/12/13 02:33:10 INFO mapreduce.Job:  map 100% reduce 74%
19/12/13 02:33:38 INFO mapreduce.Job:  map 100% reduce 75%
19/12/13 02:34:04 INFO mapreduce.Job:  map 100% reduce 76%
19/12/13 02:34:14 INFO mapreduce.Job:  map 100% reduce 77%
19/12/13 02:34:21 INFO mapreduce.Job:  map 100% reduce 78%
19/12/13 02:34:27 INFO mapreduce.Job:  map 100% reduce 79%
19/12/13 02:34:31 INFO mapreduce.Job:  map 100% reduce 80%
19/12/13 02:34:36 INFO mapreduce.Job:  map 100% reduce 81%
19/12/13 02:34:40 INFO mapreduce.Job:  map 100% reduce 82%
19/12/13 02:34:46 INFO mapreduce.Job:  map 100% reduce 83%
19/12/13 02:34:52 INFO mapreduce.Job:  map 100% reduce 84%
19/12/13 02:34:57 INFO mapreduce.Job:  map 100% reduce 85%
19/12/13 02:35:03 INFO mapreduce.Job:  map 100% reduce 86%
19/12/13 02:35:15 INFO mapreduce.Job:  map 100% reduce 87%
19/12/13 02:35:29 INFO mapreduce.Job:  map 100% reduce 88%
19/12/13 02:35:36 INFO mapreduce.Job:  map 100% reduce 89%
19/12/13 02:35:45 INFO mapreduce.Job:  map 100% reduce 90%
19/12/13 02:35:54 INFO mapreduce.Job:  map 100% reduce 91%
19/12/13 02:36:02 INFO mapreduce.Job:  map 100% reduce 92%
19/12/13 02:36:09 INFO mapreduce.Job:  map 100% reduce 93%
19/12/13 02:36:14 INFO mapreduce.Job:  map 100% reduce 94%
19/12/13 02:36:22 INFO mapreduce.Job:  map 100% reduce 95%
19/12/13 02:36:30 INFO mapreduce.Job:  map 100% reduce 96%
19/12/13 02:36:40 INFO mapreduce.Job:  map 100% reduce 97%
19/12/13 02:36:49 INFO mapreduce.Job:  map 100% reduce 98%
19/12/13 02:36:57 INFO mapreduce.Job:  map 100% reduce 99%
19/12/13 02:37:12 INFO mapreduce.Job:  map 100% reduce 100%
19/12/13 02:38:39 INFO mapreduce.Job: Job job_1573729450353_315993 completed successfully
19/12/13 02:38:39 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=152338491490
		FILE: Number of bytes written=238383785419
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1949226008316
		HDFS: Number of bytes written=82098390557
		HDFS: Number of read operations=24039
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=80
	Job Counters 
		Killed reduce tasks=20
		Launched map tasks=7993
		Launched reduce tasks=40
		Data-local map tasks=7854
		Rack-local map tasks=139
		Total time spent by all maps in occupied slots (ms)=342215121
		Total time spent by all reduces in occupied slots (ms)=158678388
		Total time spent by all map tasks (ms)=114071707
		Total time spent by all reduce tasks (ms)=13223199
		Total vcore-milliseconds taken by all map tasks=114071707
		Total vcore-milliseconds taken by all reduce tasks=13223199
		Total megabyte-milliseconds taken by all map tasks=175214141952
		Total megabyte-milliseconds taken by all reduce tasks=81243334656
	Map-Reduce Framework
		Map input records=1640548940
		Map output records=139116681
		Map output bytes=285154281778
		Map output materialized bytes=85760169600
		Input split bytes=1151945
		Combine input records=0
		Combine output records=0
		Reduce input groups=108580294
		Reduce shuffle bytes=85760169600
		Reduce input records=139116681
		Reduce output records=30870612
		Spilled Records=362065820
		Shuffled Maps =159860
		Failed Shuffles=0
		Merged Map outputs=159860
		GC time elapsed (ms)=3400243
		CPU time spent (ms)=136911930
		Physical memory (bytes) snapshot=9294498828288
		Virtual memory (bytes) snapshot=25475013451776
		Total committed heap usage (bytes)=10177965195264
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1949224856371
	File Output Format Counters 
		Bytes Written=82098390557
19/12/13 02:38:39 INFO streaming.StreamJob: Output directory: /user/ad_user/ocpc/click_join_trans/20191212/
+ judge_result hadoop_run
+ [[ 0 -ne 0 ]]
+ echo 'hadoop_run success.'
hadoop_run success.
++ date -d '1 days ago 20191212' +%Y%m%d
+ one_day_ago=20191211
+ STABLEDIR_ONEDAY_AGO=/user/ad_user/ocpc/click_join_trans/stable/20191211/
+ STABLEDIR=/user/ad_user/ocpc/click_join_trans/stable/20191212/
+ TEMPDIR=/user/ad_user/ocpc/click_join_trans/temp/20191212/
+ UNJOINDIR=/user/ad_user/ocpc/click_join_trans/unjoin/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/stable/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/click_join_trans/stable/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/stable/20191211/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -rm -r /user/ad_user/ocpc/click_join_trans/stable/20191211/
19/12/13 02:38:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.
Moved: 'hdfs://cootek/user/ad_user/ocpc/click_join_trans/stable/20191211' to trash at: hdfs://cootek/user/ad_user/.Trash/Current
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/click_join_trans/stable/20191211/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/click_join_trans/temp/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/unjoin/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/click_join_trans/unjoin/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/click_join_trans/20191212//part-*-A' /user/ad_user/ocpc/click_join_trans/temp/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -touchz /user/ad_user/ocpc/click_join_trans/temp/20191212//_SUCCESS
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/click_join_trans/20191212//part-*-B' /user/ad_user/ocpc/click_join_trans/stable/20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -touchz /user/ad_user/ocpc/click_join_trans/stable/20191212//_SUCCESS
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/click_join_trans/20191212//part-*-D' /user/ad_user/ocpc/click_join_trans/unjoin/20191212/
mv: `/user/ad_user/ocpc/click_join_trans/20191212//part-*-D': No such file or directory
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/click_join_trans/20191212//part-*-C' /user/ad_user/ocpc/click_join_trans/stable/20191211/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -touchz /user/ad_user/ocpc/click_join_trans/stable/20191211//_SUCCESS
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -rm -r /user/ad_user/ocpc/click_join_trans/20191212/
19/12/13 02:39:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.
Moved: 'hdfs://cootek/user/ad_user/ocpc/click_join_trans/20191212' to trash at: hdfs://cootek/user/ad_user/.Trash/Current
+ clear_hadoop_data /user/ad_user/ocpc/click_join_trans/temp 7 20191212
+ '[' 3 -lt 2 ']'
+ path=/user/ad_user/ocpc/click_join_trans/temp
+ windows=7
++ date -d '1 days ago' +%Y%m%d
+ start_dt=20191212
+ '[' 3 -gt 2 ']'
+ start_dt=20191212
++ date -d '7 days ago 20191212' +%Y%m%d
+ start_dt=20191205
++ seq 0 7
+ for idx in '`seq 0 7`'
++ date -d '0 days ago 20191205' +%Y%m%d
+ date=20191205
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191205
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -rm -r /user/ad_user/ocpc/click_join_trans/temp/20191205
19/12/13 02:39:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.
Moved: 'hdfs://cootek/user/ad_user/ocpc/click_join_trans/temp/20191205' to trash at: hdfs://cootek/user/ad_user/.Trash/Current
+ for idx in '`seq 0 7`'
++ date -d '1 days ago 20191205' +%Y%m%d
+ date=20191204
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191204
+ for idx in '`seq 0 7`'
++ date -d '2 days ago 20191205' +%Y%m%d
+ date=20191203
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191203
+ for idx in '`seq 0 7`'
++ date -d '3 days ago 20191205' +%Y%m%d
+ date=20191202
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191202
+ for idx in '`seq 0 7`'
++ date -d '4 days ago 20191205' +%Y%m%d
+ date=20191201
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191201
+ for idx in '`seq 0 7`'
++ date -d '5 days ago 20191205' +%Y%m%d
+ date=20191130
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191130
+ for idx in '`seq 0 7`'
++ date -d '6 days ago 20191205' +%Y%m%d
+ date=20191129
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191129
+ for idx in '`seq 0 7`'
++ date -d '7 days ago 20191205' +%Y%m%d
+ date=20191128
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191128
+ return 0
+ clear_hadoop_data /user/ad_user/ocpc/click_join_trans/statinfo/temp 7 20191212
+ '[' 3 -lt 2 ']'
+ path=/user/ad_user/ocpc/click_join_trans/statinfo/temp
+ windows=7
++ date -d '1 days ago' +%Y%m%d
+ start_dt=20191212
+ '[' 3 -gt 2 ']'
+ start_dt=20191212
++ date -d '7 days ago 20191212' +%Y%m%d
+ start_dt=20191205
++ seq 0 7
+ for idx in '`seq 0 7`'
++ date -d '0 days ago 20191205' +%Y%m%d
+ date=20191205
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/statinfo/temp/20191205
+ for idx in '`seq 0 7`'
++ date -d '1 days ago 20191205' +%Y%m%d
+ date=20191204
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/statinfo/temp/20191204
+ for idx in '`seq 0 7`'
++ date -d '2 days ago 20191205' +%Y%m%d
+ date=20191203
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/statinfo/temp/20191203
+ for idx in '`seq 0 7`'
++ date -d '3 days ago 20191205' +%Y%m%d
+ date=20191202
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/statinfo/temp/20191202
+ for idx in '`seq 0 7`'
++ date -d '4 days ago 20191205' +%Y%m%d
+ date=20191201
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/statinfo/temp/20191201
+ for idx in '`seq 0 7`'
++ date -d '5 days ago 20191205' +%Y%m%d
+ date=20191130
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/statinfo/temp/20191130
+ for idx in '`seq 0 7`'
++ date -d '6 days ago 20191205' +%Y%m%d
+ date=20191129
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/statinfo/temp/20191129
+ for idx in '`seq 0 7`'
++ date -d '7 days ago 20191205' +%Y%m%d
+ date=20191128
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/statinfo/temp/20191128
+ return 0
+ echo 'task[20191212] success!'
task[20191212] success!
+ exit 0
+ [[ 0 -ne 0 ]]
+ cd /home/ad_user/personal/ling.fang//cvr_space/shitu_generate
+ bash -x shell/stat_app_clctrans.sh
++ pwd
+ ROOT_PATH=/home/ad_user/personal/ling.fang/cvr_space/shitu_generate
+ JOB_PATH=/home/ad_user/personal/ling.fang/cvr_space/shitu_generate/
+ HADOOP_HOME=/usr/local/hadoop-2.6.3
+ SPARK_HOME=/usr/local/spark-2.1.1-bin-hadoop2.6
+ LOG_PATH=hdfs:///user/ad_user/spark_eventlog
+ '[' 0 -ge 1 ']'
++ date -d ' 1 days ago ' +%Y%m%d
+ DATE=20191212
+ DATE_LST='{20191212'
++ seq 1 50
+ for idx in '`seq 1 50`'
++ date -d '1 days ago 20191212' +%Y%m%d
+ day=20191211
+ DATE_LST='{20191212,20191211'
+ for idx in '`seq 1 50`'
++ date -d '2 days ago 20191212' +%Y%m%d
+ day=20191210
+ DATE_LST='{20191212,20191211,20191210'
+ for idx in '`seq 1 50`'
++ date -d '3 days ago 20191212' +%Y%m%d
+ day=20191209
+ DATE_LST='{20191212,20191211,20191210,20191209'
+ for idx in '`seq 1 50`'
++ date -d '4 days ago 20191212' +%Y%m%d
+ day=20191208
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208'
+ for idx in '`seq 1 50`'
++ date -d '5 days ago 20191212' +%Y%m%d
+ day=20191207
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207'
+ for idx in '`seq 1 50`'
++ date -d '6 days ago 20191212' +%Y%m%d
+ day=20191206
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206'
+ for idx in '`seq 1 50`'
++ date -d '7 days ago 20191212' +%Y%m%d
+ day=20191205
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205'
+ for idx in '`seq 1 50`'
++ date -d '8 days ago 20191212' +%Y%m%d
+ day=20191204
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204'
+ for idx in '`seq 1 50`'
++ date -d '9 days ago 20191212' +%Y%m%d
+ day=20191203
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203'
+ for idx in '`seq 1 50`'
++ date -d '10 days ago 20191212' +%Y%m%d
+ day=20191202
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202'
+ for idx in '`seq 1 50`'
++ date -d '11 days ago 20191212' +%Y%m%d
+ day=20191201
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201'
+ for idx in '`seq 1 50`'
++ date -d '12 days ago 20191212' +%Y%m%d
+ day=20191130
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130'
+ for idx in '`seq 1 50`'
++ date -d '13 days ago 20191212' +%Y%m%d
+ day=20191129
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129'
+ for idx in '`seq 1 50`'
++ date -d '14 days ago 20191212' +%Y%m%d
+ day=20191128
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128'
+ for idx in '`seq 1 50`'
++ date -d '15 days ago 20191212' +%Y%m%d
+ day=20191127
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127'
+ for idx in '`seq 1 50`'
++ date -d '16 days ago 20191212' +%Y%m%d
+ day=20191126
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126'
+ for idx in '`seq 1 50`'
++ date -d '17 days ago 20191212' +%Y%m%d
+ day=20191125
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125'
+ for idx in '`seq 1 50`'
++ date -d '18 days ago 20191212' +%Y%m%d
+ day=20191124
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124'
+ for idx in '`seq 1 50`'
++ date -d '19 days ago 20191212' +%Y%m%d
+ day=20191123
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123'
+ for idx in '`seq 1 50`'
++ date -d '20 days ago 20191212' +%Y%m%d
+ day=20191122
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122'
+ for idx in '`seq 1 50`'
++ date -d '21 days ago 20191212' +%Y%m%d
+ day=20191121
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121'
+ for idx in '`seq 1 50`'
++ date -d '22 days ago 20191212' +%Y%m%d
+ day=20191120
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120'
+ for idx in '`seq 1 50`'
++ date -d '23 days ago 20191212' +%Y%m%d
+ day=20191119
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119'
+ for idx in '`seq 1 50`'
++ date -d '24 days ago 20191212' +%Y%m%d
+ day=20191118
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118'
+ for idx in '`seq 1 50`'
++ date -d '25 days ago 20191212' +%Y%m%d
+ day=20191117
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117'
+ for idx in '`seq 1 50`'
++ date -d '26 days ago 20191212' +%Y%m%d
+ day=20191116
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116'
+ for idx in '`seq 1 50`'
++ date -d '27 days ago 20191212' +%Y%m%d
+ day=20191115
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115'
+ for idx in '`seq 1 50`'
++ date -d '28 days ago 20191212' +%Y%m%d
+ day=20191114
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114'
+ for idx in '`seq 1 50`'
++ date -d '29 days ago 20191212' +%Y%m%d
+ day=20191113
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113'
+ for idx in '`seq 1 50`'
++ date -d '30 days ago 20191212' +%Y%m%d
+ day=20191112
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112'
+ for idx in '`seq 1 50`'
++ date -d '31 days ago 20191212' +%Y%m%d
+ day=20191111
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111'
+ for idx in '`seq 1 50`'
++ date -d '32 days ago 20191212' +%Y%m%d
+ day=20191110
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110'
+ for idx in '`seq 1 50`'
++ date -d '33 days ago 20191212' +%Y%m%d
+ day=20191109
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109'
+ for idx in '`seq 1 50`'
++ date -d '34 days ago 20191212' +%Y%m%d
+ day=20191108
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108'
+ for idx in '`seq 1 50`'
++ date -d '35 days ago 20191212' +%Y%m%d
+ day=20191107
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107'
+ for idx in '`seq 1 50`'
++ date -d '36 days ago 20191212' +%Y%m%d
+ day=20191106
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106'
+ for idx in '`seq 1 50`'
++ date -d '37 days ago 20191212' +%Y%m%d
+ day=20191105
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105'
+ for idx in '`seq 1 50`'
++ date -d '38 days ago 20191212' +%Y%m%d
+ day=20191104
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104'
+ for idx in '`seq 1 50`'
++ date -d '39 days ago 20191212' +%Y%m%d
+ day=20191103
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103'
+ for idx in '`seq 1 50`'
++ date -d '40 days ago 20191212' +%Y%m%d
+ day=20191102
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102'
+ for idx in '`seq 1 50`'
++ date -d '41 days ago 20191212' +%Y%m%d
+ day=20191101
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101'
+ for idx in '`seq 1 50`'
++ date -d '42 days ago 20191212' +%Y%m%d
+ day=20191031
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031'
+ for idx in '`seq 1 50`'
++ date -d '43 days ago 20191212' +%Y%m%d
+ day=20191030
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030'
+ for idx in '`seq 1 50`'
++ date -d '44 days ago 20191212' +%Y%m%d
+ day=20191029
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029'
+ for idx in '`seq 1 50`'
++ date -d '45 days ago 20191212' +%Y%m%d
+ day=20191028
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028'
+ for idx in '`seq 1 50`'
++ date -d '46 days ago 20191212' +%Y%m%d
+ day=20191027
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027'
+ for idx in '`seq 1 50`'
++ date -d '47 days ago 20191212' +%Y%m%d
+ day=20191026
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026'
+ for idx in '`seq 1 50`'
++ date -d '48 days ago 20191212' +%Y%m%d
+ day=20191025
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025'
+ for idx in '`seq 1 50`'
++ date -d '49 days ago 20191212' +%Y%m%d
+ day=20191024
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024'
+ for idx in '`seq 1 50`'
++ date -d '50 days ago 20191212' +%Y%m%d
+ day=20191023
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023'
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023}'
+ SRC_PATH='/user/ad_user/ocpc/click_join_trans/{stable/{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023},temp/20191212}/'
+ DES_PATH=/user/ad_user/ocpc/stat_app_cvr/json/
+ sparksubmit='/usr/local/spark-2.1.1-bin-hadoop2.6/bin/spark-submit                  --master yarn                  --queue root.ad-root.etl.dailyetl.high                  --num-executors 50                  --executor-memory 12g                  --driver-memory 12g                  --conf spark.dynamicAllocation.enabled=true                  --conf spark.dynamicAllocation.maxExecutors=120                  --conf spark.dynamicAllocation.minExecutors=60                  --conf spark.driver.maxResultSize=8G                  --conf spark.shuffle.service.enabled=true                  --conf spark.eventLog.enabled=true                  --conf spark.sql.broadcastTimeout=36000                  --conf spark.eventLog.dir=hdfs:///user/ad_user/spark_eventlog'
+ /usr/local/spark-2.1.1-bin-hadoop2.6/bin/spark-submit --master yarn --queue root.ad-root.etl.dailyetl.high --num-executors 50 --executor-memory 12g --driver-memory 12g --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.maxExecutors=120 --conf spark.dynamicAllocation.minExecutors=60 --conf spark.driver.maxResultSize=8G --conf spark.shuffle.service.enabled=true --conf spark.eventLog.enabled=true --conf spark.sql.broadcastTimeout=36000 --conf spark.eventLog.dir=hdfs:///user/ad_user/spark_eventlog /home/ad_user/personal/ling.fang/cvr_space/shitu_generate//script/stat_app_clctrans.py '/user/ad_user/ocpc/click_join_trans/{stable/{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023},temp/20191212}/' /user/ad_user/ocpc/stat_app_cvr/json/
JAVA_LIBRARY_PATH: :/usr/local/hadoop-ha/lib/native:/usr/lib
LD_LIBRARY_PATH :/usr/local/hadoop-ha/lib/native:/usr/lib
19/12/13 02:39:53 INFO spark.SparkContext: Running Spark version 2.4.3
19/12/13 02:39:53 INFO spark.SparkContext: Submitted application: naga dsp: stat user state
19/12/13 02:39:53 INFO spark.SecurityManager: Changing view acls to: ad_user
19/12/13 02:39:53 INFO spark.SecurityManager: Changing modify acls to: ad_user
19/12/13 02:39:53 INFO spark.SecurityManager: Changing view acls groups to: 
19/12/13 02:39:53 INFO spark.SecurityManager: Changing modify acls groups to: 
19/12/13 02:39:53 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ad_user); groups with view permissions: Set(); users  with modify permissions: Set(ad_user); groups with modify permissions: Set()
19/12/13 02:39:53 INFO util.Utils: Successfully started service 'sparkDriver' on port 28745.
19/12/13 02:39:53 INFO spark.SparkEnv: Registering MapOutputTracker
19/12/13 02:39:53 INFO spark.SparkEnv: Registering BlockManagerMaster
19/12/13 02:39:53 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/12/13 02:39:53 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/12/13 02:39:53 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-e55a99ec-46fb-4995-944c-8d683ebd215c
19/12/13 02:39:53 INFO memory.MemoryStore: MemoryStore started with capacity 6.9 GB
19/12/13 02:39:53 INFO spark.SparkEnv: Registering OutputCommitCoordinator
19/12/13 02:39:54 INFO util.log: Logging initialized @2887ms
19/12/13 02:39:54 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
19/12/13 02:39:54 INFO server.Server: Started @2968ms
19/12/13 02:39:54 INFO server.AbstractConnector: Started ServerConnector@198b1405{HTTP/1.1,[http/1.1]}{0.0.0.0:13997}
19/12/13 02:39:54 INFO util.Utils: Successfully started service 'SparkUI' on port 13997.
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2475874{/jobs,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ca3472a{/jobs/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@353b7e9d{/jobs/job,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@766352d{/jobs/job/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@710ab8fc{/stages,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fabffd6{/stages/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17837fff{/stages/stage,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@458674cf{/stages/stage/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49f68707{/stages/pool,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f019851{/stages/pool/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21fbb59b{/storage,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57ad8d67{/storage/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a9ee62a{/storage/rdd,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19ee3038{/storage/rdd/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65c519f3{/environment,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5eb87699{/environment/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ca64d2{/executors,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b4deb63{/executors/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@268a6d78{/executors/threadDump,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b1bcb8e{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b27d4cb{/static,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b785a12{/,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6deec869{/api,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@112ce3a1{/jobs/job/kill,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7787e779{/stages/stage/kill,null,AVAILABLE,@Spark}
19/12/13 02:39:54 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.30:13997
19/12/13 02:39:54 WARN util.Utils: spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
19/12/13 02:39:54 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
19/12/13 02:39:54 INFO util.Utils: Using initial executors = 60, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
19/12/13 02:39:55 INFO client.RMProxy: Connecting to ResourceManager at hadoop2-namenode.corp.cootek.com/192.168.30.10:8032
19/12/13 02:39:55 INFO yarn.Client: Requesting a new application from cluster with 126 NodeManagers
19/12/13 02:39:55 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (32768 MB per container)
19/12/13 02:39:55 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
19/12/13 02:39:55 INFO yarn.Client: Setting up container launch context for our AM
19/12/13 02:39:55 INFO yarn.Client: Setting up the launch environment for our AM container
19/12/13 02:39:55 INFO yarn.Client: Preparing resources for our AM container
19/12/13 02:39:55 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
19/12/13 02:39:58 INFO yarn.Client: Uploading resource file:/tmp/spark-f43ab815-0665-4969-81f7-18a06df50a89/__spark_libs__5799164983631869600.zip -> hdfs://cootek/user/ad_user/.sparkStaging/application_1573729450353_316179/__spark_libs__5799164983631869600.zip
19/12/13 02:40:04 INFO yarn.Client: Uploading resource file:/usr/local/spark-2.1.1-bin-hadoop2.6/python/lib/pyspark.zip -> hdfs://cootek/user/ad_user/.sparkStaging/application_1573729450353_316179/pyspark.zip
19/12/13 02:40:04 INFO yarn.Client: Uploading resource file:/usr/local/spark-2.1.1-bin-hadoop2.6/python/lib/py4j-0.10.7-src.zip -> hdfs://cootek/user/ad_user/.sparkStaging/application_1573729450353_316179/py4j-0.10.7-src.zip
19/12/13 02:40:04 INFO yarn.Client: Uploading resource file:/tmp/spark-f43ab815-0665-4969-81f7-18a06df50a89/__spark_conf__2808151516332608419.zip -> hdfs://cootek/user/ad_user/.sparkStaging/application_1573729450353_316179/__spark_conf__.zip
19/12/13 02:40:04 INFO spark.SecurityManager: Changing view acls to: ad_user
19/12/13 02:40:04 INFO spark.SecurityManager: Changing modify acls to: ad_user
19/12/13 02:40:04 INFO spark.SecurityManager: Changing view acls groups to: 
19/12/13 02:40:04 INFO spark.SecurityManager: Changing modify acls groups to: 
19/12/13 02:40:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ad_user); groups with view permissions: Set(); users  with modify permissions: Set(ad_user); groups with modify permissions: Set()
19/12/13 02:40:04 INFO security.HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_736693376_18, ugi=ad_user@CORP.COOTEK.COM (auth:KERBEROS)]]
19/12/13 02:40:04 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5998836 for ad_user on ha-hdfs:cootek
19/12/13 02:40:05 WARN conf.HiveConf: HiveConf of name hive.mapred.num.threshold does not exist
19/12/13 02:40:05 WARN conf.HiveConf: HiveConf of name hive.query.history.Password does not exist
19/12/13 02:40:05 WARN conf.HiveConf: HiveConf of name hive.query.history.userName does not exist
19/12/13 02:40:05 WARN conf.HiveConf: HiveConf of name hive.query.history.connectionstring does not exist
19/12/13 02:40:06 INFO yarn.Client: Submitting application application_1573729450353_316179 to ResourceManager
19/12/13 02:40:06 INFO impl.YarnClientImpl: Submitted application application_1573729450353_316179
19/12/13 02:40:06 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1573729450353_316179 and attemptId None
19/12/13 02:40:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:07 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.ad-root.etl.dailyetl.high
	 start time: 1576176006613
	 final status: UNDEFINED
	 tracking URL: http://hadoop2-namenode.corp.cootek.com:8088/proxy/application_1573729450353_316179/
	 user: ad_user
19/12/13 02:40:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:34 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:35 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:36 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:37 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:38 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:39 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:40 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:41 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:42 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:43 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:44 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:45 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:46 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:47 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:48 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:49 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:50 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:51 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:52 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:53 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:54 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:55 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:56 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:57 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:58 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:40:59 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:00 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:01 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:02 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:04 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:05 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:06 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:34 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:35 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:36 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:37 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:38 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:39 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:40 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:41 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:42 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:43 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:44 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:45 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:46 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:47 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:48 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:49 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:50 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:51 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:52 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:53 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:54 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:55 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:56 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:57 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:58 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:41:59 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:00 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:01 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:02 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:03 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:04 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:05 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:06 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:34 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:35 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:36 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:37 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:38 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:39 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:40 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:41 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:42 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:43 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:44 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:45 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:46 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:47 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:48 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:49 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:50 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:51 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:52 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:53 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:54 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:55 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:56 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:57 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:58 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:42:59 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:00 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:01 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:02 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:03 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:04 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:05 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:06 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:34 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:35 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:36 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:37 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:38 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:39 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:40 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:41 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:42 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:43 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:44 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:45 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:46 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:47 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:48 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:49 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:50 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:51 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:52 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:53 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:54 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:55 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:56 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:57 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:58 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:43:59 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:00 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:01 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:02 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:03 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:04 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:05 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:06 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:34 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:35 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:36 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:37 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:38 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:39 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:40 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:41 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:42 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:43 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:44 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:45 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:46 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:47 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:48 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:49 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:50 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:51 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:52 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:53 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:54 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:55 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:56 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:57 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:58 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:44:59 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:00 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:01 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:02 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:03 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:04 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:05 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:06 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:34 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:35 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:36 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:37 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:38 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:39 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:40 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:41 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:42 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:43 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:44 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:45 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:46 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:47 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:48 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:49 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:50 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:51 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:52 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:53 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:54 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:55 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:56 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:57 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:58 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:45:59 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:00 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:01 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:02 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:03 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:04 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:05 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:06 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:35 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:36 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:37 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:38 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:39 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:40 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:41 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:42 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:43 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:44 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:45 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:46 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:47 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:48 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:49 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:50 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:51 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:52 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:53 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:54 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:55 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:56 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:57 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:58 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:46:59 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:00 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:01 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:02 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:03 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:04 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:05 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:06 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:34 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:35 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:36 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:37 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:38 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:39 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:40 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:41 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:42 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:43 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:44 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:45 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:46 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:47 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:48 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:49 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:50 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:51 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:52 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:53 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:54 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:55 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:56 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:57 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:58 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:47:59 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:00 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:01 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:02 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:03 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:04 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:05 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:06 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:07 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:08 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:09 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:10 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:11 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:12 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:13 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:14 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:15 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:16 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:17 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:18 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:19 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:20 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:21 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:22 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:23 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:24 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:25 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:26 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:27 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:28 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:29 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:30 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:31 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:32 INFO yarn.Client: Application report for application_1573729450353_316179 (state: ACCEPTED)
19/12/13 02:48:33 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hadoop2-namenode.corp.cootek.com, PROXY_URI_BASES -> http://hadoop2-namenode.corp.cootek.com:8088/proxy/application_1573729450353_316179), /proxy/application_1573729450353_316179
19/12/13 02:48:33 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
19/12/13 02:48:33 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
19/12/13 02:48:33 INFO yarn.Client: Application report for application_1573729450353_316179 (state: RUNNING)
19/12/13 02:48:33 INFO yarn.Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 192.168.30.37
	 ApplicationMaster RPC port: -1
	 queue: root.ad-root.etl.dailyetl.high
	 start time: 1576176006613
	 final status: UNDEFINED
	 tracking URL: http://hadoop2-namenode.corp.cootek.com:8088/proxy/application_1573729450353_316179/
	 user: ad_user
19/12/13 02:48:33 INFO cluster.YarnClientSchedulerBackend: Application application_1573729450353_316179 has started running.
19/12/13 02:48:33 INFO cluster.YarnScheduler: Starting speculative execution thread
19/12/13 02:48:33 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 22179.
19/12/13 02:48:33 INFO netty.NettyBlockTransferService: Server created on 192.168.0.30:22179
19/12/13 02:48:33 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/12/13 02:48:33 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.30, 22179, None)
19/12/13 02:48:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.30:22179 with 6.9 GB RAM, BlockManagerId(driver, 192.168.0.30, 22179, None)
19/12/13 02:48:33 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.30, 22179, None)
19/12/13 02:48:33 INFO storage.BlockManager: external shuffle service port = 7337
19/12/13 02:48:33 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.30, 22179, None)
19/12/13 02:48:33 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
19/12/13 02:48:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51422a30{/metrics/json,null,AVAILABLE,@Spark}
19/12/13 02:48:33 INFO scheduler.EventLoggingListener: Logging events to hdfs:/user/ad_user/spark_eventlog/application_1573729450353_316179
19/12/13 02:48:33 WARN util.Utils: spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
19/12/13 02:48:33 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
19/12/13 02:48:33 INFO util.Utils: Using initial executors = 60, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
19/12/13 02:48:33 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
+------------+------+--------------------------------+--------+
|promoted_app|events|plid                            |time    |
+------------+------+--------------------------------+--------+
|            |null  |2e6500b1b19bf49badbe867d7d1550c3|20191017|
|            |null  |dc80b52962995a293b85623eebacc114|20191017|
|            |null  |9dd7c5734a15424db26d2d1872507575|20191017|
|            |null  |dc80b52962995a293b85623eebacc114|20191017|
|            |null  |dc80b52962995a293b85623eebacc114|20191017|
|            |null  |a600fdd1c22d4ad8aed3f23ba48272cf|20191017|
|            |null  |dc80b52962995a293b85623eebacc114|20191017|
|            |null  |fa8ab8bd29bd2fcf6272d8d3e0167c0f|20191017|
|            |null  |1a5becaacce20ccfcbdb1c4344c16d63|20191017|
|            |null  |fa8ab8bd29bd2fcf6272d8d3e0167c0f|20191017|
+------------+------+--------------------------------+--------+
only showing top 10 rows

+-----------------------------+------------+--------------------------------+--------+-----+
|promoted_app                 |events      |plid                            |time    |label|
+-----------------------------+------------+--------------------------------+--------+-----+
|com.cootek.literature        |[11, 13, 12]|0e31a96f198ac3a652760e7919e1b2ef|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|8337cb08edac2d9f3259b7ef52302fe8|20191017|0    |
|com.cootek.literature        |[11, 13, 12]|32f3f1ae27de34d03ac3b21f6904fa8a|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|8337cb08edac2d9f3259b7ef52302fe8|20191017|0    |
|com.cootek.literature        |[11, 13, 12]|2e6500b1b19bf49badbe867d7d1550c3|20191017|0    |
|com.cootek.literature        |[11, 13, 12]|9a1d45660d207a4fb5e72a44c3e53919|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|8337cb08edac2d9f3259b7ef52302fe8|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|0e31a96f198ac3a652760e7919e1b2ef|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|fe8d1bfbf4a33d6f261c74a92e513adb|20191017|0    |
|com.hunting.matrix_callershow|[13, 12]    |2e6500b1b19bf49badbe867d7d1550c3|20191017|0    |
+-----------------------------+------------+--------------------------------+--------+-----+
only showing top 10 rows

+-----------------------------+------------+--------------------------------+--------+-----+
|promoted_app                 |events      |plid                            |time    |label|
+-----------------------------+------------+--------------------------------+--------+-----+
|com.cootek.literature        |[11, 13, 12]|0e31a96f198ac3a652760e7919e1b2ef|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|8337cb08edac2d9f3259b7ef52302fe8|20191017|0    |
|com.cootek.literature        |[11, 13, 12]|32f3f1ae27de34d03ac3b21f6904fa8a|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|8337cb08edac2d9f3259b7ef52302fe8|20191017|0    |
|com.cootek.literature        |[11, 13, 12]|2e6500b1b19bf49badbe867d7d1550c3|20191017|0    |
|com.cootek.literature        |[11, 13, 12]|9a1d45660d207a4fb5e72a44c3e53919|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|8337cb08edac2d9f3259b7ef52302fe8|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|0e31a96f198ac3a652760e7919e1b2ef|20191017|0    |
|com.hunting.matrix_callershow|[11, 13, 12]|fe8d1bfbf4a33d6f261c74a92e513adb|20191017|0    |
|com.hunting.matrix_callershow|[13, 12]    |2e6500b1b19bf49badbe867d7d1550c3|20191017|0    |
+-----------------------------+------------+--------------------------------+--------+-----+
only showing top 10 rows

+ /usr/local/hadoop-2.6.3/bin/hadoop fs -cat '/user/ad_user/ocpc/stat_app_cvr/json/*'
+ [[ 0 -ne 0 ]]
+ cd /home/ad_user/personal/ling.fang//cvr_space/shitu_generate
+ bash -x run_shitu.sh
+ bash /home/ad_user/kinit_ad_user.sh
+ USER_NAME=ad_user
+ HADOOP_HOME=/usr/local/hadoop-2.6.3
+ common_file=/home/ling.fang/script/tools/common.sh
+ source /home/ling.fang/script/tools/common.sh
++ bash /home/ling.fang/kinit_ad_user.sh
kinit: Key table file '/home/ling.fang/ad_user.keytab' not found while getting initial credentials
++ HADOOP_BIN=/usr/local/hadoop-2.6.3/bin/hadoop
+++ pwd
++ ROOT_DIR=/home/ad_user/personal/ling.fang/cvr_space/shitu_generate
+ bash /home/ad_user/kinit_ad_user.sh
+ '[' 0 -ge 1 ']'
++ date -d ' 1 days ago ' +%Y%m%d
+ DATE=20191212
++ date -d '1 days ago 20191212' +%Y%m%d
+ DATE_YESTDAY=20191211
+ DATE_LST='{20191212'
++ seq 1 60
+ for idx in '`seq 1 60`'
++ date -d '1 days ago 20191212' +%Y%m%d
+ day=20191211
+ DATE_LST='{20191212,20191211'
+ for idx in '`seq 1 60`'
++ date -d '2 days ago 20191212' +%Y%m%d
+ day=20191210
+ DATE_LST='{20191212,20191211,20191210'
+ for idx in '`seq 1 60`'
++ date -d '3 days ago 20191212' +%Y%m%d
+ day=20191209
+ DATE_LST='{20191212,20191211,20191210,20191209'
+ for idx in '`seq 1 60`'
++ date -d '4 days ago 20191212' +%Y%m%d
+ day=20191208
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208'
+ for idx in '`seq 1 60`'
++ date -d '5 days ago 20191212' +%Y%m%d
+ day=20191207
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207'
+ for idx in '`seq 1 60`'
++ date -d '6 days ago 20191212' +%Y%m%d
+ day=20191206
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206'
+ for idx in '`seq 1 60`'
++ date -d '7 days ago 20191212' +%Y%m%d
+ day=20191205
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205'
+ for idx in '`seq 1 60`'
++ date -d '8 days ago 20191212' +%Y%m%d
+ day=20191204
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204'
+ for idx in '`seq 1 60`'
++ date -d '9 days ago 20191212' +%Y%m%d
+ day=20191203
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203'
+ for idx in '`seq 1 60`'
++ date -d '10 days ago 20191212' +%Y%m%d
+ day=20191202
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202'
+ for idx in '`seq 1 60`'
++ date -d '11 days ago 20191212' +%Y%m%d
+ day=20191201
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201'
+ for idx in '`seq 1 60`'
++ date -d '12 days ago 20191212' +%Y%m%d
+ day=20191130
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130'
+ for idx in '`seq 1 60`'
++ date -d '13 days ago 20191212' +%Y%m%d
+ day=20191129
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129'
+ for idx in '`seq 1 60`'
++ date -d '14 days ago 20191212' +%Y%m%d
+ day=20191128
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128'
+ for idx in '`seq 1 60`'
++ date -d '15 days ago 20191212' +%Y%m%d
+ day=20191127
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127'
+ for idx in '`seq 1 60`'
++ date -d '16 days ago 20191212' +%Y%m%d
+ day=20191126
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126'
+ for idx in '`seq 1 60`'
++ date -d '17 days ago 20191212' +%Y%m%d
+ day=20191125
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125'
+ for idx in '`seq 1 60`'
++ date -d '18 days ago 20191212' +%Y%m%d
+ day=20191124
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124'
+ for idx in '`seq 1 60`'
++ date -d '19 days ago 20191212' +%Y%m%d
+ day=20191123
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123'
+ for idx in '`seq 1 60`'
++ date -d '20 days ago 20191212' +%Y%m%d
+ day=20191122
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122'
+ for idx in '`seq 1 60`'
++ date -d '21 days ago 20191212' +%Y%m%d
+ day=20191121
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121'
+ for idx in '`seq 1 60`'
++ date -d '22 days ago 20191212' +%Y%m%d
+ day=20191120
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120'
+ for idx in '`seq 1 60`'
++ date -d '23 days ago 20191212' +%Y%m%d
+ day=20191119
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119'
+ for idx in '`seq 1 60`'
++ date -d '24 days ago 20191212' +%Y%m%d
+ day=20191118
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118'
+ for idx in '`seq 1 60`'
++ date -d '25 days ago 20191212' +%Y%m%d
+ day=20191117
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117'
+ for idx in '`seq 1 60`'
++ date -d '26 days ago 20191212' +%Y%m%d
+ day=20191116
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116'
+ for idx in '`seq 1 60`'
++ date -d '27 days ago 20191212' +%Y%m%d
+ day=20191115
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115'
+ for idx in '`seq 1 60`'
++ date -d '28 days ago 20191212' +%Y%m%d
+ day=20191114
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114'
+ for idx in '`seq 1 60`'
++ date -d '29 days ago 20191212' +%Y%m%d
+ day=20191113
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113'
+ for idx in '`seq 1 60`'
++ date -d '30 days ago 20191212' +%Y%m%d
+ day=20191112
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112'
+ for idx in '`seq 1 60`'
++ date -d '31 days ago 20191212' +%Y%m%d
+ day=20191111
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111'
+ for idx in '`seq 1 60`'
++ date -d '32 days ago 20191212' +%Y%m%d
+ day=20191110
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110'
+ for idx in '`seq 1 60`'
++ date -d '33 days ago 20191212' +%Y%m%d
+ day=20191109
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109'
+ for idx in '`seq 1 60`'
++ date -d '34 days ago 20191212' +%Y%m%d
+ day=20191108
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108'
+ for idx in '`seq 1 60`'
++ date -d '35 days ago 20191212' +%Y%m%d
+ day=20191107
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107'
+ for idx in '`seq 1 60`'
++ date -d '36 days ago 20191212' +%Y%m%d
+ day=20191106
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106'
+ for idx in '`seq 1 60`'
++ date -d '37 days ago 20191212' +%Y%m%d
+ day=20191105
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105'
+ for idx in '`seq 1 60`'
++ date -d '38 days ago 20191212' +%Y%m%d
+ day=20191104
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104'
+ for idx in '`seq 1 60`'
++ date -d '39 days ago 20191212' +%Y%m%d
+ day=20191103
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103'
+ for idx in '`seq 1 60`'
++ date -d '40 days ago 20191212' +%Y%m%d
+ day=20191102
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102'
+ for idx in '`seq 1 60`'
++ date -d '41 days ago 20191212' +%Y%m%d
+ day=20191101
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101'
+ for idx in '`seq 1 60`'
++ date -d '42 days ago 20191212' +%Y%m%d
+ day=20191031
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031'
+ for idx in '`seq 1 60`'
++ date -d '43 days ago 20191212' +%Y%m%d
+ day=20191030
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030'
+ for idx in '`seq 1 60`'
++ date -d '44 days ago 20191212' +%Y%m%d
+ day=20191029
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029'
+ for idx in '`seq 1 60`'
++ date -d '45 days ago 20191212' +%Y%m%d
+ day=20191028
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028'
+ for idx in '`seq 1 60`'
++ date -d '46 days ago 20191212' +%Y%m%d
+ day=20191027
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027'
+ for idx in '`seq 1 60`'
++ date -d '47 days ago 20191212' +%Y%m%d
+ day=20191026
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026'
+ for idx in '`seq 1 60`'
++ date -d '48 days ago 20191212' +%Y%m%d
+ day=20191025
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025'
+ for idx in '`seq 1 60`'
++ date -d '49 days ago 20191212' +%Y%m%d
+ day=20191024
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024'
+ for idx in '`seq 1 60`'
++ date -d '50 days ago 20191212' +%Y%m%d
+ day=20191023
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023'
+ for idx in '`seq 1 60`'
++ date -d '51 days ago 20191212' +%Y%m%d
+ day=20191022
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022'
+ for idx in '`seq 1 60`'
++ date -d '52 days ago 20191212' +%Y%m%d
+ day=20191021
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021'
+ for idx in '`seq 1 60`'
++ date -d '53 days ago 20191212' +%Y%m%d
+ day=20191020
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020'
+ for idx in '`seq 1 60`'
++ date -d '54 days ago 20191212' +%Y%m%d
+ day=20191019
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019'
+ for idx in '`seq 1 60`'
++ date -d '55 days ago 20191212' +%Y%m%d
+ day=20191018
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018'
+ for idx in '`seq 1 60`'
++ date -d '56 days ago 20191212' +%Y%m%d
+ day=20191017
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017'
+ for idx in '`seq 1 60`'
++ date -d '57 days ago 20191212' +%Y%m%d
+ day=20191016
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016'
+ for idx in '`seq 1 60`'
++ date -d '58 days ago 20191212' +%Y%m%d
+ day=20191015
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016,20191015'
+ for idx in '`seq 1 60`'
++ date -d '59 days ago 20191212' +%Y%m%d
+ day=20191014
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016,20191015,20191014'
+ for idx in '`seq 1 60`'
++ date -d '60 days ago 20191212' +%Y%m%d
+ day=20191013
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016,20191015,20191014,20191013'
+ DATE_LST='{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016,20191015,20191014,20191013}'
+ '[' 0 -eq 2 ']'
+ JOB_TAG=
++ pwd
+ ROOT_DIR=/home/ad_user/personal/ling.fang/cvr_space/shitu_generate
+ INPUTDIR='-input "/user/ad_user/ocpc/click_join_trans/temp/20191212/" '
+ INPUTDIR='-input "/user/ad_user/ocpc/click_join_trans/stable/{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016,20191015,20191014,20191013}/" -input "/user/ad_user/ocpc/click_join_trans/temp/20191212/" '
+ echo -input '"/user/ad_user/ocpc/click_join_trans/stable/{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016,20191015,20191014,20191013}/"' -input '"/user/ad_user/ocpc/click_join_trans/temp/20191212/"'
-input "/user/ad_user/ocpc/click_join_trans/stable/{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016,20191015,20191014,20191013}/" -input "/user/ad_user/ocpc/click_join_trans/temp/20191212/"
+ OUTPUTDIR=/user/ad_user/ocpc/model_train/shitu/20191212/
+ done_file=/user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS
+ loop_check /user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS 140 1
+ file=/user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS
+ ready=0
+ check_time=140
+ mode=1
+ echo 'input params:/user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS 140 1'
input params:/user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS 140 1
+ '[' 3 -ne 3 ']'
+ '[' 140 -gt 0 ']'
+ '[' 1 -eq 1 ']'
+ hadoop_check_ready
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS
+ ready=1
+ '[' 1 -eq 1 ']'
+ mylog '/user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS is ready!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-02:54:46
+ echo '[20191213-02:54:46] /user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS' is 'ready!'
[20191213-02:54:46] /user/ad_user/ocpc/click_join_trans/temp/20191212/_SUCCESS is ready!
+ return 0
+ python script/get_creative_info.py
+ OWNER_INFO=ad_user@cootek.cn
+ TASK_NAME=naga_cvr_generate_shitu_log
+ hadoop_run
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -rm -r /user/ad_user/ocpc/model_train/shitu/20191212/
rm: `/user/ad_user/ocpc/model_train/shitu/20191212/': No such file or directory
+ /usr/local/hadoop-2.6.3/bin/hadoop jar /usr/local/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar -libjars /home/ling.fang/script/tools/mr_plugins-1.0.jar -files script -D map.output.key.field.separator=# -D num.key.fields.for.partition=1 -D mapreduce.map.memory.mb=4096 -D mapreduce.map.java.opts=-Xmx3276m -Dmapred.map.tasks.speculative.execution=false -Dmapred.reduce.tasks.speculative.execution=false -Dstream.non.zero.exit.is.failure=false -Dmapred.job.priority=HIGH -Dmapreduce.job.queuename=root.ad-root.etl.dailyetl.high -Dmapred.map.tasks=1000 -Dmapred.reduce.tasks=200 -Dmapred.job.map.capacity=1000 -Dmapred.job.reduce.capacity=200 -Dmapred.job.name=naga_cvr_generate_shitu_log_20191212 -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner -outputformat org.apache.hadoop.mapred.lib.SuffixMultipleTextOutputFormat -mapper 'python script/mapper.py' -reducer 'python script/reducer.py' -input '"/user/ad_user/ocpc/click_join_trans/stable/{20191212,20191211,20191210,20191209,20191208,20191207,20191206,20191205,20191204,20191203,20191202,20191201,20191130,20191129,20191128,20191127,20191126,20191125,20191124,20191123,20191122,20191121,20191120,20191119,20191118,20191117,20191116,20191115,20191114,20191113,20191112,20191111,20191110,20191109,20191108,20191107,20191106,20191105,20191104,20191103,20191102,20191101,20191031,20191030,20191029,20191028,20191027,20191026,20191025,20191024,20191023,20191022,20191021,20191020,20191019,20191018,20191017,20191016,20191015,20191014,20191013}/"' -input '"/user/ad_user/ocpc/click_join_trans/temp/20191212/"' -output /user/ad_user/ocpc/model_train/shitu/20191212/
19/12/13 02:54:49 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator
19/12/13 02:54:49 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
19/12/13 02:54:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
19/12/13 02:54:49 INFO Configuration.deprecation: mapred.job.priority is deprecated. Instead, use mapreduce.job.priority
19/12/13 02:54:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
19/12/13 02:54:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
19/12/13 02:54:49 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name
packageJobJar: [/tmp/hadoop-unjar4478621237542761694/] [] /tmp/streamjob6664357050739390151.jar tmpDir=null
19/12/13 02:54:49 INFO client.RMProxy: Connecting to ResourceManager at hadoop2-namenode.corp.cootek.com/192.168.30.10:8032
19/12/13 02:54:49 INFO client.RMProxy: Connecting to ResourceManager at hadoop2-namenode.corp.cootek.com/192.168.30.10:8032
19/12/13 02:54:50 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 5998933 for ad_user on ha-hdfs:cootek
19/12/13 02:54:50 INFO security.TokenCache: Got dt for hdfs://cootek; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:cootek, Ident: (HDFS_DELEGATION_TOKEN token 5998933 for ad_user)
19/12/13 02:54:50 WARN token.Token: Cannot find class for token kind kms-dt
19/12/13 02:54:50 INFO security.TokenCache: Got dt for hdfs://cootek; Kind: kms-dt, Service: 192.168.30.152:16000, Ident: 00 07 61 64 5f 75 73 65 72 06 68 61 64 6f 6f 70 00 8a 01 6e fb 76 b1 66 8a 01 6f 1f 83 35 66 8d 41 f2 8a 8e 02 a4
19/12/13 02:54:56 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries
19/12/13 02:54:56 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 52decc77982b58949890770d22720a91adce0c3f]
19/12/13 02:54:57 INFO mapred.FileInputFormat: Total input paths to process : 1240
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.220:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.61:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.216:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.124:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.107:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.123:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.224:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.70:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.125:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.41:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.204:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.217:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.74:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.109:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.127:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.60:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.232:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.24:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.114:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.56:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.128:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.226:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.228:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.75:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.222:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.156:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.58:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.63:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.207:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.62:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.68:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.122:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.209:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.221:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.37:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.42:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.21:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.66:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.126:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.219:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.44:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.50:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.129:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.76:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.132:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.218:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.133:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.47:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.212:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.213:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.211:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.111:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.73:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.59:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.118:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.117:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.77:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.23:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.64:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.67:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.231:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.53:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.105:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.119:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.154:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.225:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.52:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.40:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.208:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.29:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.71:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.104:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.205:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.227:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.214:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.51:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.230:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.229:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.89:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.30:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.108:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.223:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.43:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.113:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.31:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.210:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.155:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.131:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.55:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.215:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.206:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.72:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.203:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.34:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.79:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.115:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.19:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.36:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.65:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.48:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.57:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.121:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.116:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.120:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.49:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.27:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.130:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.20:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.32:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.25:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.18:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.69:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.106:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.112:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.45:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.103:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.22:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.33:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.78:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.90:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.35:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.38:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.28:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.54:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.39:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.88:61004
19/12/13 02:54:57 INFO net.NetworkTopology: Adding a new node: /default/rack/192.168.30.86:61004
19/12/13 02:54:58 INFO mapreduce.JobSubmitter: number of splits:1840
19/12/13 02:54:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1573729450353_316266
19/12/13 02:54:59 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:cootek, Ident: (HDFS_DELEGATION_TOKEN token 5998933 for ad_user)
19/12/13 02:54:59 WARN token.Token: Cannot find class for token kind kms-dt
19/12/13 02:54:59 WARN token.Token: Cannot find class for token kind kms-dt
Kind: kms-dt, Service: 192.168.30.152:16000, Ident: 00 07 61 64 5f 75 73 65 72 06 68 61 64 6f 6f 70 00 8a 01 6e fb 76 b1 66 8a 01 6f 1f 83 35 66 8d 41 f2 8a 8e 02 a4
19/12/13 02:54:59 INFO impl.YarnClientImpl: Submitted application application_1573729450353_316266
19/12/13 02:54:59 INFO mapreduce.Job: The url to track the job: http://hadoop2-namenode.corp.cootek.com:8088/proxy/application_1573729450353_316266/
19/12/13 02:54:59 INFO mapreduce.Job: Running job: job_1573729450353_316266
19/12/13 02:57:41 INFO mapreduce.Job: Job job_1573729450353_316266 running in uber mode : false
19/12/13 02:57:41 INFO mapreduce.Job:  map 0% reduce 0%
19/12/13 02:59:19 INFO mapreduce.Job:  map 1% reduce 0%
19/12/13 02:59:37 INFO mapreduce.Job:  map 2% reduce 0%
19/12/13 03:00:02 INFO mapreduce.Job:  map 3% reduce 0%
19/12/13 03:01:11 INFO mapreduce.Job:  map 4% reduce 0%
19/12/13 03:01:31 INFO mapreduce.Job:  map 5% reduce 0%
19/12/13 03:01:47 INFO mapreduce.Job:  map 6% reduce 0%
19/12/13 03:02:00 INFO mapreduce.Job:  map 7% reduce 0%
19/12/13 03:02:03 INFO mapreduce.Job:  map 8% reduce 0%
19/12/13 03:02:07 INFO mapreduce.Job:  map 9% reduce 0%
19/12/13 03:02:12 INFO mapreduce.Job:  map 10% reduce 0%
19/12/13 03:02:18 INFO mapreduce.Job:  map 11% reduce 0%
19/12/13 03:02:23 INFO mapreduce.Job:  map 12% reduce 0%
19/12/13 03:02:28 INFO mapreduce.Job:  map 13% reduce 0%
19/12/13 03:02:33 INFO mapreduce.Job:  map 14% reduce 0%
19/12/13 03:02:38 INFO mapreduce.Job:  map 15% reduce 0%
19/12/13 03:02:43 INFO mapreduce.Job:  map 16% reduce 0%
19/12/13 03:02:48 INFO mapreduce.Job:  map 17% reduce 0%
19/12/13 03:02:52 INFO mapreduce.Job:  map 18% reduce 0%
19/12/13 03:02:56 INFO mapreduce.Job:  map 19% reduce 0%
19/12/13 03:03:01 INFO mapreduce.Job:  map 20% reduce 0%
19/12/13 03:03:06 INFO mapreduce.Job:  map 21% reduce 0%
19/12/13 03:03:10 INFO mapreduce.Job:  map 22% reduce 0%
19/12/13 03:03:13 INFO mapreduce.Job:  map 23% reduce 0%
19/12/13 03:03:17 INFO mapreduce.Job:  map 24% reduce 0%
19/12/13 03:03:22 INFO mapreduce.Job:  map 25% reduce 0%
19/12/13 03:03:26 INFO mapreduce.Job:  map 26% reduce 0%
19/12/13 03:03:29 INFO mapreduce.Job:  map 27% reduce 0%
19/12/13 03:03:32 INFO mapreduce.Job:  map 28% reduce 0%
19/12/13 03:03:34 INFO mapreduce.Job:  map 29% reduce 0%
19/12/13 03:03:36 INFO mapreduce.Job:  map 30% reduce 0%
19/12/13 03:03:37 INFO mapreduce.Job:  map 31% reduce 0%
19/12/13 03:03:40 INFO mapreduce.Job:  map 32% reduce 0%
19/12/13 03:03:44 INFO mapreduce.Job:  map 33% reduce 0%
19/12/13 03:03:47 INFO mapreduce.Job:  map 34% reduce 0%
19/12/13 03:03:50 INFO mapreduce.Job:  map 35% reduce 0%
19/12/13 03:03:52 INFO mapreduce.Job:  map 36% reduce 0%
19/12/13 03:03:55 INFO mapreduce.Job:  map 37% reduce 0%
19/12/13 03:03:57 INFO mapreduce.Job:  map 38% reduce 0%
19/12/13 03:04:00 INFO mapreduce.Job:  map 39% reduce 0%
19/12/13 03:04:04 INFO mapreduce.Job:  map 40% reduce 0%
19/12/13 03:04:07 INFO mapreduce.Job:  map 41% reduce 0%
19/12/13 03:04:10 INFO mapreduce.Job:  map 42% reduce 0%
19/12/13 03:04:13 INFO mapreduce.Job:  map 43% reduce 0%
19/12/13 03:04:16 INFO mapreduce.Job:  map 44% reduce 0%
19/12/13 03:04:18 INFO mapreduce.Job:  map 45% reduce 0%
19/12/13 03:04:20 INFO mapreduce.Job:  map 46% reduce 0%
19/12/13 03:04:22 INFO mapreduce.Job:  map 47% reduce 0%
19/12/13 03:04:24 INFO mapreduce.Job:  map 48% reduce 0%
19/12/13 03:04:26 INFO mapreduce.Job:  map 49% reduce 0%
19/12/13 03:04:27 INFO mapreduce.Job:  map 50% reduce 0%
19/12/13 03:04:29 INFO mapreduce.Job:  map 51% reduce 0%
19/12/13 03:04:31 INFO mapreduce.Job:  map 52% reduce 0%
19/12/13 03:04:33 INFO mapreduce.Job:  map 53% reduce 0%
19/12/13 03:04:36 INFO mapreduce.Job:  map 54% reduce 0%
19/12/13 03:04:38 INFO mapreduce.Job:  map 55% reduce 0%
19/12/13 03:04:40 INFO mapreduce.Job:  map 56% reduce 0%
19/12/13 03:04:42 INFO mapreduce.Job:  map 57% reduce 0%
19/12/13 03:04:44 INFO mapreduce.Job:  map 58% reduce 0%
19/12/13 03:04:47 INFO mapreduce.Job:  map 59% reduce 0%
19/12/13 03:04:48 INFO mapreduce.Job:  map 60% reduce 0%
19/12/13 03:04:52 INFO mapreduce.Job:  map 61% reduce 0%
19/12/13 03:04:55 INFO mapreduce.Job:  map 62% reduce 0%
19/12/13 03:04:58 INFO mapreduce.Job:  map 63% reduce 0%
19/12/13 03:05:00 INFO mapreduce.Job:  map 64% reduce 0%
19/12/13 03:05:03 INFO mapreduce.Job:  map 65% reduce 0%
19/12/13 03:05:07 INFO mapreduce.Job:  map 66% reduce 0%
19/12/13 03:05:10 INFO mapreduce.Job:  map 67% reduce 0%
19/12/13 03:05:13 INFO mapreduce.Job:  map 68% reduce 0%
19/12/13 03:05:16 INFO mapreduce.Job:  map 69% reduce 0%
19/12/13 03:05:19 INFO mapreduce.Job:  map 70% reduce 0%
19/12/13 03:05:22 INFO mapreduce.Job:  map 71% reduce 0%
19/12/13 03:05:24 INFO mapreduce.Job:  map 72% reduce 0%
19/12/13 03:05:27 INFO mapreduce.Job:  map 73% reduce 0%
19/12/13 03:05:29 INFO mapreduce.Job:  map 74% reduce 0%
19/12/13 03:05:31 INFO mapreduce.Job:  map 75% reduce 0%
19/12/13 03:05:34 INFO mapreduce.Job:  map 76% reduce 0%
19/12/13 03:05:36 INFO mapreduce.Job:  map 77% reduce 0%
19/12/13 03:05:39 INFO mapreduce.Job:  map 78% reduce 0%
19/12/13 03:05:43 INFO mapreduce.Job:  map 79% reduce 0%
19/12/13 03:05:46 INFO mapreduce.Job:  map 80% reduce 0%
19/12/13 03:05:50 INFO mapreduce.Job:  map 81% reduce 0%
19/12/13 03:05:52 INFO mapreduce.Job:  map 82% reduce 0%
19/12/13 03:05:55 INFO mapreduce.Job:  map 83% reduce 0%
19/12/13 03:05:56 INFO mapreduce.Job:  map 84% reduce 0%
19/12/13 03:05:58 INFO mapreduce.Job:  map 85% reduce 0%
19/12/13 03:06:01 INFO mapreduce.Job:  map 85% reduce 1%
19/12/13 03:06:02 INFO mapreduce.Job:  map 86% reduce 1%
19/12/13 03:06:05 INFO mapreduce.Job:  map 86% reduce 2%
19/12/13 03:06:06 INFO mapreduce.Job:  map 87% reduce 2%
19/12/13 03:06:07 INFO mapreduce.Job:  map 87% reduce 3%
19/12/13 03:06:10 INFO mapreduce.Job:  map 87% reduce 4%
19/12/13 03:06:18 INFO mapreduce.Job:  map 88% reduce 4%
19/12/13 03:07:49 INFO mapreduce.Job:  map 89% reduce 4%
19/12/13 03:10:21 INFO mapreduce.Job:  map 89% reduce 0%
19/12/13 03:16:37 INFO mapreduce.Job:  map 89% reduce 1%
19/12/13 03:16:51 INFO mapreduce.Job:  map 90% reduce 2%
19/12/13 03:17:21 INFO mapreduce.Job:  map 91% reduce 2%
19/12/13 03:17:58 INFO mapreduce.Job:  map 91% reduce 0%
19/12/13 03:18:07 INFO mapreduce.Job:  map 92% reduce 0%
19/12/13 03:19:53 INFO mapreduce.Job:  map 93% reduce 0%
19/12/13 03:19:54 INFO mapreduce.Job:  map 93% reduce 1%
19/12/13 03:19:56 INFO mapreduce.Job:  map 93% reduce 2%
19/12/13 03:20:16 INFO mapreduce.Job:  map 93% reduce 0%
19/12/13 03:22:25 INFO mapreduce.Job:  map 94% reduce 0%
19/12/13 03:22:39 INFO mapreduce.Job:  map 94% reduce 2%
19/12/13 03:22:41 INFO mapreduce.Job:  map 94% reduce 3%
19/12/13 03:22:42 INFO mapreduce.Job:  map 95% reduce 3%
19/12/13 03:23:00 INFO mapreduce.Job:  map 95% reduce 0%
19/12/13 03:28:49 INFO mapreduce.Job:  map 96% reduce 0%
19/12/13 03:29:39 INFO mapreduce.Job:  map 97% reduce 0%
19/12/13 03:30:19 INFO mapreduce.Job:  map 97% reduce 1%
19/12/13 03:30:49 INFO mapreduce.Job:  map 97% reduce 0%
19/12/13 03:31:02 INFO mapreduce.Job:  map 98% reduce 0%
19/12/13 03:31:07 INFO mapreduce.Job:  map 98% reduce 1%
19/12/13 03:31:11 INFO mapreduce.Job:  map 98% reduce 2%
19/12/13 03:31:26 INFO mapreduce.Job:  map 99% reduce 2%
19/12/13 03:31:39 INFO mapreduce.Job:  map 100% reduce 2%
19/12/13 03:31:43 INFO mapreduce.Job:  map 100% reduce 4%
19/12/13 03:31:48 INFO mapreduce.Job:  map 100% reduce 5%
19/12/13 03:31:49 INFO mapreduce.Job:  map 100% reduce 7%
19/12/13 03:31:50 INFO mapreduce.Job:  map 100% reduce 9%
19/12/13 03:31:52 INFO mapreduce.Job:  map 100% reduce 10%
19/12/13 03:31:53 INFO mapreduce.Job:  map 100% reduce 11%
19/12/13 03:31:56 INFO mapreduce.Job:  map 100% reduce 12%
19/12/13 03:31:58 INFO mapreduce.Job:  map 100% reduce 14%
19/12/13 03:32:01 INFO mapreduce.Job:  map 100% reduce 15%
19/12/13 03:32:07 INFO mapreduce.Job:  map 100% reduce 16%
19/12/13 03:32:09 INFO mapreduce.Job:  map 100% reduce 19%
19/12/13 03:32:10 INFO mapreduce.Job:  map 100% reduce 22%
19/12/13 03:32:11 INFO mapreduce.Job:  map 100% reduce 25%
19/12/13 03:32:12 INFO mapreduce.Job:  map 100% reduce 33%
19/12/13 03:32:13 INFO mapreduce.Job:  map 100% reduce 40%
19/12/13 03:32:14 INFO mapreduce.Job:  map 100% reduce 45%
19/12/13 03:32:15 INFO mapreduce.Job:  map 100% reduce 49%
19/12/13 03:32:16 INFO mapreduce.Job:  map 100% reduce 52%
19/12/13 03:32:17 INFO mapreduce.Job:  map 100% reduce 53%
19/12/13 03:32:18 INFO mapreduce.Job:  map 100% reduce 59%
19/12/13 03:32:19 INFO mapreduce.Job:  map 100% reduce 62%
19/12/13 03:32:20 INFO mapreduce.Job:  map 100% reduce 64%
19/12/13 03:32:21 INFO mapreduce.Job:  map 100% reduce 67%
19/12/13 03:32:22 INFO mapreduce.Job:  map 100% reduce 68%
19/12/13 03:32:23 INFO mapreduce.Job:  map 100% reduce 69%
19/12/13 03:32:24 INFO mapreduce.Job:  map 100% reduce 71%
19/12/13 03:32:25 INFO mapreduce.Job:  map 100% reduce 72%
19/12/13 03:32:27 INFO mapreduce.Job:  map 100% reduce 73%
19/12/13 03:32:28 INFO mapreduce.Job:  map 100% reduce 74%
19/12/13 03:32:29 INFO mapreduce.Job:  map 100% reduce 75%
19/12/13 03:32:31 INFO mapreduce.Job:  map 100% reduce 76%
19/12/13 03:32:33 INFO mapreduce.Job:  map 100% reduce 77%
19/12/13 03:32:35 INFO mapreduce.Job:  map 100% reduce 78%
19/12/13 03:32:38 INFO mapreduce.Job:  map 100% reduce 79%
19/12/13 03:32:40 INFO mapreduce.Job:  map 100% reduce 80%
19/12/13 03:32:41 INFO mapreduce.Job:  map 100% reduce 81%
19/12/13 03:32:42 INFO mapreduce.Job:  map 100% reduce 83%
19/12/13 03:32:43 INFO mapreduce.Job:  map 100% reduce 84%
19/12/13 03:32:44 INFO mapreduce.Job:  map 100% reduce 86%
19/12/13 03:32:46 INFO mapreduce.Job:  map 100% reduce 87%
19/12/13 03:32:48 INFO mapreduce.Job:  map 100% reduce 88%
19/12/13 03:32:49 INFO mapreduce.Job:  map 100% reduce 89%
19/12/13 03:32:50 INFO mapreduce.Job:  map 100% reduce 90%
19/12/13 03:32:51 INFO mapreduce.Job:  map 100% reduce 91%
19/12/13 03:32:52 INFO mapreduce.Job:  map 100% reduce 92%
19/12/13 03:32:53 INFO mapreduce.Job:  map 100% reduce 94%
19/12/13 03:32:54 INFO mapreduce.Job:  map 100% reduce 95%
19/12/13 03:32:55 INFO mapreduce.Job:  map 100% reduce 96%
19/12/13 03:32:56 INFO mapreduce.Job:  map 100% reduce 97%
19/12/13 03:32:59 INFO mapreduce.Job:  map 100% reduce 98%
19/12/13 03:33:03 INFO mapreduce.Job:  map 100% reduce 99%
19/12/13 03:33:51 INFO mapreduce.Job:  map 100% reduce 100%
19/12/13 03:35:38 INFO mapreduce.Job: Task Id : attempt_1573729450353_316266_r_000137_1, Status : FAILED
AttemptID:attempt_1573729450353_316266_r_000137_1 Timed out after 180 secs
19/12/13 03:36:13 INFO mapreduce.Job: Job job_1573729450353_316266 completed successfully
19/12/13 03:36:13 INFO mapreduce.Job: Counters: 52
	File System Counters
		FILE: Number of bytes read=11001576999
		FILE: Number of bytes written=25550877361
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=424733677439
		HDFS: Number of bytes written=103738609795
		HDFS: Number of read operations=6120
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=1214
	Job Counters 
		Failed reduce tasks=1
		Killed reduce tasks=78
		Launched map tasks=1840
		Launched reduce tasks=279
		Data-local map tasks=1775
		Rack-local map tasks=65
		Total time spent by all maps in occupied slots (ms)=212431296
		Total time spent by all reduces in occupied slots (ms)=90752286
		Total time spent by all map tasks (ms)=26553912
		Total time spent by all reduce tasks (ms)=15125381
		Total vcore-milliseconds taken by all map tasks=26553912
		Total vcore-milliseconds taken by all reduce tasks=15125381
		Total megabyte-milliseconds taken by all map tasks=108764823552
		Total megabyte-milliseconds taken by all reduce tasks=46465170432
	Map-Reduce Framework
		Map input records=176767105
		Map output records=43002190
		Map output bytes=48134945143
		Map output materialized bytes=14289927154
		Input split bytes=236880
		Combine input records=0
		Combine output records=0
		Reduce input groups=10712569
		Reduce shuffle bytes=14289927154
		Reduce input records=43002190
		Reduce output records=75352104
		Spilled Records=86004380
		Shuffled Maps =368000
		Failed Shuffles=0
		Merged Map outputs=368000
		GC time elapsed (ms)=355653
		CPU time spent (ms)=32764440
		Physical memory (bytes) snapshot=2688497750016
		Virtual memory (bytes) snapshot=10836811046912
		Total committed heap usage (bytes)=3919383625728
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=424733440559
	File Output Format Counters 
		Bytes Written=103738609795
19/12/13 03:36:13 INFO streaming.StreamJob: Output directory: /user/ad_user/ocpc/model_train/shitu/20191212/
+ judge_result hadoop_run
+ [[ 0 -ne 0 ]]
+ echo 'hadoop_run success.'
hadoop_run success.
+ ADFEA_DIR=/user/ad_user/ocpc/model_train/shitu/adfea/
+ ADFEA_PATH=/user/ad_user/ocpc/model_train/shitu/adfea//20191212/
+ INS_DIR=/user/ad_user/ocpc/model_train/shitu/ins/
+ INS_PATH=/user/ad_user/ocpc/model_train/shitu/ins//20191212/
+ STATINFO_DIR=/user/ad_user/ocpc/model_train/shitu/statinfo/
+ STATINFO_PATH=/user/ad_user/ocpc/model_train/shitu/statinfo//20191212/
+ APPCLKTRANS_DIR=/user/ad_user/ocpc/model_train/shitu/appclktrans/
+ APPCLKTRANS_PATH=/user/ad_user/ocpc/model_train/shitu/appclktrans//20191212/
+ SHITULOG_DIR=/user/ad_user/ocpc/model_train/shitu/log/
+ SHITULOG_PATH=/user/ad_user/ocpc/model_train/shitu/log//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/model_train/shitu/adfea//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/model_train/shitu/adfea//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/model_train/shitu/ins//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/model_train/shitu/ins//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/model_train/shitu/statinfo//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/model_train/shitu/statinfo//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/model_train/shitu/appclktrans//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/model_train/shitu/appclktrans//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -test -e /user/ad_user/ocpc/model_train/shitu/log/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -rm -r /user/ad_user/ocpc/model_train/shitu/log/
19/12/13 03:36:33 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.
Moved: 'hdfs://cootek/user/ad_user/ocpc/model_train/shitu/log' to trash at: hdfs://cootek/user/ad_user/.Trash/Current
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mkdir -p /user/ad_user/ocpc/model_train/shitu/log/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/model_train/shitu/20191212//part-*-A' /user/ad_user/ocpc/model_train/shitu/adfea//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -touchz /user/ad_user/ocpc/model_train/shitu/adfea//20191212//_SUCCESS
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/model_train/shitu/20191212//part-*-B' /user/ad_user/ocpc/model_train/shitu/statinfo//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -touchz /user/ad_user/ocpc/model_train/shitu/statinfo//20191212//_SUCCESS
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/model_train/shitu/20191212//part-*-C' /user/ad_user/ocpc/model_train/shitu/ins//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -touchz /user/ad_user/ocpc/model_train/shitu/ins//20191212//_SUCCESS
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/model_train/shitu/20191212//part-*-D' /user/ad_user/ocpc/model_train/shitu/appclktrans//20191212/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -touchz /user/ad_user/ocpc/model_train/shitu/appclktrans//20191212//_SUCCESS
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -mv '/user/ad_user/ocpc/model_train/shitu/20191212//part-*-E' /user/ad_user/ocpc/model_train/shitu/log/
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -touchz /user/ad_user/ocpc/model_train/shitu/log//_SUCCESS
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -rm -r /user/ad_user/ocpc/model_train/shitu/20191212/
19/12/13 03:37:05 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.
Moved: 'hdfs://cootek/user/ad_user/ocpc/model_train/shitu/20191212' to trash at: hdfs://cootek/user/ad_user/.Trash/Current
+ mylog 'task[20191212] success!'
++ date +%Y%m%d-%H:%M:%S
+ now=20191213-03:37:05
+ echo '[20191213-03:37:05] task[20191212]' 'success!'
[20191213-03:37:05] task[20191212] success!
+ exit 0
+ [[ 0 -ne 0 ]]
+ cd /home/ad_user/personal/ling.fang//cvr_space/model_train
+ bash -x model_train_daily_ftrl.sh
+ bash -x /home/ad_user/kinit_ad_user.sh
+ kinit -kt /home/ad_user/ad_user.keytab ad_user
++ date -d '-1 days' +%Y%m%d
+ file_time_flag=20191212
+ echo 20191212
20191212
+ USER_NAME=ad_user
+ HADOOP_HOME=/usr/local/hadoop-2.6.3
++ pwd
+ root_path=/home/ad_user/personal/ling.fang/cvr_space/model_train
+ shitu_log=/home/ad_user/personal/ling.fang/cvr_space/model_train/shitu/shitu.log
+ hdfs_shitu_ins_path=/user/ad_user/ocpc/model_train/shitu/ins/
+ shitu_ins=/home/ad_user/personal/ling.fang/cvr_space/model_train/shitu_ins/shitu_ins
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -cat '/user/ad_user/ocpc/model_train/shitu/ins//20191212/*'
+ [[ 0 -ne 0 ]]
+ shuf /home/ad_user/personal/ling.fang/cvr_space/model_train/shitu_ins/shitu_ins -o /home/ad_user/personal/ling.fang/cvr_space/model_train/shitu_ins/shitu_ins.shuffle
+ [[ 0 -ne 0 ]]
+ model_file=./model/lr_model.dat
+ ./ftrl/bin/ftrl /home/ad_user/personal/ling.fang/cvr_space/model_train/shitu_ins/shitu_ins.shuffle alpha=0.03 beta=1 l1_reg=0.1 l2_reg=0. model_out=./model/lr_model.dat.new save_aux=1 is_incre=0
FTRLSovler Start, alpha=0.03, beta=1, l1_reg=0.1, only_weight=0, l2_reg=0, is_incre=0, save_aux=1
train instance:100000, num_key:120246
train instance:200000, num_key:199189
train instance:300000, num_key:268577
train instance:400000, num_key:332532
train instance:500000, num_key:391855
train instance:600000, num_key:447549
train instance:700000, num_key:500940
train instance:800000, num_key:552377
train instance:900000, num_key:601205
train instance:1000000, num_key:648113
train instance:1100000, num_key:693608
train instance:1200000, num_key:737545
train instance:1300000, num_key:780138
train instance:1400000, num_key:821459
train instance:1500000, num_key:861974
train instance:1600000, num_key:901519
train instance:1700000, num_key:940054
train instance:1800000, num_key:977481
train instance:1900000, num_key:1013862
train instance:2000000, num_key:1049551
train instance:2100000, num_key:1084369
train instance:2200000, num_key:1118574
train instance:2300000, num_key:1151940
train instance:2400000, num_key:1184784
train instance:2500000, num_key:1217221
train instance:2600000, num_key:1249148
train instance:2700000, num_key:1279992
train instance:2800000, num_key:1310469
train instance:2900000, num_key:1340779
train instance:3000000, num_key:1370615
train instance:3100000, num_key:1399657
train instance:3200000, num_key:1428752
train instance:3300000, num_key:1457257
train instance:3400000, num_key:1485169
train instance:3500000, num_key:1512825
train instance:3600000, num_key:1540150
train instance:3700000, num_key:1566882
train instance:3800000, num_key:1593481
train instance:3900000, num_key:1619682
train instance:4000000, num_key:1645667
train instance:4100000, num_key:1671024
train instance:4200000, num_key:1696049
train instance:4300000, num_key:1720935
train instance:4400000, num_key:1745567
train instance:4500000, num_key:1769824
train instance:4600000, num_key:1793538
train instance:4700000, num_key:1817194
train instance:4800000, num_key:1840777
train instance:4900000, num_key:1864117
train instance:5000000, num_key:1887145
train instance:5100000, num_key:1909923
train instance:5200000, num_key:1932255
train instance:5300000, num_key:1954282
train instance:5400000, num_key:1976444
train instance:5500000, num_key:1998486
train instance:5600000, num_key:2020009
train instance:5700000, num_key:2041615
train instance:5800000, num_key:2062803
train instance:5900000, num_key:2084120
train instance:6000000, num_key:2105082
train instance:6100000, num_key:2125754
train instance:6200000, num_key:2146273
train instance:6300000, num_key:2166457
train instance:6400000, num_key:2186630
train instance:6500000, num_key:2206659
train instance:6600000, num_key:2226296
train instance:6700000, num_key:2246076
train instance:6800000, num_key:2265954
train instance:6900000, num_key:2285394
train instance:7000000, num_key:2304618
train instance:7100000, num_key:2323460
train instance:7200000, num_key:2342261
train instance:7300000, num_key:2361003
train instance:7400000, num_key:2379429
train instance:7500000, num_key:2397839
train instance:7600000, num_key:2416109
train instance:7700000, num_key:2434464
train instance:7800000, num_key:2452234
train instance:7900000, num_key:2470182
train instance:8000000, num_key:2488006
train instance:8100000, num_key:2505640
train instance:8200000, num_key:2523282
train instance:8300000, num_key:2540689
train instance:8400000, num_key:2557967
train instance:8500000, num_key:2575101
train instance:8600000, num_key:2592068
train instance:8700000, num_key:2608945
train instance:8800000, num_key:2625857
train instance:8900000, num_key:2642624
train instance:9000000, num_key:2659243
train instance:9100000, num_key:2675825
train instance:9200000, num_key:2692350
train instance:9300000, num_key:2708634
train instance:9400000, num_key:2725018
train instance:9500000, num_key:2741098
train instance:9600000, num_key:2757229
train instance:9700000, num_key:2773206
train instance:9800000, num_key:2789119
train instance:9900000, num_key:2805040
train instance:10000000, num_key:2820814
train instance:10100000, num_key:2836301
train instance:10200000, num_key:2851930
train instance:10300000, num_key:2867402
train instance:10400000, num_key:2882544
train instance:10500000, num_key:2897659
train instance:10600000, num_key:2912996
train instance:10700000, num_key:2928194
+ [[ 0 -ne 0 ]]
+ mv ./model/lr_model.dat ./model_bk/lr_model.dat.20191212
+ mv ./model/lr_model.dat.new ./model/lr_model.dat
+ /usr/local/hadoop-2.6.3/bin/hadoop fs -cat '/user/ad_user/ocpc/model_train/shitu/appclktrans/20191212/*'
++ cat ./model/calibrate.dat.bk
++ wc -l
+ [[ 14 -ne 0 ]]
+ cp ./model/calibrate.dat.bk ./model/calibrate.dat
++ date -d '0 days ago' +%Y%m%d%H%M%S
+ VERSION=20191213034125
+ python script/model_push_util.py 20191213034125 conf/model_push.conf
ocpc_ftrl 9
commit percent:0.121230
commit percent:0.241641
commit percent:0.362052
commit percent:0.482463
commit percent:0.602874
commit percent:0.723285
commit percent:0.843696
commit percent:0.964108
commit percent:1.084519
commit percent:1.204930
commit percent:1.325341
commit percent:1.445752
commit percent:1.566163
commit percent:1.686574
commit percent:1.806985
commit percent:1.927396
commit percent:2.047807
commit percent:2.168218
commit percent:2.288629
commit percent:2.409041
commit percent:2.529452
commit percent:2.649863
commit percent:2.770274
commit percent:2.890685
commit percent:3.011096
commit percent:3.131507
commit percent:3.251918
commit percent:3.372329
commit percent:3.492740
commit percent:3.613151
commit percent:3.733563
commit percent:3.853974
commit percent:3.974385
commit percent:4.094796
commit percent:4.215207
commit percent:4.335618
commit percent:4.456029
commit percent:4.576440
commit percent:4.696851
commit percent:4.817262
commit percent:4.937673
commit percent:5.058085
commit percent:5.178496
commit percent:5.298907
commit percent:5.419318
commit percent:5.539729
commit percent:5.660140
commit percent:5.780551
commit percent:5.900962
commit percent:6.021373
commit percent:6.141784
commit percent:6.262195
commit percent:6.382606
commit percent:6.503018
commit percent:6.623429
commit percent:6.743840
commit percent:6.864251
commit percent:6.984662
commit percent:7.105073
commit percent:7.225484
commit percent:7.345895
commit percent:7.466306
commit percent:7.586717
commit percent:7.707128
commit percent:7.827540
commit percent:7.947951
commit percent:8.068362
commit percent:8.188773
commit percent:8.309184
commit percent:8.429595
commit percent:8.550006
commit percent:8.670417
commit percent:8.790828
commit percent:8.911239
commit percent:9.031650
commit percent:9.152062
commit percent:9.272473
commit percent:9.392884
commit percent:9.513295
commit percent:9.633706
commit percent:9.754117
commit percent:9.874528
commit percent:9.994939
commit percent:10.115350
commit percent:10.235761
commit percent:10.356172
commit percent:10.476583
commit percent:10.596995
commit percent:10.717406
commit percent:10.837817
commit percent:10.958228
commit percent:11.078639
commit percent:11.199050
commit percent:11.319461
commit percent:11.439872
commit percent:11.560283
commit percent:11.680694
commit percent:11.801105
commit percent:11.921517
commit percent:12.041928
commit percent:12.162339
commit percent:12.282750
commit percent:12.403161
commit percent:12.523572
commit percent:12.643983
commit percent:12.764394
commit percent:12.884805
commit percent:13.005216
commit percent:13.125627
commit percent:13.246039
commit percent:13.366450
commit percent:13.486861
commit percent:13.607272
commit percent:13.727683
commit percent:13.848094
commit percent:13.968505
commit percent:14.088916
commit percent:14.209327
commit percent:14.329738
commit percent:14.450149
commit percent:14.570560
commit percent:14.690972
commit percent:14.811383
commit percent:14.931794
commit percent:15.052205
commit percent:15.172616
commit percent:15.293027
commit percent:15.413438
commit percent:15.533849
commit percent:15.654260
commit percent:15.774671
commit percent:15.895082
commit percent:16.015494
commit percent:16.135905
commit percent:16.256316
commit percent:16.376727
commit percent:16.497138
commit percent:16.617549
commit percent:16.737960
commit percent:16.858371
commit percent:16.978782
commit percent:17.099193
commit percent:17.219604
commit percent:17.340016
commit percent:17.460427
commit percent:17.580838
commit percent:17.701249
commit percent:17.821660
commit percent:17.942071
commit percent:18.062482
commit percent:18.182893
commit percent:18.303304
commit percent:18.423715
commit percent:18.544126
commit percent:18.664537
commit percent:18.784949
commit percent:18.905360
commit percent:19.025771
commit percent:19.146182
commit percent:19.266593
commit percent:19.387004
commit percent:19.507415
commit percent:19.627826
commit percent:19.748237
commit percent:19.868648
commit percent:19.989059
commit percent:20.109471
commit percent:20.229882
commit percent:20.350293
commit percent:20.470704
commit percent:20.591115
commit percent:20.711526
commit percent:20.831937
commit percent:20.952348
commit percent:21.072759
commit percent:21.193170
commit percent:21.313581
commit percent:21.433993
commit percent:21.554404
commit percent:21.674815
commit percent:21.795226
commit percent:21.915637
commit percent:22.036048
commit percent:22.156459
commit percent:22.276870
commit percent:22.397281
commit percent:22.517692
commit percent:22.638103
commit percent:22.758514
commit percent:22.878926
commit percent:22.999337
commit percent:23.119748
commit percent:23.240159
commit percent:23.360570
commit percent:23.480981
commit percent:23.601392
commit percent:23.721803
commit percent:23.842214
commit percent:23.962625
commit percent:24.083036
commit percent:24.203448
commit percent:24.323859
commit percent:24.444270
commit percent:24.564681
commit percent:24.685092
commit percent:24.805503
commit percent:24.925914
commit percent:25.046325
commit percent:25.166736
commit percent:25.252854
commit percent:25.373265
commit percent:25.493676
commit percent:25.614088
commit percent:25.734499
commit percent:25.854910
commit percent:25.975321
commit percent:26.095732
commit percent:26.216143
commit percent:26.336554
commit percent:26.456965
commit percent:26.577376
commit percent:26.697787
commit percent:26.818198
commit percent:26.938609
commit percent:27.059021
commit percent:27.179432
commit percent:27.299843
commit percent:27.420254
commit percent:27.540665
commit percent:27.661076
commit percent:27.781487
commit percent:27.901898
commit percent:28.022309
commit percent:28.142720
commit percent:28.263131
commit percent:28.383543
commit percent:28.503954
commit percent:28.624365
commit percent:28.744776
commit percent:28.865187
commit percent:28.985598
commit percent:29.106009
commit percent:29.226420
commit percent:29.346831
commit percent:29.467242
commit percent:29.587653
commit percent:29.708065
commit percent:29.828476
commit percent:29.948887
commit percent:30.069298
commit percent:30.189709
commit percent:30.310120
commit percent:30.430531
commit percent:30.550942
commit percent:30.671353
commit percent:30.791764
commit percent:30.912175
commit percent:31.032586
commit percent:31.152998
commit percent:31.273409
commit percent:31.393820
commit percent:31.514231
commit percent:31.634642
commit percent:31.755053
commit percent:31.875464
commit percent:31.995875
commit percent:32.116286
commit percent:32.236697
commit percent:32.357108
commit percent:32.477520
commit percent:32.597931
commit percent:32.718342
commit percent:32.838753
commit percent:32.959164
commit percent:33.079575
commit percent:33.199986
commit percent:33.320397
commit percent:33.440808
commit percent:33.561219
commit percent:33.681630
commit percent:33.802042
commit percent:33.922453
commit percent:34.042864
commit percent:34.163275
commit percent:34.283686
commit percent:34.404097
commit percent:34.524508
commit percent:34.644919
commit percent:34.765330
commit percent:34.885741
commit percent:35.006152
commit percent:35.126563
commit percent:35.246975
commit percent:35.367386
commit percent:35.487797
commit percent:35.608208
commit percent:35.728619
commit percent:35.849030
commit percent:35.969441
commit percent:36.089852
commit percent:36.210263
commit percent:36.330674
commit percent:36.451085
commit percent:36.571497
commit percent:36.691908
commit percent:36.812319
commit percent:36.932730
commit percent:37.053141
commit percent:37.173552
commit percent:37.293963
commit percent:37.414374
commit percent:37.534785
commit percent:37.655196
commit percent:37.775607
commit percent:37.896019
commit percent:38.016430
commit percent:38.136841
commit percent:38.257252
commit percent:38.377663
commit percent:38.498074
commit percent:38.618485
commit percent:38.738896
commit percent:38.859307
commit percent:38.979718
commit percent:39.100129
commit percent:39.220540
commit percent:39.340952
commit percent:39.461363
commit percent:39.581774
commit percent:39.702185
commit percent:39.822596
commit percent:39.943007
commit percent:40.063418
commit percent:40.183829
commit percent:40.304240
commit percent:40.424651
commit percent:40.545062
commit percent:40.665474
commit percent:40.785885
commit percent:40.906296
commit percent:41.026707
commit percent:41.147118
commit percent:41.267529
commit percent:41.387940
commit percent:41.508351
commit percent:41.628762
commit percent:41.749173
commit percent:41.869584
commit percent:41.989996
commit percent:42.110407
commit percent:42.230818
commit percent:42.351229
commit percent:42.471640
commit percent:42.592051
commit percent:42.712462
commit percent:42.832873
commit percent:42.953284
commit percent:43.073695
commit percent:43.194106
commit percent:43.314517
commit percent:43.434929
commit percent:43.555340
commit percent:43.675751
commit percent:43.796162
commit percent:43.916573
commit percent:44.036984
commit percent:44.157395
commit percent:44.277806
commit percent:44.398217
commit percent:44.518628
commit percent:44.639039
commit percent:44.759451
commit percent:44.879862
commit percent:45.000273
commit percent:45.120684
commit percent:45.241095
commit percent:45.361506
commit percent:45.481917
commit percent:45.602328
commit percent:45.722739
commit percent:45.843150
commit percent:45.963561
commit percent:46.083973
commit percent:46.204384
commit percent:46.324795
commit percent:46.445206
commit percent:46.565617
commit percent:46.686028
commit percent:46.806439
commit percent:46.926850
commit percent:47.047261
commit percent:47.167672
commit percent:47.288083
commit percent:47.408494
commit percent:47.528906
commit percent:47.649317
commit percent:47.769728
commit percent:47.890139
commit percent:48.010550
commit percent:48.130961
commit percent:48.251372
commit percent:48.371783
commit percent:48.492194
commit percent:48.612605
commit percent:48.733016
commit percent:48.853428
commit percent:48.973839
commit percent:49.094250
commit percent:49.214661
commit percent:49.335072
commit percent:49.455483
commit percent:49.575894
commit percent:49.696305
commit percent:49.816716
commit percent:49.937127
commit percent:50.057538
commit percent:50.177950
commit percent:50.298361
commit percent:50.418772
commit percent:50.539183
commit percent:50.659594
commit percent:50.780005
commit percent:50.900416
commit percent:51.020827
commit percent:51.141238
commit percent:51.261649
commit percent:51.382060
commit percent:51.502471
commit percent:51.622883
commit percent:51.743294
commit percent:51.863705
commit percent:51.984116
commit percent:52.104527
commit percent:52.224938
commit percent:52.345349
commit percent:52.465760
commit percent:52.586171
commit percent:52.706582
commit percent:52.826993
commit percent:52.947405
commit percent:53.067816
commit percent:53.188227
commit percent:53.308638
commit percent:53.429049
commit percent:53.549460
commit percent:53.669871
commit percent:53.790282
commit percent:53.910693
commit percent:54.031104
commit percent:54.151515
commit percent:54.271927
commit percent:54.392338
commit percent:54.512749
commit percent:54.633160
commit percent:54.753571
commit percent:54.873982
commit percent:54.994393
commit percent:55.114804
commit percent:55.235215
commit percent:55.355626
commit percent:55.476037
commit percent:55.596448
commit percent:55.716860
commit percent:55.837271
commit percent:55.957682
commit percent:56.078093
commit percent:56.198504
commit percent:56.318915
commit percent:56.439326
commit percent:56.559737
commit percent:56.680148
commit percent:56.800559
commit percent:56.920970
commit percent:57.041382
commit percent:57.161793
commit percent:57.282204
commit percent:57.402615
commit percent:57.523026
commit percent:57.643437
commit percent:57.763848
commit percent:57.884259
commit percent:58.004670
commit percent:58.125081
commit percent:58.245492
commit percent:58.365904
commit percent:58.486315
commit percent:58.606726
commit percent:58.727137
commit percent:58.847548
commit percent:58.967959
commit percent:59.088370
commit percent:59.208781
commit percent:59.329192
commit percent:59.449603
commit percent:59.570014
commit percent:59.690426
commit percent:59.810837
commit percent:59.931248
commit percent:60.051659
commit percent:60.172070
commit percent:60.292481
commit percent:60.412892
commit percent:60.533303
commit percent:60.653714
commit percent:60.774125
commit percent:60.894536
commit percent:61.014947
commit percent:61.135359
commit percent:61.255770
commit percent:61.376181
commit percent:61.496592
commit percent:61.617003
commit percent:61.737414
commit percent:61.857825
commit percent:61.978236
commit percent:62.098647
commit percent:62.219058
commit percent:62.339469
commit percent:62.459881
commit percent:62.580292
commit percent:62.700703
commit percent:62.821114
commit percent:62.941525
commit percent:63.061936
commit percent:63.182347
commit percent:63.302758
commit percent:63.423169
commit percent:63.543580
commit percent:63.663991
commit percent:63.784403
commit percent:63.904814
commit percent:64.025225
commit percent:64.145636
commit percent:64.266047
commit percent:64.386458
commit percent:64.506869
commit percent:64.627280
commit percent:64.747691
commit percent:64.868102
commit percent:64.988513
commit percent:65.108924
commit percent:65.229336
commit percent:65.349747
commit percent:65.470158
commit percent:65.590569
commit percent:65.710980
commit percent:65.831391
commit percent:65.951802
commit percent:66.072213
commit percent:66.192624
commit percent:66.313035
commit percent:66.433446
commit percent:66.553858
commit percent:66.674269
commit percent:66.794680
commit percent:66.915091
commit percent:67.035502
commit percent:67.155913
commit percent:67.276324
commit percent:67.396735
commit percent:67.517146
commit percent:67.637557
commit percent:67.757968
commit percent:67.878380
commit percent:67.998791
commit percent:68.119202
commit percent:68.239613
commit percent:68.360024
commit percent:68.480435
commit percent:68.600846
commit percent:68.721257
commit percent:68.841668
commit percent:68.962079
commit percent:69.082490
commit percent:69.202901
commit percent:69.323313
commit percent:69.443724
commit percent:69.564135
commit percent:69.684546
commit percent:69.804957
commit percent:69.925368
commit percent:70.045779
commit percent:70.166190
commit percent:70.286601
commit percent:70.407012
commit percent:70.527423
commit percent:70.647835
commit percent:70.768246
commit percent:70.888657
commit percent:71.009068
commit percent:71.129479
commit percent:71.249890
commit percent:71.370301
commit percent:71.490712
commit percent:71.611123
commit percent:71.731534
commit percent:71.851945
commit percent:71.972357
commit percent:72.092768
commit percent:72.213179
commit percent:72.333590
commit percent:72.454001
commit percent:72.574412
commit percent:72.694823
commit percent:72.815234
commit percent:72.935645
commit percent:73.056056
commit percent:73.176467
commit percent:73.296878
commit percent:73.417290
commit percent:73.537701
commit percent:73.658112
commit percent:73.778523
commit percent:73.898934
commit percent:74.019345
commit percent:74.139756
commit percent:74.260167
commit percent:74.380578
commit percent:74.500989
commit percent:74.621400
commit percent:74.741812
commit percent:74.862223
commit percent:74.982634
commit percent:75.103045
commit percent:75.223456
commit percent:75.343867
commit percent:75.464278
commit percent:75.584689
commit percent:75.705100
commit percent:75.825511
commit percent:75.945922
commit percent:76.066334
commit percent:76.186745
commit percent:76.307156
commit percent:76.427567
commit percent:76.547978
commit percent:76.668389
commit percent:76.788800
commit percent:76.909211
commit percent:77.029622
commit percent:77.150033
commit percent:77.270444
commit percent:77.390855
commit percent:77.511267
commit percent:77.631678
commit percent:77.752089
commit percent:77.872500
commit percent:77.992911
commit percent:78.113322
commit percent:78.233733
commit percent:78.354144
commit percent:78.474555
commit percent:78.594966
commit percent:78.715377
commit percent:78.835789
commit percent:78.956200
commit percent:79.076611
commit percent:79.197022
commit percent:79.317433
commit percent:79.437844
commit percent:79.558255
commit percent:79.678666
commit percent:79.799077
commit percent:79.919488
commit percent:80.039899
commit percent:80.160311
commit percent:80.280722
commit percent:80.401133
commit percent:80.521544
commit percent:80.641955
commit percent:80.762366
commit percent:80.882777
commit percent:81.003188
commit percent:81.123599
commit percent:81.244010
commit percent:81.364421
commit percent:81.484832
commit percent:81.605244
commit percent:81.725655
commit percent:81.846066
commit percent:81.966477
commit percent:82.086888
commit percent:82.207299
commit percent:82.327710
commit percent:82.448121
commit percent:82.568532
commit percent:82.688943
commit percent:82.809354
commit percent:82.929766
commit percent:83.050177
commit percent:83.170588
commit percent:83.290999
commit percent:83.411410
commit percent:83.531821
commit percent:83.652232
commit percent:83.772643
commit percent:83.893054
commit percent:84.013465
commit percent:84.133876
commit percent:84.254288
commit percent:84.374699
commit percent:84.495110
commit percent:84.615521
commit percent:84.735932
commit percent:84.856343
commit percent:84.976754
commit percent:85.097165
commit percent:85.217576
commit percent:85.337987
commit percent:85.458398
commit percent:85.578809
commit percent:85.699221
commit percent:85.819632
commit percent:85.940043
commit percent:86.060454
commit percent:86.180865
commit percent:86.301276
commit percent:86.421687
commit percent:86.542098
commit percent:86.662509
commit percent:86.782920
commit percent:86.903331
commit percent:87.023743
commit percent:87.144154
commit percent:87.264565
commit percent:87.384976
commit percent:87.505387
commit percent:87.625798
commit percent:87.746209
commit percent:87.866620
commit percent:87.987031
commit percent:88.107442
commit percent:88.227853
commit percent:88.348265
commit percent:88.468676
commit percent:88.589087
commit percent:88.709498
commit percent:88.829909
commit percent:88.950320
commit percent:89.070731
commit percent:89.191142
commit percent:89.311553
commit percent:89.431964
commit percent:89.552375
commit percent:89.672786
commit percent:89.793198
commit percent:89.913609
commit percent:90.034020
commit percent:90.154431
commit percent:90.274842
commit percent:90.395253
commit percent:90.515664
commit percent:90.636075
commit percent:90.756486
commit percent:90.876897
commit percent:90.997308
commit percent:91.117720
commit percent:91.238131
commit percent:91.358542
commit percent:91.478953
commit percent:91.599364
commit percent:91.719775
commit percent:91.840186
commit percent:91.960597
commit percent:92.081008
commit percent:92.201419
commit percent:92.321830
commit percent:92.442242
commit percent:92.562653
commit percent:92.683064
commit percent:92.803475
commit percent:92.923886
commit percent:93.044297
commit percent:93.164708
commit percent:93.285119
commit percent:93.405530
commit percent:93.525941
commit percent:93.646352
commit percent:93.766763
commit percent:93.887175
commit percent:94.007586
commit percent:94.127997
commit percent:94.248408
commit percent:94.368819
commit percent:94.489230
commit percent:94.609641
commit percent:94.730052
commit percent:94.850463
commit percent:94.970874
commit percent:95.091285
commit percent:95.211697
commit percent:95.332108
commit percent:95.452519
commit percent:95.572930
commit percent:95.693341
commit percent:95.813752
commit percent:95.934163
commit percent:96.054574
commit percent:96.174985
commit percent:96.295396
commit percent:96.415807
commit percent:96.536219
commit percent:96.656630
commit percent:96.777041
commit percent:96.897452
commit percent:97.017863
commit percent:97.138274
commit percent:97.258685
commit percent:97.379096
commit percent:97.499507
commit percent:97.619918
commit percent:97.740329
commit percent:97.860740
commit percent:97.981152
commit percent:98.101563
commit percent:98.221974
commit percent:98.342385
commit percent:98.462796
commit percent:98.583207
commit percent:98.703618
commit percent:98.824029
commit percent:98.944440
commit percent:99.064851
commit percent:99.185262
commit percent:99.305674
commit percent:99.426085
commit percent:99.546496
commit percent:99.666907
commit percent:99.787318
commit percent:99.907729
commit percent:100.000000
===========
./model_version/ocpc_ftrl.ver
20191210
['20191909171935\n', '20191911155111\n', '20191911155111\n', '20191120030517\n', '20191121022855\n', '20191121120545\n', '20191121210342\n', '20191122031815\n', '20191123025356\n']
delete meepo[20191213034125] occured exception: local variable 'line' referenced before assignment
===========
ocpc_fea_list 13
40
commit percent:0.479900
commit percent:0.956558
commit percent:1.433217
commit percent:1.909875
commit percent:2.386534
commit percent:2.863192
commit percent:3.339851
commit percent:3.816509
commit percent:4.293168
commit percent:4.769826
commit percent:5.246485
commit percent:5.723143
commit percent:6.199802
commit percent:6.676460
commit percent:7.153119
commit percent:7.629777
commit percent:8.106436
commit percent:8.583094
commit percent:9.059753
commit percent:9.536411
commit percent:10.013070
commit percent:10.489728
commit percent:10.966387
commit percent:11.443046
commit percent:11.919704
commit percent:12.396363
commit percent:12.873021
commit percent:13.349680
commit percent:13.826338
commit percent:14.302997
commit percent:14.779655
commit percent:15.256314
commit percent:15.732972
commit percent:16.209631
commit percent:16.686289
commit percent:17.162948
commit percent:17.639606
commit percent:18.116265
commit percent:18.592923
commit percent:19.069582
commit percent:19.546240
commit percent:20.022899
commit percent:20.499557
commit percent:20.976216
commit percent:21.452874
commit percent:21.929533
commit percent:22.406191
commit percent:22.882850
commit percent:23.359508
commit percent:23.836167
commit percent:24.312825
commit percent:24.789484
commit percent:25.266142
commit percent:25.742801
commit percent:26.219459
commit percent:26.696118
commit percent:27.172776
commit percent:27.649435
commit percent:28.126093
commit percent:28.602752
commit percent:29.079410
commit percent:29.556069
commit percent:30.032727
commit percent:30.509386
commit percent:30.986044
commit percent:31.462703
commit percent:31.939361
commit percent:32.416020
commit percent:32.892678
commit percent:33.369337
commit percent:33.845995
commit percent:34.322654
commit percent:34.799312
commit percent:35.275971
commit percent:35.752629
commit percent:36.229288
commit percent:36.705947
commit percent:37.182605
commit percent:37.659264
commit percent:38.135922
commit percent:38.612581
commit percent:39.089239
commit percent:39.565898
commit percent:40.042556
commit percent:40.519215
commit percent:40.995873
commit percent:41.472532
commit percent:41.949190
commit percent:42.425849
commit percent:42.902507
commit percent:43.379166
commit percent:43.855824
commit percent:44.332483
commit percent:44.809141
commit percent:45.285800
commit percent:45.762458
commit percent:46.239117
commit percent:46.715775
commit percent:47.192434
commit percent:47.669092
commit percent:48.145751
commit percent:48.622409
commit percent:49.099068
commit percent:49.575726
commit percent:50.052385
commit percent:50.529043
commit percent:51.005702
commit percent:51.482360
commit percent:51.959019
commit percent:52.435677
commit percent:52.912336
commit percent:53.388994
commit percent:53.865653
commit percent:54.342311
commit percent:54.818970
commit percent:55.295628
commit percent:55.772287
commit percent:56.248945
commit percent:56.725604
commit percent:57.202262
commit percent:57.678921
commit percent:58.155579
commit percent:58.632238
commit percent:59.108896
commit percent:59.585555
commit percent:60.062213
commit percent:60.538872
commit percent:61.015530
commit percent:61.492189
commit percent:61.968848
commit percent:62.445506
commit percent:62.922165
commit percent:63.398823
commit percent:63.875482
commit percent:64.352140
commit percent:64.828799
commit percent:65.305457
commit percent:65.782116
commit percent:66.258774
commit percent:66.735433
commit percent:67.212091
commit percent:67.688750
commit percent:68.165408
commit percent:68.642067
commit percent:69.118725
commit percent:69.595384
commit percent:70.072042
commit percent:70.548701
commit percent:71.025359
commit percent:71.502018
commit percent:71.978676
commit percent:72.455335
commit percent:72.931993
commit percent:73.408652
commit percent:73.885310
commit percent:74.361969
commit percent:74.838627
commit percent:75.315286
commit percent:75.791944
commit percent:76.268603
commit percent:76.745261
commit percent:77.221920
commit percent:77.698578
commit percent:78.175237
commit percent:78.651895
commit percent:79.128554
commit percent:79.605212
commit percent:80.081871
commit percent:80.558529
commit percent:81.035188
commit percent:81.511846
commit percent:81.988505
commit percent:82.465163
commit percent:82.941822
commit percent:83.418480
commit percent:83.895139
commit percent:84.371797
commit percent:84.848456
commit percent:85.325114
commit percent:85.801773
commit percent:86.278431
commit percent:86.755090
commit percent:87.231749
commit percent:87.708407
commit percent:88.185066
commit percent:88.661724
commit percent:89.138383
commit percent:89.615041
commit percent:90.091700
commit percent:90.568358
commit percent:91.045017
commit percent:91.521675
commit percent:91.998334
commit percent:92.474992
commit percent:92.951651
commit percent:93.428309
commit percent:93.904968
commit percent:94.381626
commit percent:94.858285
commit percent:95.334943
commit percent:95.811602
commit percent:96.288260
commit percent:96.764919
commit percent:97.241577
commit percent:97.718236
commit percent:98.194894
commit percent:98.671553
commit percent:99.148211
commit percent:99.624870
commit percent:99.965776
commit percent:100.000000
ocpc_calibrate 14
commit percent:0.479981
commit percent:0.956720
commit percent:1.433459
commit percent:1.910198
commit percent:2.386937
commit percent:2.863676
commit percent:3.340414
commit percent:3.817153
commit percent:4.293892
commit percent:4.770631
commit percent:5.247370
commit percent:5.724109
commit percent:6.200848
commit percent:6.677587
commit percent:7.154326
commit percent:7.631065
commit percent:8.107804
commit percent:8.584543
commit percent:9.061282
commit percent:9.538021
commit percent:10.014760
commit percent:10.491499
commit percent:10.968238
commit percent:11.444977
commit percent:11.921716
commit percent:12.398455
commit percent:12.875194
commit percent:13.351933
commit percent:13.828671
commit percent:14.305410
commit percent:14.782149
commit percent:15.258888
commit percent:15.735627
commit percent:16.212366
commit percent:16.689105
commit percent:17.165844
commit percent:17.642583
commit percent:18.119322
commit percent:18.596061
commit percent:19.072800
commit percent:19.549539
commit percent:20.026278
commit percent:20.503017
commit percent:20.979756
commit percent:21.456495
commit percent:21.933234
commit percent:22.409973
commit percent:22.886712
commit percent:23.363451
commit percent:23.840189
commit percent:24.316928
commit percent:24.793667
commit percent:25.270406
commit percent:25.747145
commit percent:26.223884
commit percent:26.700623
commit percent:27.177362
commit percent:27.654101
commit percent:28.130840
commit percent:28.607579
commit percent:29.084318
commit percent:29.561057
commit percent:30.037796
commit percent:30.514535
commit percent:30.991274
commit percent:31.468013
commit percent:31.944752
commit percent:32.421491
commit percent:32.898230
commit percent:33.374969
commit percent:33.851707
commit percent:34.328446
commit percent:34.805185
commit percent:35.281924
commit percent:35.758663
commit percent:36.235402
commit percent:36.712141
commit percent:37.188880
commit percent:37.665619
commit percent:38.142358
commit percent:38.619097
commit percent:39.095836
commit percent:39.572575
commit percent:40.049314
commit percent:40.526053
commit percent:41.002792
commit percent:41.479531
commit percent:41.956270
commit percent:42.433009
commit percent:42.909748
commit percent:43.386487
commit percent:43.863226
commit percent:44.339964
commit percent:44.816703
commit percent:45.293442
commit percent:45.770181
commit percent:46.246920
commit percent:46.723659
commit percent:47.200398
commit percent:47.677137
commit percent:48.153876
commit percent:48.630615
commit percent:49.107354
commit percent:49.584093
commit percent:50.060832
commit percent:50.537571
commit percent:51.014310
commit percent:51.491049
commit percent:51.967788
commit percent:52.444527
commit percent:52.921266
commit percent:53.398005
commit percent:53.874744
commit percent:54.351482
commit percent:54.828221
commit percent:55.304960
commit percent:55.781699
commit percent:56.258438
commit percent:56.735177
commit percent:57.211916
commit percent:57.688655
commit percent:58.165394
commit percent:58.642133
commit percent:59.118872
commit percent:59.595611
commit percent:60.072350
commit percent:60.549089
commit percent:61.025828
commit percent:61.502567
commit percent:61.979306
commit percent:62.456045
commit percent:62.932784
commit percent:63.409523
commit percent:63.886262
commit percent:64.363000
commit percent:64.839739
commit percent:65.316478
commit percent:65.793217
commit percent:66.269956
commit percent:66.746695
commit percent:67.223434
commit percent:67.700173
commit percent:68.176912
commit percent:68.653651
commit percent:69.130390
commit percent:69.607129
commit percent:70.083868
commit percent:70.560607
commit percent:71.037346
commit percent:71.514085
commit percent:71.990824
commit percent:72.467563
commit percent:72.944302
commit percent:73.421041
commit percent:73.897780
commit percent:74.374518
commit percent:74.851257
commit percent:75.327996
commit percent:75.804735
commit percent:76.281474
commit percent:76.758213
commit percent:77.234952
commit percent:77.711691
commit percent:78.188430
commit percent:78.665169
commit percent:79.141908
commit percent:79.618647
commit percent:80.095386
commit percent:80.572125
commit percent:81.048864
commit percent:81.525603
commit percent:82.002342
commit percent:82.479081
commit percent:82.955820
commit percent:83.432559
commit percent:83.909298
commit percent:84.386037
commit percent:84.862775
commit percent:85.339514
commit percent:85.816253
commit percent:86.292992
commit percent:86.769731
commit percent:87.246470
commit percent:87.723209
commit percent:88.199948
commit percent:88.676687
commit percent:89.153426
commit percent:89.630165
commit percent:90.106904
commit percent:90.583643
commit percent:91.060382
commit percent:91.537121
commit percent:92.013860
commit percent:92.490599
commit percent:92.967338
commit percent:93.444077
commit percent:93.920816
commit percent:94.397555
commit percent:94.874293
commit percent:95.351032
commit percent:95.827771
commit percent:96.304510
commit percent:96.781249
commit percent:97.257988
commit percent:97.734727
commit percent:98.211466
commit percent:98.688205
commit percent:99.164944
commit percent:99.641683
commit percent:99.982647
commit percent:100.000000
set redis_key[ad:ocpc_ftrl] redis_value[20191213034125]
+ [[ 0 -ne 0 ]]
+ python utils/sms_sender.py '[20191213034125] ftrl cvr model train success!'
no process in __main__, /home/ad_user/personal/ling.fang/cvr_space/model_train/utils/sms_sender.py
+ exit 0
+ [[ 0 -ne 0 ]]
